{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# all this is same as before\n",
    " \n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fasttext.util as fasttext_util\n",
    "import fasttext\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# For Google Colab (if needed)\n",
    "# from google.colab import drive\n",
    "\n",
    "# specify custom functions --------------------------------------------------------------\n",
    "def custom_tokenizer(text):\n",
    "    tokenized_text = nlp(text)\n",
    "    return [tok.text for tok in tokenized_text]\n",
    "\n",
    "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
    "    vocab_size = len(vocabulary)\n",
    "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
    "    return embedding_matrix\n",
    "\n",
    "# download pretrained embeddings --------------------------------------------------------\n",
    "# for local\n",
    "#fasttext_util.download_model('en', if_exists='ignore')\n",
    "\n",
    "# for Gdrive\n",
    "drive.mount('/content/drive')\n",
    "model_path = \"/content/drive/MyDrive/cc.en.300.bin\"\n",
    "ft = fasttext.load_model(model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from '/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lauren Boebert</td>\n",
       "      <td>@laurenboebert</td>\n",
       "      <td>2022-01-17T23:32:38.000Z</td>\n",
       "      <td>Lauren Boebert\\n@laurenboebert\\n·\\nJan 18</td>\n",
       "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1,683</td>\n",
       "      <td>2,259</td>\n",
       "      <td>11.7K</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/laurenboebert/status/14832...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Catherine</td>\n",
       "      <td>@catherine___c</td>\n",
       "      <td>2022-01-17T22:54:02.000Z</td>\n",
       "      <td>Catherine\\n@catherine___c\\n·\\nJan 17</td>\n",
       "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>64</td>\n",
       "      <td>762</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/catherine___c/status/14832...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king Keith</td>\n",
       "      <td>@KaConfessor</td>\n",
       "      <td>2022-01-17T23:51:41.000Z</td>\n",
       "      <td>king Keith\\n@KaConfessor\\n·\\nJan 18</td>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>118</td>\n",
       "      <td>159</td>\n",
       "      <td>['https://pbs.twimg.com/ext_tw_video_thumb/148...</td>\n",
       "      <td>https://twitter.com/KaConfessor/status/1483225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PETRIFIED CLIMATE PARENT</td>\n",
       "      <td>@climate_parent</td>\n",
       "      <td>2022-01-17T21:42:04.000Z</td>\n",
       "      <td>PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...</td>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>158</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/climate_parent/status/1483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thomas Speight</td>\n",
       "      <td>@Thomas_Sp8</td>\n",
       "      <td>2022-01-17T21:10:40.000Z</td>\n",
       "      <td>Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17</td>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "      <td>🅾</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>127</td>\n",
       "      <td>['https://pbs.twimg.com/profile_images/1544171...</td>\n",
       "      <td>https://twitter.com/Thomas_Sp8/status/14831850...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             UserScreenName         UserName                 Timestamp  \\\n",
       "0            Lauren Boebert   @laurenboebert  2022-01-17T23:32:38.000Z   \n",
       "1                 Catherine   @catherine___c  2022-01-17T22:54:02.000Z   \n",
       "2                king Keith     @KaConfessor  2022-01-17T23:51:41.000Z   \n",
       "3  PETRIFIED CLIMATE PARENT  @climate_parent  2022-01-17T21:42:04.000Z   \n",
       "4            Thomas Speight      @Thomas_Sp8  2022-01-17T21:10:40.000Z   \n",
       "\n",
       "                                                Text  \\\n",
       "0          Lauren Boebert\\n@laurenboebert\\n·\\nJan 18   \n",
       "1               Catherine\\n@catherine___c\\n·\\nJan 17   \n",
       "2                king Keith\\n@KaConfessor\\n·\\nJan 18   \n",
       "3  PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...   \n",
       "4             Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17   \n",
       "\n",
       "                                       Embedded_text Emojis Comments  Likes  \\\n",
       "0  The only solution I’ve ever heard the Left pro...    NaN    1,683  2,259   \n",
       "1  Climate change doesn’t cause volcanic eruption...    NaN      158     64   \n",
       "2  Vaccinated tennis ball boy collapses in the te...    NaN       24    118   \n",
       "3  North America has experienced an average winte...    NaN       15     50   \n",
       "4  They're gonna do the same with Climate Change ...      🅾        4     24   \n",
       "\n",
       "  Retweets                                         Image link  \\\n",
       "0    11.7K                                                 []   \n",
       "1      762                                                 []   \n",
       "2      159  ['https://pbs.twimg.com/ext_tw_video_thumb/148...   \n",
       "3      158                                                 []   \n",
       "4      127  ['https://pbs.twimg.com/profile_images/1544171...   \n",
       "\n",
       "                                           Tweet URL  \n",
       "0  https://twitter.com/laurenboebert/status/14832...  \n",
       "1  https://twitter.com/catherine___c/status/14832...  \n",
       "2  https://twitter.com/KaConfessor/status/1483225...  \n",
       "3  https://twitter.com/climate_parent/status/1483...  \n",
       "4  https://twitter.com/Thomas_Sp8/status/14831850...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading climate df\n",
    "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
    "\n",
    "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
    "\n",
    "df_climate = pd.read_csv(input_path_climate)\n",
    "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
    "df_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (9050, 2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  The only solution I’ve ever heard the Left pro...  None\n",
       "1  Climate change doesn’t cause volcanic eruption...  None\n",
       "2  Vaccinated tennis ball boy collapses in the te...  None\n",
       "3  North America has experienced an average winte...  None\n",
       "4  They're gonna do the same with Climate Change ...  None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
    "df_climate_inference['label'] = None\n",
    "\n",
    "print(f\"Train shape {df_climate_inference.shape} \\n\")\n",
    "df_climate_inference.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: use the **same** tokenizer (with the same learned vocabulary) when applying my trained model to new data. Typically, we train the tokenizer **only** on the training set (misinformation tweets) to avoid data leakage. Then, at inference time, you apply that exact tokenizer to the climate tweets.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use the Same Trained Tokenizer?\n",
    "\n",
    "1. **Consistent Feature Representation:**  \n",
    "   Your model has learned to map _specific token indices_ (or token n-grams) to output classes. If you change the tokenizer (e.g., by retraining it on different data or adding new vocabulary), the numeric indices won’t match the same words your model learned during training. This mismatch can degrade performance or completely break your inference pipeline.\n",
    "\n",
    "2. **Avoid Data Leakage:**  \n",
    "   Training the tokenizer on any unlabeled “future” data (the climate tweets) can inadvertently leak information about that new dataset into your model pipeline, which is generally considered poor practice. Typically, the training set should define the vocabulary for your model.\n",
    "\n",
    "3. **Practical Workflows in NLP:**  \n",
    "   In real-world scenarios, your training pipeline is frozen at some point (with its tokenizer, label encoder, etc.). When you deploy your model to predict on new data, you must replicate _exactly_ the same text-preprocessing steps.\n",
    "\n",
    "---\n",
    "\n",
    "## What if the Climate Tweets Contain New or Domain-Specific Vocabulary?\n",
    "\n",
    "- **Option 1: Accept Out-of-Vocabulary Words**  \n",
    "  If the climate dataset uses words that weren’t in the original misinformation tweets vocabulary, your tokenizer will treat them as out-of-vocabulary (OOV) or handle them in some fallback way (e.g., ignoring them or assigning them to a generic “unknown” token). This is normal in most NLP pipelines: if the new text has unseen terms, the model can’t magically handle them unless the meaning can be derived from context or subword embeddings.\n",
    "\n",
    "- **Option 2: Retrain or Extend the Vocabulary**  \n",
    "  If your new dataset is large, domain-specific, and you want the model to learn from it, you could retrain or fine-tune the entire pipeline (including the tokenizer) on a combined dataset (misinformation + climate). However, that means retraining (or at least fine-tuning) the model from scratch. This is more advanced and depends on your use case:\n",
    "  1. **Combine both datasets** into a single corpus for tokenizer training.  \n",
    "  2. Tokenize both datasets with the newly trained tokenizer.  \n",
    "  3. Retrain the classifier using the combined or partial data approach.\n",
    "\n",
    "  But if your goal is purely inference (i.e., you are not re-training the model, just applying it to climate tweets), _stick to the trained tokenizer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pickle files already exist\n",
    "# local\n",
    "climate_tokens_file = 'climate_tokens.pkl'\n",
    "\n",
    "\n",
    "# for Hertie GPU:\n",
    "# climate_tokens_file = '/workspace/workspace/cache/climate_tokens.pkl'\n",
    "\n",
    "\n",
    "if os.path.exists(climate_tokens_file):\n",
    "    print(\"Tokenized text pkl files found: loading data...\")\n",
    "    # Load the pre-saved tokenized data\n",
    "    with open(climate_tokens_file, 'rb') as f:\n",
    "        climate_tokens = pickle.load(f)\n",
    "else:\n",
    "    print(\"Pickle files not found. Running tokenization...\")\n",
    "\n",
    "    print(\"Loading spaCy model...\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "\n",
    "    print(\"Tokenizing Train Data in Batches...\")\n",
    "    climate_tokens = batch_tokenize(df_climate_inference, misinfo_tokenizer_analyzer)\n",
    "\n",
    "    # Save tokenized train and test data\n",
    "    with open(climate_tokens_file, 'wb') as f:\n",
    "        pickle.dump(climate_tokens, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: INPUT PIPELINE ================================================================\n",
    "\n",
    "# vocabulary indexing -------------------------------------------------------------------\n",
    "print (\"vocab indexing\")\n",
    "\n",
    "def vocab_mapping(tokenized_text):\n",
    "    token_counts = Counter()\n",
    "    for text in tokenized_text:\n",
    "        token_counts.update(text)\n",
    "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
    "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "    return vocab\n",
    "\n",
    "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokens)\n",
    "\n",
    "# create data loaders -------------------------------------------------------------------\n",
    "\n",
    "print(\"creating data loaders\")\n",
    "\n",
    "def collate_fn(data):\n",
    "    text_list, label_list = [], []\n",
    "    for _text, _label in data:\n",
    "        # integer encoding with truncation\n",
    "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(_label)\n",
    "    label_list = torch.tensor(label_list)\n",
    "    # padding\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=0)\n",
    "    return padded_text_list, label_list\n",
    "\n",
    "max_seq_length = 300\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokens,\n",
    "                                         df_misinfo_train[\"label\"])), # was meant to be balanced\n",
    "                        batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokens,\n",
    "                                         df_misinfo_test[\"label\"])),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"created data loaders\")\n",
    "\n",
    "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
    "\n",
    "# Define the file path for the pickle file\n",
    "# for local:\n",
    "pickle_file_path = \"mapped_pretrained_embeddings.pkl\"\n",
    "# for Hertie GPU:\n",
    "#pickle_file_path = \"/workspace/workspace/mapped_pretrained_embeddings.pkl\"\n",
    "\n",
    "# Check if the pickle file already exists\n",
    "if os.path.exists(pickle_file_path):\n",
    "    # If the file exists, load it from the pickle file\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        embedding_tensor = pickle.load(f)\n",
    "    print(f\"Emebddings pre-exists: loaded embeddings from {pickle_file_path}. Shape: {embedding_tensor.shape}\")\n",
    "else:\n",
    "    # If the file does not exist, proceed with creating the embeddings and save them\n",
    "    # Load pre-trained FastText model\n",
    "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
    "\n",
    "    # Map pretrained FastText embeddings to vocabulary indices\n",
    "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
    "                                                              pre_trained_embeddings=ft)\n",
    "\n",
    "    # Convert mapped embeddings to a tensor\n",
    "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
    "\n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(pickle_file_path, 'wb') as f:\n",
    "        pickle.dump(embedding_tensor, f)\n",
    "    print(f\"Saved embeddings to {pickle_file_path}. Shape: {embedding_tensor.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
