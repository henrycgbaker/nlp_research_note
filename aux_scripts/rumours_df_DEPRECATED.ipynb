{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & process `rumours` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\n",
    "print(\"Working Directory:\", os.getcwd())\n",
    "\n",
    "path_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\n",
    "print(\"Path to rumour dataset files:\", path_rumour)\n",
    "\n",
    "path_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
    "print(\"Path to dataset files:\", path_climate)\n",
    "\n",
    "# List all files in the directory\n",
    "files_rumour = os.listdir(path_rumour)\n",
    "files_climate = os.listdir(path_climate)\n",
    "\n",
    "# Print the list of files\n",
    "print(\"Files in rumour dataset directory:\", files_rumour)\n",
    "print(\"Files in climate dataset directory:\", files_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "\n",
    "for numb in [15, 16]:\n",
    "    path_numb = files_rumour[numb - 15]  # 'twitter15' or 'twitter16' folder path\n",
    "    label_file_path_numb = os.path.join(path_rumour, path_numb, 'label.txt')\n",
    "    tweets_file_path_numb = os.path.join(path_rumour, path_numb, 'source_tweets.txt')\n",
    "\n",
    "    # Read label.txt\n",
    "    label_dict = {}\n",
    "    with open(label_file_path_numb, 'r') as file:\n",
    "        for line in file:\n",
    "            label, tweet_id = line.strip().split(':')\n",
    "            label_dict[tweet_id] = label\n",
    "\n",
    "    # Read source_tweets.txt\n",
    "    tweets_dict = {}\n",
    "    with open(tweets_file_path_numb, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet_id, tweet_content = line.strip().split('\\t', 1)\n",
    "            tweets_dict[tweet_id] = tweet_content\n",
    "\n",
    "    # Combine labels with tweets\n",
    "    for tweet_id, tweet_content in tweets_dict.items():\n",
    "        if tweet_id in label_dict:\n",
    "            combined_data.append((label_dict[tweet_id], tweet_content))\n",
    "\n",
    "    print(f\"twitter_{numb}:\")\n",
    "\n",
    "    for label, tweet in combined_data[:5]:\n",
    "        print(f\"    Label: {label}, Tweet: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\n",
    "\n",
    "unique_labels = set(label for label, tweet in combined_data)\n",
    "\n",
    "print(\"Unique labels in combined data:\")\n",
    "for label in unique_labels:\n",
    "    print(f\"   \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\n",
    "\n",
    "print(df_combined.shape)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_combined.groupby(\"Label\")\n",
    "\n",
    "# 5 random examples for each label\n",
    "for label, group in grouped:\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    sample = group.sample(n=5, random_state=42)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"   Tweet: {row['Tweet']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
