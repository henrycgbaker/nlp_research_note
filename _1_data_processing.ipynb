{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of keywords related to climate change\n",
    "keywords = [\"climate\", \"sustainability\", \"global warming\", \"carbon\", \"greenhouse\", \"emissions\", \"renewable\", \"biodiversity\", \"ecology\", \"sustainable\"]\n",
    "pattern = \"|\".join(keywords)\n",
    "\n",
    "model_dev_dict = {'train': pd.DataFrame(), 'test': pd.DataFrame()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Development Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered train shape (92394, 4)\n",
      "filtered train shape (2406, 2)\n",
      "unfiltered test shape (10267, 4)\n",
      "filtered test shape (285, 2)\n",
      "                                                  text  label\n",
      "14   WASHINGTON (Reuters) - U.S. President-elect Do...      0\n",
      "26   Now it beings the government has changed who i...      1\n",
      "65   As Hurricane Dorian Impacts East Coast, Food L...      0\n",
      "117  Ben and Jerry, the  ber liberal Vermont ice cr...      1\n",
      "123  BRASILIA (Reuters) - A congressional committee...      0\n",
      "\n",
      "Class Balance train:\n",
      "label\n",
      "0    1601\n",
      "1     805\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Balance test:\n",
      "label\n",
      "0    180\n",
      "1    105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# i) Twitter Misinformation (https://huggingface.co/datasets/roupenminassian/twitter-misinformation)\n",
    "# (1: misinformation, 0: factual)\n",
    "\n",
    "twitter_misinfo_dict = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
    "\n",
    "for df_key in ['train', 'test']:\n",
    "    df = twitter_misinfo_dict[df_key].to_pandas()\n",
    "    # remove extra columns\n",
    "    df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], errors='ignore')\n",
    "    # filter for climate relevance\n",
    "    df_filtered = df[df['text'].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)]  \n",
    "    # Save the filtered dataframe back into the dictionary \n",
    "    twitter_misinfo_dict[f'{df_key}_filtered'] = df_filtered\n",
    "    # save to global dict\n",
    "    model_dev_dict[df_key] = pd.concat([model_dev_dict[df_key], df_filtered[['text', 'label']]], ignore_index=True)\n",
    "\n",
    "print('unfiltered train shape', twitter_misinfo_dict['train'].shape)\n",
    "print('filtered train shape', twitter_misinfo_dict['train_filtered'].shape)\n",
    "print('unfiltered test shape', twitter_misinfo_dict['test'].shape)\n",
    "print('filtered test shape', twitter_misinfo_dict['test_filtered'].shape)\n",
    "print(twitter_misinfo_dict['train_filtered'].head(5))\n",
    "\n",
    "for df_key in ['train', 'test']:\n",
    "    if 'label' in twitter_misinfo_dict[f'{df_key}_filtered'].columns:\n",
    "        class_balance = twitter_misinfo_dict[f'{df_key}_filtered']['label'].value_counts()\n",
    "        print(f\"\\nClass Balance {df_key}:\")\n",
    "        print(class_balance)\n",
    "        \n",
    "#print('dict structure', twitter_misinfo_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) ‘Reddit Lies Tweets’ (https://www.kaggle.com/datasets/konradb/reddit-lies-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "unfiltered train shape (72134, 4)\n",
      "filtered train shape (2981, 2)\n",
      "                                                    text  label\n",
      "30053  Michelle Obama fed her husband s feud with Don...      0\n",
      "51055  WASHINGTON  —   On the fourth floor of the Eis...      1\n",
      "36478  President Obama, who as a candidate once urged...      0\n",
      "3966   What an evil bunch of freaks! The agenda is so...      0\n",
      "64588  President Barack Obama has officially hit the ...      1\n",
      "\n",
      "Class Balance test:\n",
      "label\n",
      "1    1661\n",
      "0    1320\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (iii) ‘Fake News Classification’ (https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "# (1 = misinfo and 0 = real)\n",
    "\n",
    "fake_news_classif_dir = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
    "fake_news_classif_dict = load_dataset(fake_news_classif_dir)\n",
    "\n",
    "df = fake_news_classif_dict['train'].to_pandas()\n",
    "df = df.drop(columns=['Unnamed: 0', 'title'], errors='ignore')\n",
    "# Reverse the encoding of the 'label' column\n",
    "df['label'] = df['label'].apply(lambda x: 1 - x)\n",
    "df_filtered = df[df['text'].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)]\n",
    "# Split into train and test sets\n",
    "train_split, test_split = train_test_split(df_filtered, test_size=0.2, random_state=42)\n",
    "# add to global dict\n",
    "model_dev_dict['train'] = pd.concat([model_dev_dict['train'], train_split[['text', 'label']]], ignore_index=True)\n",
    "model_dev_dict['test'] = pd.concat([model_dev_dict['test'], test_split[['text', 'label']]], ignore_index=True)\n",
    "# add to local dict\n",
    "train_split_filtered = train_split\n",
    "test_split_filtered = test_split\n",
    "fake_news_classif_dict['train_filtered'] = train_split_filtered\n",
    "fake_news_classif_dict['test_filtered'] = test_split_filtered\n",
    "\n",
    "print('unfiltered train shape', fake_news_classif_dict['train'].shape)\n",
    "print('filtered train shape', fake_news_classif_dict['train_filtered'].shape)\n",
    "\n",
    "print(fake_news_classif_dict['train_filtered'].head(5))\n",
    "\n",
    "if 'label' in fake_news_classif_dict['train_filtered'].columns:\n",
    "    class_balance = fake_news_classif_dict['train_filtered']['label'].value_counts()\n",
    "    print(f\"\\nClass Balance {df_key}:\")\n",
    "    print(class_balance)\n",
    "        \n",
    "#print('dict structure', fake_news_classif_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Unfiltered BuzzFeed train shape: (182, 2)\n",
      "Filtered BuzzFeed train shape: (2, 2)\n",
      "BuzzFeed train (filtered) sample:\n",
      "                                                   text  label\n",
      "142  France becomes the first country to ban plasti...      0\n",
      "146  Obama weighs in on the debate Obama weighs in ...      0\n",
      "\n",
      "Class Balance BuzzFeed train:\n",
      "label\n",
      "0    2\n",
      "Name: count, dtype: int64\n",
      "Unfiltered PolitiFact train shape: (240, 2)\n",
      "Filtered PolitiFact train shape: (4, 2)\n",
      "PolitiFact train (filtered) sample:\n",
      "                                                   text  label\n",
      "232  Trump’s High-Energy War on American Politics T...      0\n",
      "112  Trump’s High-Energy War on American Politics T...      1\n",
      "226  Louisiana Cop Claims Murdering A 6-Year Old Ch...      0\n",
      "184  BREAKING: North Carolina Drops Anti-LGBT 'Bath...      0\n",
      "\n",
      "Class Balance PolitiFact train:\n",
      "label\n",
      "0    3\n",
      "1    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (iv) ‘Fake News Net) (https://www.kaggle.com/datasets/mdepak/fakenewsnet)\n",
    "# 1 = misinfo; 0 = real\n",
    "\n",
    "fake_news_net_dir = kagglehub.dataset_download(\"mdepak/fakenewsnet\")\n",
    "\n",
    "BuzzFeed_dict = {}\n",
    "Politifact_dict = {}\n",
    "\n",
    "for source, source_dict in zip(['BuzzFeed', 'PolitiFact'], [BuzzFeed_dict, Politifact_dict]):\n",
    "    source_df = []\n",
    "    \n",
    "    for label in ['fake', 'real']:\n",
    "        file_path = os.path.join(fake_news_net_dir, f'{source}_{label}_news_content.csv')\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop unnecessary columns and create combined text\n",
    "        df = df.drop(columns=['id'], errors='ignore')\n",
    "        df['text'] = df['title'] + ' ' + df['text']\n",
    "        df['label'] = 1 if label == 'fake' else 0\n",
    "\n",
    "        source_df.append(df[['text', 'label']])\n",
    "    \n",
    "    # Combine fake and real data\n",
    "    source_df = pd.concat(source_df, ignore_index=True)\n",
    "    \n",
    "    # Filter the data based on the pattern\n",
    "    source_filtered_df = source_df[source_df['text'].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)]\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_filtered, test_filtered = train_test_split(source_filtered_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Store train and test data in the respective dictionary\n",
    "    source_dict['train'] = train_filtered\n",
    "    source_dict['test'] = test_filtered\n",
    "\n",
    "    # store in global dict\n",
    "    model_dev_dict['train'] = pd.concat([model_dev_dict['train'], train_filtered[['text', 'label']]], ignore_index=True)\n",
    "    model_dev_dict['test'] = pd.concat([model_dev_dict['test'], test_filtered[['text', 'label']]], ignore_index=True)\n",
    "        \n",
    "    # Debugging output\n",
    "    print(f'Unfiltered {source} train shape:', source_df.shape)\n",
    "    print(f'Filtered {source} train shape:', source_dict['train'].shape)\n",
    "    print(f'{source} train (filtered) sample:\\n', source_dict['train'].head(5))\n",
    "\n",
    "    if 'label' in source_dict['train'].columns:\n",
    "        class_balance = source_dict['train']['label'].value_counts()\n",
    "        print(f\"\\nClass Balance {source} train:\")\n",
    "        print(class_balance)\n",
    "\n",
    "    #print(f'{source} dict structure:', source_dict)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Train Dataset Shape: (5393, 2)\n",
      "Combined Test Dataset Shape: (1034, 2)\n"
     ]
    }
   ],
   "source": [
    "model_dev_dict = {\n",
    "    'train_df': model_dev_dict.pop('train'),\n",
    "    'test_df': model_dev_dict.pop('test'),\n",
    "}\n",
    "\n",
    "print(\"Combined Train Dataset Shape:\", model_dev_dict['train_df'].shape)\n",
    "print(\"Combined Test Dataset Shape:\", model_dev_dict['test_df'].shape)\n",
    "\n",
    "model_dev_dict['train_df'].to_csv(\"./data/train_data.csv\", index=False)\n",
    "model_dev_dict['test_df'].to_csv(\"./data/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Shape of reddit twitter dataset: (9050, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The only solution I’ve ever heard the Left pro...\n",
       "1  Climate change doesn’t cause volcanic eruption..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (i) ‘Climate Change Tweets’ (https://www.kaggle.com/datasets/die9origephit/climate-change-tweets)\n",
    "\n",
    "dir_path = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
    "file_name = os.listdir(dir_path)[0]\n",
    "path = os.path.join(dir_path, file_name)\n",
    "df = pd.read_csv(path)\n",
    "df = df[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
    "inference_tweets_df = df\n",
    "\n",
    "print(f\"Shape of reddit twitter dataset: {inference_tweets_df.shape}\")\n",
    "inference_tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) the ‘Reddit Climate Change' (https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "total rows: 26718281\n",
      "loaded\n",
      "Shape of inference dataset: (10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Yeah but what the above commenter is saying is...\n",
       "1  Any comparison of efficiency between solar and..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (ii) the ‘Reddit Climate Change' (https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset)\n",
    "dir_path = kagglehub.dataset_download(\"pavellexyr/the-reddit-climate-change-dataset\")\n",
    "file_name = os.listdir(dir_path)[0]\n",
    "path = os.path.join(dir_path, file_name) \n",
    "\n",
    "# it's too big\n",
    "total_rows = sum(1 for _ in open(path)) - 1\n",
    "print(\"total rows:\", total_rows)\n",
    "num_rows_to_load = 10000\n",
    "df_subset = pd.read_csv(path, nrows=num_rows_to_load)\n",
    "print(\"loaded\")\n",
    "\n",
    "df_subset = df_subset[['body']].rename(columns={'body': 'text'})\n",
    "inference_reddit_df = df_subset\n",
    "print(f\"Shape of reddit inference dataset: {inference_reddit_df.shape}\")\n",
    "inference_reddit_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined inference dataset shape: (19050, 1)\n",
      "                                                text\n",
      "0  The only solution I’ve ever heard the Left pro...\n",
      "1  Climate change doesn’t cause volcanic eruption...\n",
      "2  Vaccinated tennis ball boy collapses in the te...\n",
      "3  North America has experienced an average winte...\n",
      "4  They're gonna do the same with Climate Change ...\n"
     ]
    }
   ],
   "source": [
    "# combine\n",
    "inference_df = pd.concat([inference_tweets_df, inference_reddit_df], ignore_index=True)\n",
    "\n",
    "print(f\"Combined inference dataset shape: {inference_df.shape}\")\n",
    "print(inference_df.head(5))\n",
    "\n",
    "# Optional: Save the combined dataset to a CSV file\n",
    "inference_df.to_csv(\"/data/inference_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
