{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "from rapidfuzz import fuzz, process\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of keywords related to climate change\n",
    "keywords = [\n",
    "    \"climate\", \"sustainability\", \"global warming\", \"carbon\", \"greenhouse\", \"emissions\",\n",
    "    \"renewable\", \"biodiversity\", \"ecology\", \"sustainable\", \"fossil fuels\", \"energy transition\",\n",
    "    \"carbon footprint\", \"net zero\", \"solar power\", \"wind energy\", \"climate crisis\",\n",
    "    \"carbon neutrality\", \"deforestation\", \"environmental\", \"pollution\"\n",
    "]\n",
    "\n",
    "# load NLP model for phrase matching\n",
    "nlp = spacy.blank(\"en\")  # blank spaCy model\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(keyword) for keyword in keywords]\n",
    "phrase_matcher.add(\"ClimateKeywords\", patterns)\n",
    "\n",
    "model_dev_dict = {'train': pd.DataFrame(), 'test': pd.DataFrame()}\n",
    "\n",
    "FUZZY_THRESHOLD = 80\n",
    "\n",
    "def fuzzy_match(text, keywords, threshold=FUZZY_THRESHOLD):\n",
    "    matched = process.extractOne(text, keywords, scorer=fuzz.partial_ratio)\n",
    "    return matched[1] >= threshold if matched else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Development Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebalanced train shape: (7598, 2)\n",
      "Rebalanced test shape: (1900, 2)\n",
      "Rebalanced train sample:\n",
      "                                                   text  label\n",
      "2413  patrick henningsen 21st century wiremuch was m...      1\n",
      "5680  whole foods is a store that many people love. ...      1\n",
      "1139  united nations (reuters) - u.s. secretary of s...      0\n",
      "1897  somebody must have put some truth serum in lit...      1\n",
      "5715                                                         1\n",
      "Rebalanced test sample:\n",
      "                                                   text  label\n",
      "7788  the nyt allegedly wouldn t run alan dershowitz...      1\n",
      "6856  washington (reuters) - president barack obama ...      0\n",
      "5112  look at the #campfire. 88 now confirmed dead, ...      0\n",
      "2944  climate change, rosenstein, migrant children: ...      0\n",
      "6836  moscow (reuters) - the association of european...      0\n",
      "\n",
      "Class Balance (train):\n",
      "label\n",
      "0    3932\n",
      "1    3666\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Balance (test):\n",
      "label\n",
      "0    983\n",
      "1    917\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# i) Twitter Misinformation (https://huggingface.co/datasets/roupenminassian/twitter-misinformation)\n",
    "# (1: misinformation, 0: factual)\n",
    "\n",
    "desired_train_ratio = 0.8\n",
    "\n",
    "twitter_misinfo_dict = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
    "\n",
    "model_dev_dict = {'train': pd.DataFrame(columns=['text', 'label']),\n",
    "                  'test': pd.DataFrame(columns=['text', 'label'])}\n",
    "\n",
    "for df_key in ['train', 'test']:\n",
    "    df = twitter_misinfo_dict[df_key].to_pandas()\n",
    "    \n",
    "    # remove extra columns\n",
    "    df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], errors='ignore')\n",
    "    \n",
    "    # normalize text to lowercase for consistent matching\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    # apply fuzzy and phrase matching\n",
    "    def is_relevant(text):\n",
    "        # check for exact phrase match\n",
    "        doc = nlp(text)\n",
    "        matches = phrase_matcher(doc)\n",
    "        if matches:\n",
    "            return True\n",
    "        # check for fuzzy match\n",
    "        return fuzzy_match(text, keywords)\n",
    "\n",
    "    # filter for climate relevance\n",
    "    df_filtered = df[df['text'].apply(is_relevant)]\n",
    "    \n",
    "    # save the filtered DataFrame into the dictionary\n",
    "    twitter_misinfo_dict[f'{df_key}_filtered'] = df_filtered\n",
    "\n",
    "# combine for redistribution\n",
    "combined_df = pd.concat([twitter_misinfo_dict['train_filtered'], twitter_misinfo_dict['test_filtered']], ignore_index=True)\n",
    "\n",
    "# rebalance 80:20 split\n",
    "train_split, test_split = train_test_split(combined_df, test_size=1 - desired_train_ratio, random_state=42)\n",
    "\n",
    "# update global model_dev_dict\n",
    "model_dev_dict['train'] = train_split\n",
    "model_dev_dict['test'] = test_split\n",
    "\n",
    "# twitter_misinfo_dict\n",
    "twitter_misinfo_dict['train_filtered'] = train_split.copy()\n",
    "twitter_misinfo_dict['test_filtered'] = test_split.copy()\n",
    "\n",
    "print('Rebalanced train shape:', model_dev_dict['train'].shape)\n",
    "print('Rebalanced test shape:', model_dev_dict['test'].shape)\n",
    "print('Rebalanced train sample:')\n",
    "print(model_dev_dict['train'].head(5))\n",
    "print('Rebalanced test sample:')\n",
    "print(model_dev_dict['test'].head(5))\n",
    "\n",
    "for df_key in ['train', 'test']:\n",
    "    if 'label' in model_dev_dict[df_key].columns:\n",
    "        class_balance = model_dev_dict[df_key]['label'].value_counts()\n",
    "        print(f\"\\nClass Balance ({df_key}):\")\n",
    "        print(class_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) ‘Reddit Lies Tweets’ (https://www.kaggle.com/datasets/konradb/reddit-lies-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Unfiltered train shape: (72134, 4)\n",
      "Filtered train shape: (12908, 2)\n",
      "                                                    text  label\n",
      "44137  protesters, police still clashing over dispute...      0\n",
      "11344  it is increasingly apparent that the u.s. war ...      1\n",
      "50620  rights? in the new america you don’t get any r...      0\n",
      "44705  back in february, analyzing donald trump’s app...      1\n",
      "25607  bangkok (reuters) - thailand on wednesday mark...      1\n",
      "\n",
      "Class Balance train:\n",
      "label\n",
      "1    6553\n",
      "0    6355\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (iii) ‘Fake News Classification’ (https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "# (1 = misinfo and 0 = factual)\n",
    "fake_news_classif_dir = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
    "fake_news_classif_dict = load_dataset(fake_news_classif_dir)\n",
    "\n",
    "df = fake_news_classif_dict['train'].to_pandas()\n",
    "df = df.drop(columns=['Unnamed: 0', 'title'], errors='ignore')\n",
    "df['label'] = df['label'].apply(lambda x: 1 - x)  # Reverse the encoding\n",
    "\n",
    "df = df[df['text'].notnull()]\n",
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "\n",
    "# Filter\n",
    "df_filtered = df[df['text'].apply(is_relevant)]\n",
    "\n",
    "# train and test sets\n",
    "train_split, test_split = train_test_split(df_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# to global dict\n",
    "model_dev_dict['train'] = pd.concat([model_dev_dict['train'], train_split[['text', 'label']]], ignore_index=True)\n",
    "model_dev_dict['test'] = pd.concat([model_dev_dict['test'], test_split[['text', 'label']]], ignore_index=True)\n",
    "\n",
    "#  to local dict\n",
    "fake_news_classif_dict['train_filtered'] = train_split\n",
    "fake_news_classif_dict['test_filtered'] = test_split\n",
    "\n",
    "print('Unfiltered train shape:', fake_news_classif_dict['train'].shape)\n",
    "print('Filtered train shape:', fake_news_classif_dict['train_filtered'].shape)\n",
    "print(fake_news_classif_dict['train_filtered'].head(5))\n",
    "\n",
    "if 'label' in fake_news_classif_dict['train_filtered'].columns:\n",
    "    class_balance = fake_news_classif_dict['train_filtered']['label'].value_counts()\n",
    "    print(f\"\\nClass Balance train:\")\n",
    "    print(class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Unfiltered BuzzFeed train shape: (182, 2)\n",
      "Filtered BuzzFeed train shape: (27, 2)\n",
      "BuzzFeed train (filtered) sample:\n",
      "                                                   text  label\n",
      "77   the black sphere with kevin jackson chicago en...      1\n",
      "172  trump’s puzzling pitch to black voters clevela...      0\n",
      "60   trump supreme court pick sued by feds for raci...      1\n",
      "1    charity: clinton foundation distributed “water...      1\n",
      "15   trump just made a campaign promise so ridiculo...      1\n",
      "\n",
      "Class Balance BuzzFeed train:\n",
      "label\n",
      "0    14\n",
      "1    13\n",
      "Name: count, dtype: int64\n",
      "Unfiltered PolitiFact train shape: (240, 2)\n",
      "Filtered PolitiFact train shape: (27, 2)\n",
      "PolitiFact train (filtered) sample:\n",
      "                                                   text  label\n",
      "106  louisiana cop claims murdering a 6-year old ch...      1\n",
      "235  donald trump, germany’s disfavored son – polit...      0\n",
      "61   former miss universe sizes up melania trump: '...      1\n",
      "4    monuments to the battle for the new south nine...      1\n",
      "39   donald trump, germany’s disfavored son – polit...      1\n",
      "\n",
      "Class Balance PolitiFact train:\n",
      "label\n",
      "1    15\n",
      "0    12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (iv) ‘Fake News Net) (https://www.kaggle.com/datasets/mdepak/fakenewsnet)\n",
    "# 1 = misinfo; 0 = factual\n",
    "\n",
    "fake_news_net_dir = kagglehub.dataset_download(\"mdepak/fakenewsnet\")\n",
    "\n",
    "BuzzFeed_dict = {}\n",
    "Politifact_dict = {}\n",
    "\n",
    "for source, source_dict in zip(['BuzzFeed', 'PolitiFact'], [BuzzFeed_dict, Politifact_dict]):\n",
    "    source_df = []\n",
    "    \n",
    "    for label in ['fake', 'real']:\n",
    "        file_path = os.path.join(fake_news_net_dir, f'{source}_{label}_news_content.csv')\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop unnecessary columns and create combined text\n",
    "        df = df.drop(columns=['id'], errors='ignore')\n",
    "        df['text'] = df['title'] + ' ' + df['text']\n",
    "        df['label'] = 1 if label == 'fake' else 0\n",
    "        \n",
    "        # Normalize text\n",
    "        df['text'] = df['text'].str.lower()\n",
    "\n",
    "        source_df.append(df[['text', 'label']])\n",
    "    \n",
    "    # Combine fake and real data\n",
    "    source_df = pd.concat(source_df, ignore_index=True)\n",
    "    \n",
    "    # Filter the data using combined logic\n",
    "    source_filtered_df = source_df[source_df['text'].apply(is_relevant)]\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_filtered, test_filtered = train_test_split(source_filtered_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Store train and test data in the respective dictionary\n",
    "    source_dict['train'] = train_filtered\n",
    "    source_dict['test'] = test_filtered\n",
    "\n",
    "    # Add to global dict\n",
    "    model_dev_dict['train'] = pd.concat([model_dev_dict['train'], train_filtered[['text', 'label']]], ignore_index=True)\n",
    "    model_dev_dict['test'] = pd.concat([model_dev_dict['test'], test_filtered[['text', 'label']]], ignore_index=True)\n",
    "        \n",
    "    # Debugging output\n",
    "    print(f'Unfiltered {source} train shape:', source_df.shape)\n",
    "    print(f'Filtered {source} train shape:', source_dict['train'].shape)\n",
    "    print(f'{source} train (filtered) sample:\\n', source_dict['train'].head(5))\n",
    "\n",
    "    if 'label' in source_dict['train'].columns:\n",
    "        class_balance = source_dict['train']['label'].value_counts()\n",
    "        print(f\"\\nClass Balance {source} train:\")\n",
    "        print(class_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Train Dataset Shape: (20560, 2)\n",
      "Combined Test Dataset Shape: (5141, 2)\n",
      "\n",
      "Twitter-Only Train Dataset Shape: (7598, 2)\n",
      "Twitter-Only Test Dataset Shape: (1900, 2)\n"
     ]
    }
   ],
   "source": [
    "# combined datasets\n",
    "model_dev_dict = {\n",
    "    'train_df': model_dev_dict.pop('train'),\n",
    "    'test_df': model_dev_dict.pop('test'),\n",
    "}\n",
    "\n",
    "print(\"Combined Train Dataset Shape:\", model_dev_dict['train_df'].shape)\n",
    "print(\"Combined Test Dataset Shape:\", model_dev_dict['test_df'].shape)\n",
    "\n",
    "model_dev_dict['train_df'].to_csv(\"./data/train_data.csv\", index=False)\n",
    "model_dev_dict['test_df'].to_csv(\"./data/test_data.csv\", index=False)\n",
    "\n",
    "# Twitter-only datasets\n",
    "model_dev_TwitterOnly_dict = {}\n",
    "\n",
    "model_dev_TwitterOnly_dict['train_df'] = twitter_misinfo_dict['train_filtered'][['text', 'label']].reset_index(drop=True)\n",
    "model_dev_TwitterOnly_dict['test_df'] = twitter_misinfo_dict['test_filtered'][['text', 'label']].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTwitter-Only Train Dataset Shape:\", model_dev_TwitterOnly_dict['train_df'].shape)\n",
    "print(\"Twitter-Only Test Dataset Shape:\", model_dev_TwitterOnly_dict['test_df'].shape)\n",
    "\n",
    "model_dev_TwitterOnly_dict['train_df'].to_csv(\"./data/twitter_train_data.csv\", index=False)\n",
    "model_dev_TwitterOnly_dict['test_df'].to_csv(\"./data/twitter_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "Before cleaning: (9050, 1)\n",
      "Shape of reddit twitter dataset: (7539, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate change doesn’t cause volcanic eruptions.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The only solution I’ve ever heard the Left pro...\n",
       "1   Climate change doesn’t cause volcanic eruptions."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (i) ‘Climate Change Tweets’ (https://www.kaggle.com/datasets/die9origephit/climate-change-tweets)\n",
    "\n",
    "dir_path = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
    "file_name = os.listdir(dir_path)[0]\n",
    "path = os.path.join(dir_path, file_name)\n",
    "df = pd.read_csv(path)\n",
    "df = df[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # tweets starting with \"Replying to\" , \"Quote Tweet\"\n",
    "    if re.search(r\"^(Replying to|.*Quote Tweet.*)\", text, re.IGNORECASE):\n",
    "        return None\n",
    "    # URLs, mentions, and other extraneous content after first tweet\n",
    "    cleaned_text = re.split(r\"(@\\w+|https?://\\S+)\", text)[0]\n",
    "    \n",
    "    # trailing numbers\n",
    "    cleaned_text = re.sub(r\"(\\n\\d+|,\\d+|\\s\\d+(\\.\\d+)?[Kk]?)$\", \"\", cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # if tweet starts with this string at start, drop \n",
    "    if re.search(r\"^(Replying to|.*Quote Tweet.*)\", text, re.IGNORECASE):\n",
    "        return None\n",
    "    \n",
    "    # remove all after the first mention or URL\n",
    "    cleaned_text = re.split(r\"(@\\w+|https?://\\S+)\", text)[0].strip()\n",
    "    \n",
    "    # split into lines\n",
    "    lines = cleaned_text.splitlines()\n",
    "    \n",
    "    # Regex for lines that are \"only numbers\" with commas, decimals, and optional 'K'/'k'\n",
    "    numeric_pattern = re.compile(r'^\\s*\\d+(,\\d+)*(\\.\\d+)?[Kk]?\\s*$')\n",
    "    \n",
    "    # [op off trailing numeric lines\n",
    "    while lines and numeric_pattern.match(lines[-1]):\n",
    "        lines.pop()\n",
    "    \n",
    "    # reassemble\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "print(\"Before cleaning:\", df.shape)\n",
    "\n",
    "df['text'] = df['text'].apply(clean_tweet)\n",
    "df = df.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "inference_tweets_df = df\n",
    "\n",
    "print(f\"Shape of reddit twitter dataset: {inference_tweets_df.shape}\")\n",
    "inference_tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) the ‘Reddit Climate Change' (https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "total rows: 26718281\n",
      "loaded\n",
      "Shape of reddit inference dataset: (10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Yeah but what the above commenter is saying is...\n",
       "1  Any comparison of efficiency between solar and..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (ii) the ‘Reddit Climate Change' (https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset)\n",
    "dir_path = kagglehub.dataset_download(\"pavellexyr/the-reddit-climate-change-dataset\")\n",
    "file_name = os.listdir(dir_path)[0]\n",
    "path = os.path.join(dir_path, file_name) \n",
    "\n",
    "# it's too big\n",
    "total_rows = sum(1 for _ in open(path)) - 1\n",
    "print(\"total rows:\", total_rows)\n",
    "num_rows_to_load = 10000\n",
    "df_subset = pd.read_csv(path, nrows=num_rows_to_load)\n",
    "print(\"loaded\")\n",
    "\n",
    "df_subset = df_subset[['body']].rename(columns={'body': 'text'})\n",
    "inference_reddit_df = df_subset\n",
    "print(f\"Shape of reddit inference dataset: {inference_reddit_df.shape}\")\n",
    "inference_reddit_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined inference dataset shape: (17539, 1)\n",
      "                                                text\n",
      "0  The only solution I’ve ever heard the Left pro...\n",
      "1   Climate change doesn’t cause volcanic eruptions.\n",
      "2  Vaccinated tennis ball boy collapses in the te...\n",
      "3  North America has experienced an average winte...\n",
      "4  They're gonna do the same with Climate Change ...\n",
      "\n",
      "Twitter-only inference dataset shape: (7539, 1)\n",
      "                                                text\n",
      "0  The only solution I’ve ever heard the Left pro...\n",
      "1   Climate change doesn’t cause volcanic eruptions.\n",
      "2  Vaccinated tennis ball boy collapses in the te...\n",
      "3  North America has experienced an average winte...\n",
      "4  They're gonna do the same with Climate Change ...\n"
     ]
    }
   ],
   "source": [
    "# combined inference dataset\n",
    "inference_df = pd.concat([inference_tweets_df, inference_reddit_df], ignore_index=True)\n",
    "\n",
    "print(f\"Combined inference dataset shape: {inference_df.shape}\")\n",
    "print(inference_df.head(5))\n",
    "\n",
    "inference_df.to_csv(\"./data/inference_data.csv\", index=False)\n",
    "\n",
    "# Twitter-only inference dataset\n",
    "inference_TwitterOnly_df = inference_tweets_df\n",
    "print(f\"\\nTwitter-only inference dataset shape: {inference_TwitterOnly_df.shape}\")\n",
    "print(inference_TwitterOnly_df.head(5))\n",
    "\n",
    "inference_TwitterOnly_df.to_csv(\"./data/twitter_inference_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
