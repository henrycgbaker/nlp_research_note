{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import warnings\n",
        "import sys\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W5n3aNK6TvsQ",
        "outputId": "3d42ab7b-7a93-4d6b-b948-94acd1764080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "850e0fac-7e02-455c-820b-77035ce993cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nsys.path.append('./aux_scripts')\\nfrom  misinfo_tokenizer import (get_trained_tokenizer,\\n                                batch_tokenize,\\n                                #vocab_mapping,\\n                                custom_analyzer\\n                                )\\nfrom data_loader_helpers import (#Collator,\\n                                 embedding_mapping_fasttext\\n                                 )\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# download pretrained embeddings -----------------------------------------------\n",
        "# for local\n",
        "#fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "# for Colab\n",
        "# !pip install datasets fasttext evaluate\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/Othercomputers/My MacBook Pro/Documents/repositories/nlp/nlp_research_note')\n",
        "#ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "ft = fasttext.load_model(ft_path)\n",
        "\n",
        "# download spacy model for tokenization ----------------------------------------\n",
        "cache_path = './cache/'\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "os.environ['SPACY_DATA'] = cache_path\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# load helper functions & scripts ----------------------------------------------\n",
        "'''\n",
        "sys.path.append('./aux_scripts')\n",
        "from  misinfo_tokenizer import (get_trained_tokenizer,\n",
        "                                batch_tokenize,\n",
        "                                #vocab_mapping,\n",
        "                                custom_analyzer\n",
        "                                )\n",
        "from data_loader_helpers import (#Collator,\n",
        "                                 embedding_mapping_fasttext\n",
        "                                 )\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd7tkDtkpkE",
        "outputId": "bf93912c-46bc-4a76-d5ec-20a88bb636c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found in cache. Downloading...\n",
            "Cache Directory: \n",
            "./cache/huggingface/datasets\n",
            "\n",
            "External Structure: \n",
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "\n",
            "Internal Structure: \n",
            "Dataset({\n",
            "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label'],\n",
            "    num_rows: 92394\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"./cache/huggingface/datasets\")\n",
        "dataset_path = os.path.join(hf_cache_dir, \"roupenminassian\", \"twitter-misinformation\")\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found in cache: {dataset_path}\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\", cache_dir=hf_cache_dir)\n",
        "else:\n",
        "    print(f\"Dataset not found in cache. Downloading...\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "\n",
        "print(f'Cache Directory: \\n{hf_cache_dir}')\n",
        "print(f'\\nExternal Structure: \\n{ds.shape}')\n",
        "print(f'\\nInternal Structure: \\n{ds[\"train\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "mt7Fa1Y6kpkF",
        "outputId": "95cb04b7-3a39-450f-8edc-5774de6bd47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape (5000, 2) \n",
            "\n",
            "\n",
            "0: factual, 1: misinformation\n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.646\n",
            "1    0.354\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.6548\n",
            "1    0.3452\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62905</th>\n",
              "      <td>A sudden there was a flood on the road, and th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48977</th>\n",
              "      <td>No food, no FEMA: Hurricane Michael’s survivor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20691</th>\n",
              "      <td>President Trump visits Florida hospital, prai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32672</th>\n",
              "      <td>During my 2nd week at @sacbee_news, I covered ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70612</th>\n",
              "      <td>Irma is a 5 category hurricane, and your prior...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "62905  A sudden there was a flood on the road, and th...      0\n",
              "48977  No food, no FEMA: Hurricane Michael’s survivor...      0\n",
              "20691   President Trump visits Florida hospital, prai...      1\n",
              "32672  During my 2nd week at @sacbee_news, I covered ...      0\n",
              "70612  Irma is a 5 category hurricane, and your prior...      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATA PARTITIONING =====================================================================\n",
        "ds_cloned = ds.copy()\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "df_misinfo_test = df_misinfo_test.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print('\\n0: factual, 1: misinformation\\n')\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickle files not found. Running tokenization...\n",
            "No pre-fitted tokenizer found or no file specified. Creating a new one...\n",
            "Saving fitted tokenizer to './cache/misinfo_tokenizer.pkl'...\n",
            "Tokenizing Train Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Tokenizing Test Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Train inputs tokenised: 5000\n",
            "Test inputs tokenised: 5000\n"
          ]
        }
      ],
      "source": [
        "# DEFINE TOKENIZATION FLOW =====================================================================\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", \n",
        "                 disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "    \"\"\"\n",
        "    Uses the custom_tokenizer, then replaces out-of-vocabulary tokens with <unk>.\n",
        "    \"\"\"\n",
        "    tokens = custom_tokenizer(text)\n",
        "    vocab = trained_tokenizer.vocabulary_\n",
        "    return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "def get_trained_tokenizer(text_series, tokenizer_file=None, min_df=3):\n",
        "    \"\"\"\n",
        "    1) Checks if a previously fitted tokenizer exists in tokenizer_file.\n",
        "    2) If not, create a new CountVectorizer, fit it on 'text_series'.\n",
        "    3) Save the fitted tokenizer if tokenizer_file is provided.\n",
        "    4) Return the tokenizer.\n",
        "    \"\"\"\n",
        "    # If a tokenizer file path is given and exists, load it\n",
        "    if tokenizer_file and os.path.exists(tokenizer_file):\n",
        "        print(f\"Tokenizer file '{tokenizer_file}' found. Loading it...\")\n",
        "        with open(tokenizer_file, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        # Otherwise, create a new one and fit\n",
        "        print(\"No pre-fitted tokenizer found or no file specified. Creating a new one...\")\n",
        "        tokenizer = CountVectorizer(\n",
        "            analyzer=\"word\",\n",
        "            tokenizer=custom_tokenizer,  # We define custom_tokenizer for splitting\n",
        "            lowercase=False,\n",
        "            min_df=min_df\n",
        "        )\n",
        "        tokenizer.fit(text_series)\n",
        "        \n",
        "        # Save the tokenizer if a path was provided\n",
        "        if tokenizer_file:\n",
        "            print(f\"Saving fitted tokenizer to '{tokenizer_file}'...\")\n",
        "            with open(tokenizer_file, 'wb') as f:\n",
        "                pickle.dump(tokenizer, f)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def batch_tokenize(text_series, batch_size, analyzer_func):\n",
        "    \"\"\"\n",
        "    Tokenizes a Pandas Series of text in batches to avoid memory issues.\n",
        "    \"\"\"\n",
        "    tokenized_result = []\n",
        "    total = len(text_series)\n",
        "    num_batches = (total // batch_size) + (1 if total % batch_size != 0 else 0)\n",
        "    \n",
        "    for batch_idx in range(0, total, batch_size):\n",
        "        \n",
        "        # Print progress every 200 batches or at the last batch\n",
        "        if (batch_idx // batch_size + 1) % 200 == 0 or (batch_idx + batch_size >= total):\n",
        "            print(f'Tokenizing batch {batch_idx // batch_size + 1} of {num_batches}...')\n",
        "        \n",
        "        batch_texts = text_series[batch_idx : batch_idx + batch_size]\n",
        "        for text in batch_texts:\n",
        "            tokenized_result.append(analyzer_func(text))\n",
        "    \n",
        "    return tokenized_result\n",
        "\n",
        "# TOKENIZATION ==========================================================================\n",
        "\n",
        "TOKENIZER_DIR = './cache/misinfo_tokenizer.pkl'\n",
        "TRAIN_TOKENISED_DIR = './cache/misinfo_train_tokenised.pkl'\n",
        "TEST_TOKENISED_DIR = './cache/misinfo_test_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(TRAIN_TOKENISED_DIR) and os.path.exists(TEST_TOKENISED_DIR):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load pre-saved tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_train_tokenised = pickle.load(f)\n",
        "    with open(TEST_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_test_tokenised = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    # 1) Train tokenizer\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=TOKENIZER_DIR,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # Build the default analyzer from our tokenizer\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    # 2) Tokenize train data in batches using the built analyzer (trained on train set)\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokenised = batch_tokenize(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        32,\n",
        "        misinfo_tokenizer_analyzer\n",
        "    )\n",
        "    \n",
        "    # 3) Tokenize test data in batches using custom_analyzer (which replaces OOV tokens with <unk>)\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokenised = batch_tokenize(\n",
        "        df_misinfo_test[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer)\n",
        "    )\n",
        "\n",
        "    # Optionally, save the tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokenised, f)\n",
        "    with open(TEST_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokenised, f)\n",
        "\n",
        "print(\"Train inputs tokenised:\", len(misinfo_train_tokenised))\n",
        "print(\"Test inputs tokenised:\", len(misinfo_test_tokenised))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 46425\n",
            "Vocab example: [('<pad>', 0), ('<unk>', 1), ('the', 2), (',', 3), ('.', 4), ('to', 5), ('of', 6), ('and', 7), ('a', 8), ('in', 9)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokenised)\n",
        "\n",
        "print(f\"Vocab size: {len(vocab_idx)}\")\n",
        "print(f\"Vocab example: {list(vocab_idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(data, include_lengths=True):\n",
        "    text_list, label_list, lengths = [], [], []\n",
        "    for _text, _label in data:\n",
        "        # Integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    # Padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    if include_lengths:\n",
        "        return padded_text_list, label_list, lengths\n",
        "    else:\n",
        "        return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300 # too long for classic RNN\n",
        "batch_size = 32\n",
        "\n",
        "# standard dls with collate_fn\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "# dls w/o collate_fn for CNNs:\n",
        "train_dl_cnn = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))\n",
        "\n",
        "test_dl_cnn = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                         batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d06330da-b6a8-40c7-a9b4-f4bda78aaa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\n",
            "Saved embeddings to ./cache/mapped_pretrained_embeddings.pkl. Shape: torch.Size([46425, 300])\n"
          ]
        }
      ],
      "source": [
        "# EMBEDDING MAPPING =====================================================================\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "EMBEDDINGS_FILE_PATH = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH):\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_dl, test_dl, use_lengths=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss_train\": [],\n",
        "        \"loss_test\": [],\n",
        "        \"accuracy_train\": [],\n",
        "        \"accuracy_test\": [],\n",
        "        \"f1_train\": [],\n",
        "        \"f1_test\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch in tqdm(train_dl, desc=\"Training\", leave=False):\n",
        "            if use_lengths:\n",
        "                x_batch, y_batch, lengths = batch\n",
        "                x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                pred = model(x_batch, lengths)[:, 0]  # Include lengths for RNNs/LSTMs\n",
        "            else:\n",
        "                x_batch, y_batch = batch\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                pred = model(x_batch)[:, 0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item() * y_batch.size(0)\n",
        "            preds = (pred >= 0.5).float()\n",
        "            train_correct += (preds == y_batch).float().sum().item()\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_train\"].append(train_loss / len(train_dl.dataset))\n",
        "        metrics[\"accuracy_train\"].append(train_correct / len(train_dl.dataset))\n",
        "        metrics[\"f1_train\"].append(f1_score(all_train_labels, all_train_preds))\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss, test_correct = 0, 0\n",
        "        all_test_preds, all_test_labels = [], []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dl, desc=\"Evaluating\", leave=False):\n",
        "                if use_lengths:\n",
        "                    x_batch, y_batch, lengths = batch\n",
        "                    x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                    pred = model(x_batch, lengths)[:, 0]\n",
        "                else:\n",
        "                    x_batch, y_batch = batch\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                    pred = model(x_batch)[:, 0]\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # Metrics\n",
        "                test_loss += loss.item() * y_batch.size(0)\n",
        "                preds = (pred >= 0.5).float()\n",
        "                test_correct += (preds == y_batch).float().sum().item()\n",
        "                all_test_preds.extend(preds.cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_test\"].append(test_loss / len(test_dl.dataset))\n",
        "        metrics[\"accuracy_test\"].append(test_correct / len(test_dl.dataset))\n",
        "        metrics[\"f1_test\"].append(f1_score(all_test_labels, all_test_preds))\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Loss: {metrics['loss_train'][-1]:.4f}, Accuracy: {metrics['accuracy_train'][-1]:.3f}, F1: {metrics['f1_train'][-1]:.3f}\")\n",
        "        print(f\"    Test  - Loss: {metrics['loss_test'][-1]:.4f}, Accuracy: {metrics['accuracy_test'][-1]:.3f}, F1: {metrics['f1_test'][-1]:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8qQ1LhWHrXP-",
        "outputId": "3469629f-d284-41b2-fac6-f3c8c480c93a"
      },
      "outputs": [],
      "source": [
        "# MODEL BUILDING ================================================================\n",
        "\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = TextClassificationModel(embedding_tensor=embedding_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.2702, Accuracy: 0.864, F1: 0.767\n",
            "    Test  - Loss: 0.3076, Accuracy: 0.841, F1: 0.706\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 4\u001b[0m hist_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/cnn_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_cnn, f)\n",
            "Cell \u001b[0;32mIn[41], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     76\u001b[0m preds \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     77\u001b[0m test_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "hist_cnn = train(model_cnn, num_epochs, train_dl_cnn, test_dl_cnn, use_lengths=False)\n",
        "\n",
        "with open(\"./models/train_hist/cnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_cnn, f)\n",
        "    \n",
        "torch.save(model_cnn, \"./models/cnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5884, Accuracy: 0.666, F1: 0.168\n",
            "    Test  - Loss: 0.5511, Accuracy: 0.746, F1: 0.617\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     30\u001b[0m model_rnn \u001b[38;5;241m=\u001b[39m RNNTextClassificationModel(embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor)\n\u001b[0;32m---> 31\u001b[0m hist_rnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fluctuating f1 scores, exploding gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/rnn_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_rnn, f)\n",
            "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_rnn = train(model_rnn, num_epochs, train_dl, test_dl, use_lengths=True) # fluctuating f1 scores, exploding gradients\n",
        "\n",
        "with open(\"./models/train_hist/rnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_rnn, f)\n",
        "\n",
        "torch.save(model_rnn, \"./models/rnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     29\u001b[0m model_lstm \u001b[38;5;241m=\u001b[39m LSTMTextClassificationModel(embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor)\n\u001b[0;32m---> 31\u001b[0m hist_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/lstm_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_lstm, f)\n",
            "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_lstm = train(model_lstm, num_epochs=10, train_dl=train_dl, test_dl=test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm, f)\n",
        "    \n",
        "torch.save(model_lstm, \"./models/lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.6151, Accuracy: 0.646, F1: 0.001\n",
            "    Test  - Loss: 0.5594, Accuracy: 0.655, F1: 0.000\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.5095, Accuracy: 0.699, F1: 0.350\n",
            "    Test  - Loss: 0.4939, Accuracy: 0.717, F1: 0.372\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.4619, Accuracy: 0.739, F1: 0.528\n",
            "    Test  - Loss: 0.4533, Accuracy: 0.755, F1: 0.536\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.4459, Accuracy: 0.767, F1: 0.580\n",
            "    Test  - Loss: 0.6065, Accuracy: 0.685, F1: 0.172\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.4907, Accuracy: 0.731, F1: 0.458\n",
            "    Test  - Loss: 0.4248, Accuracy: 0.805, F1: 0.727\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.3840, Accuracy: 0.816, F1: 0.721\n",
            "    Test  - Loss: 0.3956, Accuracy: 0.821, F1: 0.755\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3288, Accuracy: 0.853, F1: 0.776\n",
            "    Test  - Loss: 0.3073, Accuracy: 0.870, F1: 0.799\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.3954, Accuracy: 0.796, F1: 0.681\n",
            "    Test  - Loss: 0.3181, Accuracy: 0.861, F1: 0.792\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.3182, Accuracy: 0.867, F1: 0.810\n",
            "    Test  - Loss: 0.4231, Accuracy: 0.796, F1: 0.737\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.2768, Accuracy: 0.877, F1: 0.812\n",
            "    Test  - Loss: 0.3157, Accuracy: 0.832, F1: 0.694\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm_stacked = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_lstm_stacked = train(model_lstm_stacked, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_stacked_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm_stacked, f)\n",
        "\n",
        "torch.save(model_lstm_stacked, \"./models/lstm_stacked_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.4323, Accuracy: 0.742, F1: 0.447\n",
            "    Test  - Loss: 0.2439, Accuracy: 0.889, F1: 0.813\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.1747, Accuracy: 0.918, F1: 0.876\n",
            "    Test  - Loss: 0.1507, Accuracy: 0.928, F1: 0.887\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.1223, Accuracy: 0.944, F1: 0.917\n",
            "    Test  - Loss: 0.1579, Accuracy: 0.940, F1: 0.913\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.1041, Accuracy: 0.952, F1: 0.931\n",
            "    Test  - Loss: 0.1719, Accuracy: 0.910, F1: 0.855\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.0892, Accuracy: 0.959, F1: 0.941\n",
            "    Test  - Loss: 0.1305, Accuracy: 0.944, F1: 0.917\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.0820, Accuracy: 0.965, F1: 0.949\n",
            "    Test  - Loss: 0.1294, Accuracy: 0.945, F1: 0.917\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.0916, Accuracy: 0.960, F1: 0.942\n",
            "    Test  - Loss: 0.1187, Accuracy: 0.948, F1: 0.924\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.0672, Accuracy: 0.969, F1: 0.956\n",
            "    Test  - Loss: 0.1367, Accuracy: 0.938, F1: 0.906\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.0805, Accuracy: 0.963, F1: 0.947\n",
            "    Test  - Loss: 0.1324, Accuracy: 0.945, F1: 0.919\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.0668, Accuracy: 0.971, F1: 0.958\n",
            "    Test  - Loss: 0.1481, Accuracy: 0.940, F1: 0.910\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_bi_lstm = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_bi_lstm = train(model_bi_lstm, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/bi_lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_bi_lstm, f)\n",
        "\n",
        "torch.save(model_bi_lstm, \"./models/bi_lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "\n",
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "Unique label values in training data: {0, 1}\n",
            "Class name mapping: <bound method ClassLabel.int2str of ClassLabel(names=['factual', 'misinfo'], id=None)>\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['factual', 'misinfo']),\n",
        "})\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.reset_index(drop=True)\n",
        "df_misinfo_test = df_misinfo_test.reset_index(drop=True)\n",
        "\n",
        "# Convert train and test data to Hugging Face Dataset\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(dataset_train[2]) \n",
        "\n",
        "# Check the unique values of the 'label' column to ensure the classes are correct\n",
        "unique_labels = set(dataset_train['label'])\n",
        "print(\"Unique label values in training data:\", unique_labels)\n",
        "\n",
        "# Check the mapping of integer labels to class names\n",
        "print(\"Class name mapping:\", dataset_train.features['label'].int2str)\n",
        "\n",
        "# Create a Hugging Face DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "\n",
        "# Print the DatasetDict to check its contents\n",
        "print(f'\\n {dataset_dict}')\n",
        "\n",
        "print(dataset_train[2]) \n",
        "print(dataset_dict['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3992.74 examples/s]\n",
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4752.76 examples/s]\n",
            "  3%|▎         | 156/4710 [00:43<20:46,  3.65it/s]\n",
            "  3%|▎         | 157/4710 [01:00<20:46,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1574542075395584, 'eval_accuracy': 0.9654, 'eval_f1': 0.9487103468722206, 'eval_runtime': 16.9765, 'eval_samples_per_second': 294.525, 'eval_steps_per_second': 9.248, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            "  7%|▋         | 314/4710 [02:02<20:36,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14587999880313873, 'eval_accuracy': 0.9716, 'eval_f1': 0.9589832466782207, 'eval_runtime': 17.6708, 'eval_samples_per_second': 282.952, 'eval_steps_per_second': 8.885, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 10%|█         | 471/4710 [03:06<19:45,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.18625576794147491, 'eval_accuracy': 0.966, 'eval_f1': 0.9497635933806147, 'eval_runtime': 18.5116, 'eval_samples_per_second': 270.101, 'eval_steps_per_second': 8.481, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 500/4710 [03:14<19:36,  3.58it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0182, 'grad_norm': 0.07430180162191391, 'learning_rate': 4.469214437367304e-05, 'epoch': 3.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 13%|█▎        | 628/4710 [04:08<18:42,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.23330871760845184, 'eval_accuracy': 0.9602, 'eval_f1': 0.9403298350824588, 'eval_runtime': 17.7036, 'eval_samples_per_second': 282.428, 'eval_steps_per_second': 8.868, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 17%|█▋        | 785/4710 [05:10<17:58,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1902061551809311, 'eval_accuracy': 0.9676, 'eval_f1': 0.9526315789473684, 'eval_runtime': 17.8866, 'eval_samples_per_second': 279.539, 'eval_steps_per_second': 8.778, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 785/4710 [05:10<25:52,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 310.5572, 'train_samples_per_second': 483.003, 'train_steps_per_second': 15.166, 'train_loss': 0.014178895039163577, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./models/transformer_results/tokenizer_config.json',\n",
              " './models/transformer_results/special_tokens_map.json',\n",
              " './models/transformer_results/vocab.txt',\n",
              " './models/transformer_results/added_tokens.json',\n",
              " './models/transformer_results/tokenizer.json')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return bert_tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./transformer_results/bert_uncased\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=False,\n",
        "                                  use_cpu=False)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    bert_uncased,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer after training\n",
        "trainer.save_model(\"./models/transformer_results\")  \n",
        "bert_tokenizer.save_pretrained(\"./models/transformer_results\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "F1 Score: 0.9589832466782207\n",
            "Accuracy: 0.9716\n"
          ]
        }
      ],
      "source": [
        "def batch_predict(model, tokenizer, texts, batch_size=16, device='mps', max_length=512):\n",
        "    \"\"\"\n",
        "    Predict labels for a batch of texts using the specified model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "        model: The pre-trained model (e.g., BERT).\n",
        "        tokenizer: The tokenizer associated with the pre-trained model.\n",
        "        texts: List of input texts to predict.\n",
        "        batch_size: Number of samples per batch.\n",
        "        device: Device to use ('mps', 'cuda', or 'cpu').\n",
        "        max_length: Maximum sequence length for tokenization.\n",
        "\n",
        "    Returns:\n",
        "        List of predicted labels.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for prediction\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            # Tokenize the batch with truncation and padding\n",
        "            tokenized_batch = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Move tokenized inputs to the same device as the model\n",
        "            tokenized_batch = {key: value.to(device) for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(**tokenized_batch)\n",
        "\n",
        "            # Apply softmax to logits and determine predicted labels\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "            # Collect predictions\n",
        "            predictions.extend(predicted_labels.cpu().numpy())  # Move predictions to CPU before storing\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# USAGE\n",
        "# set device\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare dataset\n",
        "disinfo_test_texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "\n",
        "# predictions in batches\n",
        "predicted_labels = batch_predict(bert_uncased, bert_tokenizer, disinfo_test_texts, batch_size=16, device=device)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Climate Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from '/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv'...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserScreenName</th>\n",
              "      <th>UserName</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Text</th>\n",
              "      <th>Embedded_text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Retweets</th>\n",
              "      <th>Image link</th>\n",
              "      <th>Tweet URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lauren Boebert</td>\n",
              "      <td>@laurenboebert</td>\n",
              "      <td>2022-01-17T23:32:38.000Z</td>\n",
              "      <td>Lauren Boebert\\n@laurenboebert\\n·\\nJan 18</td>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1,683</td>\n",
              "      <td>2,259</td>\n",
              "      <td>11.7K</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/laurenboebert/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Catherine</td>\n",
              "      <td>@catherine___c</td>\n",
              "      <td>2022-01-17T22:54:02.000Z</td>\n",
              "      <td>Catherine\\n@catherine___c\\n·\\nJan 17</td>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>158</td>\n",
              "      <td>64</td>\n",
              "      <td>762</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/catherine___c/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>king Keith</td>\n",
              "      <td>@KaConfessor</td>\n",
              "      <td>2022-01-17T23:51:41.000Z</td>\n",
              "      <td>king Keith\\n@KaConfessor\\n·\\nJan 18</td>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24</td>\n",
              "      <td>118</td>\n",
              "      <td>159</td>\n",
              "      <td>['https://pbs.twimg.com/ext_tw_video_thumb/148...</td>\n",
              "      <td>https://twitter.com/KaConfessor/status/1483225...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PETRIFIED CLIMATE PARENT</td>\n",
              "      <td>@climate_parent</td>\n",
              "      <td>2022-01-17T21:42:04.000Z</td>\n",
              "      <td>PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...</td>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>50</td>\n",
              "      <td>158</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/climate_parent/status/1483...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thomas Speight</td>\n",
              "      <td>@Thomas_Sp8</td>\n",
              "      <td>2022-01-17T21:10:40.000Z</td>\n",
              "      <td>Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17</td>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "      <td>🅾</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>127</td>\n",
              "      <td>['https://pbs.twimg.com/profile_images/1544171...</td>\n",
              "      <td>https://twitter.com/Thomas_Sp8/status/14831850...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             UserScreenName         UserName                 Timestamp  \\\n",
              "0            Lauren Boebert   @laurenboebert  2022-01-17T23:32:38.000Z   \n",
              "1                 Catherine   @catherine___c  2022-01-17T22:54:02.000Z   \n",
              "2                king Keith     @KaConfessor  2022-01-17T23:51:41.000Z   \n",
              "3  PETRIFIED CLIMATE PARENT  @climate_parent  2022-01-17T21:42:04.000Z   \n",
              "4            Thomas Speight      @Thomas_Sp8  2022-01-17T21:10:40.000Z   \n",
              "\n",
              "                                                Text  \\\n",
              "0          Lauren Boebert\\n@laurenboebert\\n·\\nJan 18   \n",
              "1               Catherine\\n@catherine___c\\n·\\nJan 17   \n",
              "2                king Keith\\n@KaConfessor\\n·\\nJan 18   \n",
              "3  PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...   \n",
              "4             Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17   \n",
              "\n",
              "                                       Embedded_text Emojis Comments  Likes  \\\n",
              "0  The only solution I’ve ever heard the Left pro...    NaN    1,683  2,259   \n",
              "1  Climate change doesn’t cause volcanic eruption...    NaN      158     64   \n",
              "2  Vaccinated tennis ball boy collapses in the te...    NaN       24    118   \n",
              "3  North America has experienced an average winte...    NaN       15     50   \n",
              "4  They're gonna do the same with Climate Change ...      🅾        4     24   \n",
              "\n",
              "  Retweets                                         Image link  \\\n",
              "0    11.7K                                                 []   \n",
              "1      762                                                 []   \n",
              "2      159  ['https://pbs.twimg.com/ext_tw_video_thumb/148...   \n",
              "3      158                                                 []   \n",
              "4      127  ['https://pbs.twimg.com/profile_images/1544171...   \n",
              "\n",
              "                                           Tweet URL  \n",
              "0  https://twitter.com/laurenboebert/status/14832...  \n",
              "1  https://twitter.com/catherine___c/status/14832...  \n",
              "2  https://twitter.com/KaConfessor/status/1483225...  \n",
              "3  https://twitter.com/climate_parent/status/1483...  \n",
              "4  https://twitter.com/Thomas_Sp8/status/14831850...  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
        "\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape (9050, 2) \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  The only solution I’ve ever heard the Left pro...  None\n",
              "1  Climate change doesn’t cause volcanic eruption...  None\n",
              "2  Vaccinated tennis ball boy collapses in the te...  None\n",
              "3  North America has experienced an average winte...  None\n",
              "4  They're gonna do the same with Climate Change ...  None"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
        "df_climate_inference['label'] = None\n",
        "\n",
        "print(f\"Train shape {df_climate_inference.shape} \\n\")\n",
        "df_climate_inference.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TOKENISATION ==========================================================================\n",
        "\n",
        "# Check if the pickle files already exist\n",
        "# local\n",
        "CLIMATE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(CLIMATE_TOKENISED_DIR):\n",
        "    print(\"Tokenized climate tweets pkl files found: loading data...\")\n",
        "    # Load the pre-saved tokenized data\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'rb') as f:\n",
        "        climate_tokenised = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle files not found. Tokenizating climate tweets...\")\n",
        "\n",
        "\n",
        "    climate_tokenised = batch_tokenize(\n",
        "        df_climate_inference,\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer) # NB using misinfo tokenizer - have to be the same\n",
        "    )\n",
        "\n",
        "    # Save tokenized train and test data\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(climate_tokenised, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# I THINK A LOT OF THIS NOT NEEDED\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=climate_tokenised) # is this correct?? ithink not\n",
        " \n",
        "climate_dl = DataLoader(\n",
        "    dataset=list(zip(climate_tokenised,climate_tokenised[\"label\"])), # THIS SHOULD BE BLANK...\n",
        "    batch_size=32, \n",
        "    shuffle=False, \n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "EMBEDDINGS_FILE_PATH_CLIMATE = \"./cache/mapped_pretrained_embeddings_climate.pkl\"\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH_CLIMATE):\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'rb') as f:\n",
        "        embedding_tensor_climate = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings_climate = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings_climate)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
