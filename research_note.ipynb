{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets fasttext evaluate\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import warnings\n",
        "import sys\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W5n3aNK6TvsQ",
        "outputId": "3d42ab7b-7a93-4d6b-b948-94acd1764080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "850e0fac-7e02-455c-820b-77035ce993cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m334.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nsys.path.append('./aux_scripts')\\nfrom  misinfo_tokenizer import (get_trained_tokenizer,\\n                                batch_tokenize,\\n                                #vocab_mapping,\\n                                custom_analyzer\\n                                )\\nfrom data_loader_helpers import (#Collator,\\n                                 embedding_mapping_fasttext\\n                                 )\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# download pretrained embeddings -----------------------------------------------\n",
        "# for local\n",
        "#fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "# for Colab\n",
        "# !pip install datasets fasttext evaluate\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/Othercomputers/My MacBook Pro/Documents/repositories/nlp/nlp_research_note')\n",
        "#ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "ft = fasttext.load_model(ft_path)\n",
        "\n",
        "# download spacy model for tokenization ----------------------------------------\n",
        "cache_path = './cache/'\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "os.environ['SPACY_DATA'] = cache_path\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# load helper functions & scripts ----------------------------------------------\n",
        "'''\n",
        "sys.path.append('./aux_scripts')\n",
        "from  misinfo_tokenizer import (get_trained_tokenizer,\n",
        "                                batch_tokenize,\n",
        "                                #vocab_mapping,\n",
        "                                custom_analyzer\n",
        "                                )\n",
        "from data_loader_helpers import (#Collator,\n",
        "                                 embedding_mapping_fasttext\n",
        "                                 )\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd7tkDtkpkE",
        "outputId": "bf93912c-46bc-4a76-d5ec-20a88bb636c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found in cache. Downloading...\n",
            "Cache Directory: \n",
            "./cache/huggingface/datasets\n",
            "\n",
            "External Structure: \n",
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "\n",
            "Internal Structure: \n",
            "Dataset({\n",
            "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label'],\n",
            "    num_rows: 92394\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"./cache/huggingface/datasets\")\n",
        "dataset_path = os.path.join(hf_cache_dir, \"roupenminassian\", \"twitter-misinformation\")\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found in cache: {dataset_path}\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\", cache_dir=hf_cache_dir)\n",
        "else:\n",
        "    print(f\"Dataset not found in cache. Downloading...\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "\n",
        "print(f'Cache Directory: \\n{hf_cache_dir}')\n",
        "print(f'\\nExternal Structure: \\n{ds.shape}')\n",
        "print(f'\\nInternal Structure: \\n{ds[\"train\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "mt7Fa1Y6kpkF",
        "outputId": "95cb04b7-3a39-450f-8edc-5774de6bd47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape (5000, 2) \n",
            "\n",
            "\n",
            "0: factual, 1: misinformation\n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.646\n",
            "1    0.354\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.6548\n",
            "1    0.3452\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62905</th>\n",
              "      <td>A sudden there was a flood on the road, and th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48977</th>\n",
              "      <td>No food, no FEMA: Hurricane Michael’s survivor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20691</th>\n",
              "      <td>President Trump visits Florida hospital, prai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32672</th>\n",
              "      <td>During my 2nd week at @sacbee_news, I covered ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70612</th>\n",
              "      <td>Irma is a 5 category hurricane, and your prior...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "62905  A sudden there was a flood on the road, and th...      0\n",
              "48977  No food, no FEMA: Hurricane Michael’s survivor...      0\n",
              "20691   President Trump visits Florida hospital, prai...      1\n",
              "32672  During my 2nd week at @sacbee_news, I covered ...      0\n",
              "70612  Irma is a 5 category hurricane, and your prior...      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATA PARTITIONING =====================================================================\n",
        "ds_cloned = ds.copy()\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "df_misinfo_test = df_misinfo_test.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print('\\n0: factual, 1: misinformation\\n')\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickle files not found. Running tokenization...\n",
            "No pre-fitted tokenizer found or no file specified. Creating a new one...\n",
            "Saving fitted tokenizer to './cache/misinfo_tokenizer.pkl'...\n",
            "Tokenizing Train Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Tokenizing Test Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Train inputs tokenised: 5000\n",
            "Test inputs tokenised: 5000\n"
          ]
        }
      ],
      "source": [
        "# DEFINE TOKENIZATION FLOW =====================================================================\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", \n",
        "                 disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "    \"\"\"\n",
        "    Uses the custom_tokenizer, then replaces out-of-vocabulary tokens with <unk>.\n",
        "    \"\"\"\n",
        "    tokens = custom_tokenizer(text)\n",
        "    vocab = trained_tokenizer.vocabulary_\n",
        "    return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "def get_trained_tokenizer(text_series, tokenizer_file=None, min_df=3):\n",
        "    \"\"\"\n",
        "    1) Checks if a previously fitted tokenizer exists in tokenizer_file.\n",
        "    2) If not, create a new CountVectorizer, fit it on 'text_series'.\n",
        "    3) Save the fitted tokenizer if tokenizer_file is provided.\n",
        "    4) Return the tokenizer.\n",
        "    \"\"\"\n",
        "    # If a tokenizer file path is given and exists, load it\n",
        "    if tokenizer_file and os.path.exists(tokenizer_file):\n",
        "        print(f\"Tokenizer file '{tokenizer_file}' found. Loading it...\")\n",
        "        with open(tokenizer_file, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        # Otherwise, create a new one and fit\n",
        "        print(\"No pre-fitted tokenizer found or no file specified. Creating a new one...\")\n",
        "        tokenizer = CountVectorizer(\n",
        "            analyzer=\"word\",\n",
        "            tokenizer=custom_tokenizer,  # We define custom_tokenizer for splitting\n",
        "            lowercase=False,\n",
        "            min_df=min_df\n",
        "        )\n",
        "        tokenizer.fit(text_series)\n",
        "        \n",
        "        # Save the tokenizer if a path was provided\n",
        "        if tokenizer_file:\n",
        "            print(f\"Saving fitted tokenizer to '{tokenizer_file}'...\")\n",
        "            with open(tokenizer_file, 'wb') as f:\n",
        "                pickle.dump(tokenizer, f)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def batch_tokenize(text_series, batch_size, analyzer_func):\n",
        "    \"\"\"\n",
        "    Tokenizes a Pandas Series of text in batches to avoid memory issues.\n",
        "    \"\"\"\n",
        "    tokenized_result = []\n",
        "    total = len(text_series)\n",
        "    num_batches = (total // batch_size) + (1 if total % batch_size != 0 else 0)\n",
        "    \n",
        "    for batch_idx in range(0, total, batch_size):\n",
        "        \n",
        "        # Print progress every 200 batches or at the last batch\n",
        "        if (batch_idx // batch_size + 1) % 200 == 0 or (batch_idx + batch_size >= total):\n",
        "            print(f'Tokenizing batch {batch_idx // batch_size + 1} of {num_batches}...')\n",
        "        \n",
        "        batch_texts = text_series[batch_idx : batch_idx + batch_size]\n",
        "        for text in batch_texts:\n",
        "            tokenized_result.append(analyzer_func(text))\n",
        "    \n",
        "    return tokenized_result\n",
        "\n",
        "# TOKENIZATION ==========================================================================\n",
        "\n",
        "TOKENIZER_DIR = './cache/misinfo_tokenizer.pkl'\n",
        "TRAIN_TOKENISED_DIR = './cache/misinfo_train_tokenised.pkl'\n",
        "TEST_TOKENISED_DIR = './cache/misinfo_test_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(TRAIN_TOKENISED_DIR) and os.path.exists(TEST_TOKENISED_DIR):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load pre-saved tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_train_tokenised = pickle.load(f)\n",
        "    with open(TEST_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_test_tokenised = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    # 1) Train tokenizer\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=TOKENIZER_DIR,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # Build the default analyzer from our tokenizer\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    # 2) Tokenize train data in batches using the built analyzer (trained on train set)\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokenised = batch_tokenize(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        32,\n",
        "        misinfo_tokenizer_analyzer\n",
        "    )\n",
        "    \n",
        "    # 3) Tokenize test data in batches using custom_analyzer (which replaces OOV tokens with <unk>)\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokenised = batch_tokenize(\n",
        "        df_misinfo_test[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer)\n",
        "    )\n",
        "\n",
        "    # Optionally, save the tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokenised, f)\n",
        "    with open(TEST_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokenised, f)\n",
        "\n",
        "print(\"Train inputs tokenised:\", len(misinfo_train_tokenised))\n",
        "print(\"Test inputs tokenised:\", len(misinfo_test_tokenised))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 46425\n",
            "Vocab example: [('<pad>', 0), ('<unk>', 1), ('the', 2), (',', 3), ('.', 4), ('to', 5), ('of', 6), ('and', 7), ('a', 8), ('in', 9)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokenised)\n",
        "\n",
        "print(f\"Vocab size: {len(vocab_idx)}\")\n",
        "print(f\"Vocab example: {list(vocab_idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(data, include_lengths=True):\n",
        "    text_list, label_list, lengths = [], [], []\n",
        "    for _text, _label in data:\n",
        "        # Integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    # Padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    if include_lengths:\n",
        "        return padded_text_list, label_list, lengths\n",
        "    else:\n",
        "        return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300 # too long for classic RNN\n",
        "batch_size = 32\n",
        "\n",
        "# standard dls with collate_fn\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "# dls w/o collate_fn for CNNs:\n",
        "train_dl_cnn = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))\n",
        "\n",
        "test_dl_cnn = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                         batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d06330da-b6a8-40c7-a9b4-f4bda78aaa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\n",
            "Saved embeddings to ./cache/mapped_pretrained_embeddings.pkl. Shape: torch.Size([46425, 300])\n"
          ]
        }
      ],
      "source": [
        "# EMBEDDING MAPPING =====================================================================\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "EMBEDDINGS_FILE_PATH = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH):\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_dl, test_dl, use_lengths=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss_train\": [],\n",
        "        \"loss_test\": [],\n",
        "        \"accuracy_train\": [],\n",
        "        \"accuracy_test\": [],\n",
        "        \"f1_train\": [],\n",
        "        \"f1_test\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch in tqdm(train_dl, desc=\"Training\", leave=False):\n",
        "            if use_lengths:\n",
        "                x_batch, y_batch, lengths = batch\n",
        "                x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                pred = model(x_batch, lengths)[:, 0]  # Include lengths for RNNs/LSTMs\n",
        "            else:\n",
        "                x_batch, y_batch = batch\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                pred = model(x_batch)[:, 0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item() * y_batch.size(0)\n",
        "            preds = (pred >= 0.5).float()\n",
        "            train_correct += (preds == y_batch).float().sum().item()\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_train\"].append(train_loss / len(train_dl.dataset))\n",
        "        metrics[\"accuracy_train\"].append(train_correct / len(train_dl.dataset))\n",
        "        metrics[\"f1_train\"].append(f1_score(all_train_labels, all_train_preds))\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss, test_correct = 0, 0\n",
        "        all_test_preds, all_test_labels = [], []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dl, desc=\"Evaluating\", leave=False):\n",
        "                if use_lengths:\n",
        "                    x_batch, y_batch, lengths = batch\n",
        "                    x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                    pred = model(x_batch, lengths)[:, 0]\n",
        "                else:\n",
        "                    x_batch, y_batch = batch\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                    pred = model(x_batch)[:, 0]\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # Metrics\n",
        "                test_loss += loss.item() * y_batch.size(0)\n",
        "                preds = (pred >= 0.5).float()\n",
        "                test_correct += (preds == y_batch).float().sum().item()\n",
        "                all_test_preds.extend(preds.cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_test\"].append(test_loss / len(test_dl.dataset))\n",
        "        metrics[\"accuracy_test\"].append(test_correct / len(test_dl.dataset))\n",
        "        metrics[\"f1_test\"].append(f1_score(all_test_labels, all_test_preds))\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Loss: {metrics['loss_train'][-1]:.4f}, Accuracy: {metrics['accuracy_train'][-1]:.3f}, F1: {metrics['f1_train'][-1]:.3f}\")\n",
        "        print(f\"    Test  - Loss: {metrics['loss_test'][-1]:.4f}, Accuracy: {metrics['accuracy_test'][-1]:.3f}, F1: {metrics['f1_test'][-1]:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8qQ1LhWHrXP-",
        "outputId": "3469629f-d284-41b2-fac6-f3c8c480c93a"
      },
      "outputs": [],
      "source": [
        "# MODEL BUILDING ================================================================\n",
        "\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = TextClassificationModel(embedding_tensor=embedding_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5413, Accuracy: 0.742, F1: 0.431\n",
            "    Test  - Loss: 0.4735, Accuracy: 0.810, F1: 0.628\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.4366, Accuracy: 0.813, F1: 0.648\n",
            "    Test  - Loss: 0.4213, Accuracy: 0.819, F1: 0.650\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.3934, Accuracy: 0.833, F1: 0.694\n",
            "    Test  - Loss: 0.3950, Accuracy: 0.830, F1: 0.682\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.3679, Accuracy: 0.839, F1: 0.709\n",
            "    Test  - Loss: 0.3739, Accuracy: 0.829, F1: 0.676\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.3480, Accuracy: 0.843, F1: 0.718\n",
            "    Test  - Loss: 0.3579, Accuracy: 0.833, F1: 0.687\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.3312, Accuracy: 0.846, F1: 0.725\n",
            "    Test  - Loss: 0.3495, Accuracy: 0.833, F1: 0.687\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3181, Accuracy: 0.848, F1: 0.730\n",
            "    Test  - Loss: 0.3348, Accuracy: 0.843, F1: 0.715\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.3074, Accuracy: 0.848, F1: 0.732\n",
            "    Test  - Loss: 0.3239, Accuracy: 0.843, F1: 0.712\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.2928, Accuracy: 0.853, F1: 0.742\n",
            "    Test  - Loss: 0.3192, Accuracy: 0.850, F1: 0.731\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.2850, Accuracy: 0.857, F1: 0.752\n",
            "    Test  - Loss: 0.3081, Accuracy: 0.849, F1: 0.728\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "hist_cnn = train(model_cnn, num_epochs, train_dl_cnn, test_dl_cnn, use_lengths=False)\n",
        "\n",
        "with open(\"./models/train_hist/cnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_cnn, f)\n",
        "    \n",
        "torch.save(model_cnn, \"./models/cnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5848, Accuracy: 0.657, F1: 0.081\n",
            "    Test  - Loss: 0.5134, Accuracy: 0.749, F1: 0.572\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.4408, Accuracy: 0.780, F1: 0.622\n",
            "    Test  - Loss: 0.3977, Accuracy: 0.823, F1: 0.695\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.4872, Accuracy: 0.753, F1: 0.608\n",
            "    Test  - Loss: 0.4288, Accuracy: 0.807, F1: 0.657\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.4607, Accuracy: 0.773, F1: 0.628\n",
            "    Test  - Loss: 0.5279, Accuracy: 0.703, F1: 0.282\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.4825, Accuracy: 0.728, F1: 0.440\n",
            "    Test  - Loss: 0.4635, Accuracy: 0.754, F1: 0.523\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.3651, Accuracy: 0.834, F1: 0.732\n",
            "    Test  - Loss: 0.3764, Accuracy: 0.839, F1: 0.757\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3177, Accuracy: 0.864, F1: 0.795\n",
            "    Test  - Loss: 0.3460, Accuracy: 0.856, F1: 0.784\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.5536, Accuracy: 0.705, F1: 0.326\n",
            "    Test  - Loss: 0.5579, Accuracy: 0.678, F1: 0.138\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.5239, Accuracy: 0.689, F1: 0.240\n",
            "    Test  - Loss: 0.5315, Accuracy: 0.704, F1: 0.286\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.4692, Accuracy: 0.769, F1: 0.592\n",
            "    Test  - Loss: 0.5719, Accuracy: 0.692, F1: 0.306\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_rnn = train(model_rnn, num_epochs, train_dl, test_dl, use_lengths=True) # fluctuating f1 scores, exploding gradients\n",
        "\n",
        "with open(\"./models/train_hist/rnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_rnn, f)\n",
        "\n",
        "torch.save(model_rnn, \"./models/rnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5976, Accuracy: 0.647, F1: 0.006\n",
            "    Test  - Loss: 0.5113, Accuracy: 0.663, F1: 0.051\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.4723, Accuracy: 0.701, F1: 0.336\n",
            "    Test  - Loss: 0.5220, Accuracy: 0.737, F1: 0.529\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.4886, Accuracy: 0.735, F1: 0.446\n",
            "    Test  - Loss: 0.4324, Accuracy: 0.777, F1: 0.563\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.3715, Accuracy: 0.821, F1: 0.695\n",
            "    Test  - Loss: 0.3813, Accuracy: 0.812, F1: 0.647\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.4796, Accuracy: 0.775, F1: 0.674\n",
            "    Test  - Loss: 0.4782, Accuracy: 0.742, F1: 0.500\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.4176, Accuracy: 0.775, F1: 0.591\n",
            "    Test  - Loss: 0.4269, Accuracy: 0.743, F1: 0.442\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3379, Accuracy: 0.832, F1: 0.721\n",
            "    Test  - Loss: 0.3375, Accuracy: 0.844, F1: 0.736\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.3049, Accuracy: 0.863, F1: 0.784\n",
            "    Test  - Loss: 0.3452, Accuracy: 0.835, F1: 0.701\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.2662, Accuracy: 0.886, F1: 0.827\n",
            "    Test  - Loss: 0.2909, Accuracy: 0.863, F1: 0.770\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.2462, Accuracy: 0.897, F1: 0.844\n",
            "    Test  - Loss: 0.2696, Accuracy: 0.883, F1: 0.822\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_lstm = train(model_lstm, num_epochs=10, train_dl=train_dl, test_dl=test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm, f)\n",
        "    \n",
        "torch.save(model_lstm, \"./models/lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5795, Accuracy: 0.662, F1: 0.102\n",
            "    Test  - Loss: 0.4925, Accuracy: 0.757, F1: 0.481\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.4520, Accuracy: 0.781, F1: 0.640\n",
            "    Test  - Loss: 0.3677, Accuracy: 0.829, F1: 0.698\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.3735, Accuracy: 0.839, F1: 0.762\n",
            "    Test  - Loss: 0.7458, Accuracy: 0.606, F1: 0.623\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.4254, Accuracy: 0.763, F1: 0.629\n",
            "    Test  - Loss: 0.3408, Accuracy: 0.846, F1: 0.782\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.3649, Accuracy: 0.841, F1: 0.767\n",
            "    Test  - Loss: 0.5022, Accuracy: 0.679, F1: 0.161\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.3712, Accuracy: 0.814, F1: 0.711\n",
            "    Test  - Loss: 0.3677, Accuracy: 0.830, F1: 0.766\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3779, Accuracy: 0.819, F1: 0.741\n",
            "    Test  - Loss: 0.3364, Accuracy: 0.844, F1: 0.764\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.2839, Accuracy: 0.876, F1: 0.818\n",
            "    Test  - Loss: 0.2964, Accuracy: 0.862, F1: 0.783\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.2492, Accuracy: 0.897, F1: 0.848\n",
            "    Test  - Loss: 0.3010, Accuracy: 0.838, F1: 0.707\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.2069, Accuracy: 0.914, F1: 0.871\n",
            "    Test  - Loss: 0.2288, Accuracy: 0.897, F1: 0.837\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm_stacked = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_lstm_stacked = train(model_lstm_stacked, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_stacked_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm_stacked, f)\n",
        "\n",
        "torch.save(model_lstm_stacked, \"./models/lstm_stacked_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.4503, Accuracy: 0.733, F1: 0.419\n",
            "    Test  - Loss: 0.2394, Accuracy: 0.897, F1: 0.833\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.1779, Accuracy: 0.921, F1: 0.882\n",
            "    Test  - Loss: 0.1463, Accuracy: 0.934, F1: 0.901\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.1184, Accuracy: 0.948, F1: 0.924\n",
            "    Test  - Loss: 0.1392, Accuracy: 0.936, F1: 0.902\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.0995, Accuracy: 0.955, F1: 0.935\n",
            "    Test  - Loss: 0.1261, Accuracy: 0.946, F1: 0.920\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.0827, Accuracy: 0.965, F1: 0.950\n",
            "    Test  - Loss: 0.1219, Accuracy: 0.949, F1: 0.925\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.0737, Accuracy: 0.967, F1: 0.953\n",
            "    Test  - Loss: 0.1284, Accuracy: 0.946, F1: 0.919\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.0666, Accuracy: 0.971, F1: 0.958\n",
            "    Test  - Loss: 0.1194, Accuracy: 0.950, F1: 0.926\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.0561, Accuracy: 0.974, F1: 0.963\n",
            "    Test  - Loss: 0.1365, Accuracy: 0.947, F1: 0.920\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.0575, Accuracy: 0.975, F1: 0.964\n",
            "    Test  - Loss: 0.1219, Accuracy: 0.950, F1: 0.926\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.0460, Accuracy: 0.980, F1: 0.971\n",
            "    Test  - Loss: 0.1318, Accuracy: 0.949, F1: 0.925\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_bi_lstm = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_bi_lstm = train(model_bi_lstm, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/bi_lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_bi_lstm, f)\n",
        "\n",
        "torch.save(model_bi_lstm, \"./models/bi_lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "\n",
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "Unique label values in training data: {0, 1}\n",
            "Class name mapping: <bound method ClassLabel.int2str of ClassLabel(names=['factual', 'misinfo'], id=None)>\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['factual', 'misinfo']),\n",
        "})\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.reset_index(drop=True)\n",
        "df_misinfo_test = df_misinfo_test.reset_index(drop=True)\n",
        "\n",
        "# Convert train and test data to Hugging Face Dataset\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(dataset_train[2]) \n",
        "\n",
        "# Check the unique values of the 'label' column to ensure the classes are correct\n",
        "unique_labels = set(dataset_train['label'])\n",
        "print(\"Unique label values in training data:\", unique_labels)\n",
        "\n",
        "# Check the mapping of integer labels to class names\n",
        "print(\"Class name mapping:\", dataset_train.features['label'].int2str)\n",
        "\n",
        "# Create a Hugging Face DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "\n",
        "# Print the DatasetDict to check its contents\n",
        "print(f'\\n {dataset_dict}')\n",
        "\n",
        "print(dataset_train[2]) \n",
        "print(dataset_dict['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3416.03 examples/s]\n",
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4338.26 examples/s]\n",
            "                                                  \n",
            "  3%|▎         | 157/4710 [01:05<38:20,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2487318068742752, 'eval_accuracy': 0.9322, 'eval_f1': 0.9019947961838681, 'eval_runtime': 17.8979, 'eval_samples_per_second': 279.363, 'eval_steps_per_second': 8.772, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            "  7%|▋         | 314/4710 [02:08<17:04,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1135304793715477, 'eval_accuracy': 0.9632, 'eval_f1': 0.9476975554292212, 'eval_runtime': 17.9256, 'eval_samples_per_second': 278.93, 'eval_steps_per_second': 8.758, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 10%|█         | 471/4710 [03:11<19:42,  3.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.09109988808631897, 'eval_accuracy': 0.9688, 'eval_f1': 0.9554540262707024, 'eval_runtime': 17.1711, 'eval_samples_per_second': 291.186, 'eval_steps_per_second': 9.143, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 500/4710 [03:19<19:57,  3.52it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.239, 'grad_norm': 2.7938454151153564, 'learning_rate': 4.469214437367304e-05, 'epoch': 3.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 13%|█▎        | 628/4710 [04:13<15:17,  4.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.09751715511083603, 'eval_accuracy': 0.9686, 'eval_f1': 0.954663586485706, 'eval_runtime': 18.0933, 'eval_samples_per_second': 276.346, 'eval_steps_per_second': 8.677, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 17%|█▋        | 785/4710 [05:14<18:13,  3.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.09592843055725098, 'eval_accuracy': 0.9712, 'eval_f1': 0.9585014409221903, 'eval_runtime': 17.188, 'eval_samples_per_second': 290.901, 'eval_steps_per_second': 9.134, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 20%|██        | 942/4710 [06:16<17:19,  3.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.10295017063617706, 'eval_accuracy': 0.9708, 'eval_f1': 0.9583333333333334, 'eval_runtime': 17.2155, 'eval_samples_per_second': 290.437, 'eval_steps_per_second': 9.12, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 1000/4710 [06:33<17:05,  3.62it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0568, 'grad_norm': 5.111602783203125, 'learning_rate': 3.9384288747346076e-05, 'epoch': 6.37}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            " 23%|██▎       | 1099/4710 [07:18<17:03,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1158042699098587, 'eval_accuracy': 0.9698, 'eval_f1': 0.9559124087591241, 'eval_runtime': 17.4745, 'eval_samples_per_second': 286.132, 'eval_steps_per_second': 8.985, 'epoch': 7.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 27%|██▋       | 1256/4710 [08:20<15:35,  3.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.11715184152126312, 'eval_accuracy': 0.9714, 'eval_f1': 0.9584664536741214, 'eval_runtime': 17.0541, 'eval_samples_per_second': 293.184, 'eval_steps_per_second': 9.206, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 1256/4710 [08:20<22:56,  2.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 500.6115, 'train_samples_per_second': 299.634, 'train_steps_per_second': 9.408, 'train_loss': 0.12583174219556675, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./models/transformer_results/tokenizer_config.json',\n",
              " './models/transformer_results/special_tokens_map.json',\n",
              " './models/transformer_results/vocab.txt',\n",
              " './models/transformer_results/added_tokens.json',\n",
              " './models/transformer_results/tokenizer.json')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return bert_tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./transformer_results/bert_uncased\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=False,\n",
        "                                  use_cpu=False)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    bert_uncased,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer after training\n",
        "trainer.save_model(\"./models/transformer_results\")  \n",
        "bert_tokenizer.save_pretrained(\"./models/transformer_results\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "F1 Score: 0.9585014409221903\n",
            "Accuracy: 0.9712\n"
          ]
        }
      ],
      "source": [
        "def batch_predict(model, tokenizer, texts, batch_size=16, device='mps', max_length=512):\n",
        "    \"\"\"\n",
        "    Predict labels for a batch of texts using the specified model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "        model: The pre-trained model (e.g., BERT).\n",
        "        tokenizer: The tokenizer associated with the pre-trained model.\n",
        "        texts: List of input texts to predict.\n",
        "        batch_size: Number of samples per batch.\n",
        "        device: Device to use ('mps', 'cuda', or 'cpu').\n",
        "        max_length: Maximum sequence length for tokenization.\n",
        "\n",
        "    Returns:\n",
        "        List of predicted labels.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for prediction\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            # Tokenize the batch with truncation and padding\n",
        "            tokenized_batch = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Move tokenized inputs to the same device as the model\n",
        "            tokenized_batch = {key: value.to(device) for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(**tokenized_batch)\n",
        "\n",
        "            # Apply softmax to logits and determine predicted labels\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "            # Collect predictions\n",
        "            predictions.extend(predicted_labels.cpu().numpy())  # Move predictions to CPU before storing\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# USAGE\n",
        "# set device\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare dataset\n",
        "disinfo_test_texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "\n",
        "# predictions in batches\n",
        "predicted_labels = batch_predict(bert_uncased, bert_tokenizer, disinfo_test_texts, batch_size=16, device=device)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "could do a load here about why the non-transformers didn't work so well (maybe a tweet is not long enough, didn;t do any hyperparameter tuning, non-decreasing LR means it often overshot; but of interest is that Transformers worked, so will take that moving forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Climate Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from '/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv'...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserScreenName</th>\n",
              "      <th>UserName</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Text</th>\n",
              "      <th>Embedded_text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Retweets</th>\n",
              "      <th>Image link</th>\n",
              "      <th>Tweet URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lauren Boebert</td>\n",
              "      <td>@laurenboebert</td>\n",
              "      <td>2022-01-17T23:32:38.000Z</td>\n",
              "      <td>Lauren Boebert\\n@laurenboebert\\n·\\nJan 18</td>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1,683</td>\n",
              "      <td>2,259</td>\n",
              "      <td>11.7K</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/laurenboebert/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Catherine</td>\n",
              "      <td>@catherine___c</td>\n",
              "      <td>2022-01-17T22:54:02.000Z</td>\n",
              "      <td>Catherine\\n@catherine___c\\n·\\nJan 17</td>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>158</td>\n",
              "      <td>64</td>\n",
              "      <td>762</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/catherine___c/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>king Keith</td>\n",
              "      <td>@KaConfessor</td>\n",
              "      <td>2022-01-17T23:51:41.000Z</td>\n",
              "      <td>king Keith\\n@KaConfessor\\n·\\nJan 18</td>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24</td>\n",
              "      <td>118</td>\n",
              "      <td>159</td>\n",
              "      <td>['https://pbs.twimg.com/ext_tw_video_thumb/148...</td>\n",
              "      <td>https://twitter.com/KaConfessor/status/1483225...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PETRIFIED CLIMATE PARENT</td>\n",
              "      <td>@climate_parent</td>\n",
              "      <td>2022-01-17T21:42:04.000Z</td>\n",
              "      <td>PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...</td>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>50</td>\n",
              "      <td>158</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/climate_parent/status/1483...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thomas Speight</td>\n",
              "      <td>@Thomas_Sp8</td>\n",
              "      <td>2022-01-17T21:10:40.000Z</td>\n",
              "      <td>Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17</td>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "      <td>🅾</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>127</td>\n",
              "      <td>['https://pbs.twimg.com/profile_images/1544171...</td>\n",
              "      <td>https://twitter.com/Thomas_Sp8/status/14831850...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             UserScreenName         UserName                 Timestamp  \\\n",
              "0            Lauren Boebert   @laurenboebert  2022-01-17T23:32:38.000Z   \n",
              "1                 Catherine   @catherine___c  2022-01-17T22:54:02.000Z   \n",
              "2                king Keith     @KaConfessor  2022-01-17T23:51:41.000Z   \n",
              "3  PETRIFIED CLIMATE PARENT  @climate_parent  2022-01-17T21:42:04.000Z   \n",
              "4            Thomas Speight      @Thomas_Sp8  2022-01-17T21:10:40.000Z   \n",
              "\n",
              "                                                Text  \\\n",
              "0          Lauren Boebert\\n@laurenboebert\\n·\\nJan 18   \n",
              "1               Catherine\\n@catherine___c\\n·\\nJan 17   \n",
              "2                king Keith\\n@KaConfessor\\n·\\nJan 18   \n",
              "3  PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...   \n",
              "4             Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17   \n",
              "\n",
              "                                       Embedded_text Emojis Comments  Likes  \\\n",
              "0  The only solution I’ve ever heard the Left pro...    NaN    1,683  2,259   \n",
              "1  Climate change doesn’t cause volcanic eruption...    NaN      158     64   \n",
              "2  Vaccinated tennis ball boy collapses in the te...    NaN       24    118   \n",
              "3  North America has experienced an average winte...    NaN       15     50   \n",
              "4  They're gonna do the same with Climate Change ...      🅾        4     24   \n",
              "\n",
              "  Retweets                                         Image link  \\\n",
              "0    11.7K                                                 []   \n",
              "1      762                                                 []   \n",
              "2      159  ['https://pbs.twimg.com/ext_tw_video_thumb/148...   \n",
              "3      158                                                 []   \n",
              "4      127  ['https://pbs.twimg.com/profile_images/1544171...   \n",
              "\n",
              "                                           Tweet URL  \n",
              "0  https://twitter.com/laurenboebert/status/14832...  \n",
              "1  https://twitter.com/catherine___c/status/14832...  \n",
              "2  https://twitter.com/KaConfessor/status/1483225...  \n",
              "3  https://twitter.com/climate_parent/status/1483...  \n",
              "4  https://twitter.com/Thomas_Sp8/status/14831850...  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of inference dataset: (9050, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The only solution I’ve ever heard the Left pro...\n",
              "1  Climate change doesn’t cause volcanic eruption...\n",
              "2  Vaccinated tennis ball boy collapses in the te...\n",
              "3  North America has experienced an average winte...\n",
              "4  They're gonna do the same with Climate Change ..."
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
        "print(f\"Shape of inference dataset: {df_climate_inference.shape}\")\n",
        "df_climate_inference.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickle file not found. Tokenizing climate tweets...\n",
            "Tokenized climate tweets and saved to file.\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# TRANSFORMER INFERENCE ================================================================\n",
        "\n",
        "# Tokenize climate tweets\n",
        "\n",
        "CLIMATE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(CLIMATE_TOKENISED_DIR):\n",
        "    print(\"Tokenized climate tweets pkl file found. Loading data...\")\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'rb') as f:\n",
        "        climate_tokenised = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle file not found. Tokenizing climate tweets...\")\n",
        "    climate_tokenised = bert_tokenizer(\n",
        "        list(df_climate_inference[\"text\"]),  # Use raw text data\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(climate_tokenised, f)\n",
        "    print(\"Tokenized climate tweets and saved to file.\")\n",
        "\n",
        "# Predict using fine-tuned BERT model\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def predict_climate_tweets(model, tokenized_texts, batch_size=32, device='mps'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(tokenized_texts['input_ids']), batch_size):\n",
        "            batch_input_ids = tokenized_texts['input_ids'][i:i + batch_size].to(device)\n",
        "            batch_attention_mask = tokenized_texts['attention_mask'][i:i + batch_size].to(device)\n",
        "\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "predicted_labels = predict_climate_tweets(bert_uncased, climate_tokenised, batch_size=32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text predicted_label\n",
            "0  The only solution I’ve ever heard the Left pro...  misinformation\n",
            "1  Climate change doesn’t cause volcanic eruption...         factual\n",
            "2  Vaccinated tennis ball boy collapses in the te...  misinformation\n",
            "3  North America has experienced an average winte...         factual\n",
            "4  They're gonna do the same with Climate Change ...  misinformation\n",
            "Predictions saved to /Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df_climate_inference['predicted_label'] = predicted_labels\n",
        "\n",
        "# Replace 0 with 'factual' and 1 with 'misinformation' in the 'predicted_label' column\n",
        "df_climate_inference['predicted_label'] = df_climate_inference['predicted_label'].replace({0: 'factual', 1: 'misinformation'})\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(df_climate_inference.head())\n",
        "\n",
        "# Save predictions to CSV\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv\"\n",
        "df_climate_inference.to_csv(output_path_climate, index=False)\n",
        "print(f\"Predictions saved to {output_path_climate}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribution of Factual vs Misinformation:\n",
            "predicted_label\n",
            "misinformation    5668\n",
            "factual           3382\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count occurrences of each label\n",
        "label_counts = df_climate_inference['predicted_label'].value_counts()\n",
        "\n",
        "print(\"Distribution of Factual vs Misinformation:\")\n",
        "print(label_counts)\n",
        "\n",
        "# If you want to visualise this distribution:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Factual vs Misinformation')\n",
        "plt.xlabel('Label (0: Factual, 1: Misinformation)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "learly overly categorising things as misinfo... I could retune to reduce more false positives, but given that it got 96+% on F1 score, the model is accurate to the dataset; the issue lies with either a) the internal validity of the dataset; b) the external valifity / OOD assumptions when applied to climate tweets.\n",
        "\n",
        "a) looking at the constituent datasets, I can see how this happened...\n",
        "b) climate tweets are different...inference write up: not great, overrepresentation of misinfo - prob because in the training data i used climate was disproportionately in the misinfo category -> out of distribution... generally it identifies factual well, but overly classifies as misinfo\n",
        "\n",
        "was going to do an LDA, but unless the model is better performing, it makes more sense to do xAI to work out where it's performing badly\n",
        "\n",
        "looking at the tweets labeled misinfo, it's clear the model doesn't understand the substantive poits / the science, it's just classifying on style\n",
        "\n",
        "so next steps\n",
        "1) clear up the tweets to remove the other text -> get it closer to the others text\n",
        "2) xAI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Examples of Factual (predicted_label 0):\n",
            "                                                 text predicted_label\n",
            "1   Climate change doesn’t cause volcanic eruption...         factual\n",
            "3   North America has experienced an average winte...         factual\n",
            "6   fucking hell this weather makes me really fuck...         factual\n",
            "11  Ronald Reagan (1989): \"Because changes in the ...         factual\n",
            "12  #Geopolitics could have a material impact on #...         factual\n",
            "13  The climate crisis is front page news in South...         factual\n",
            "17  ... don't be the person that only thinks about...         factual\n",
            "18  Is this guy real? BBC News - Climate change: W...         factual\n",
            "22  Toronto Ontario; the city that thought snow pl...         factual\n",
            "24  They now say we have to invest in our infrastr...         factual\n",
            "\n",
            "10 Examples of Misinformation (predicted_label 1):\n",
            "                                                 text predicted_label\n",
            "0   The only solution I’ve ever heard the Left pro...  misinformation\n",
            "2   Vaccinated tennis ball boy collapses in the te...  misinformation\n",
            "4   They're gonna do the same with Climate Change ...  misinformation\n",
            "5   HELLO AMERICA,\\n\\nWho would have ever thought ...  misinformation\n",
            "7   Great to finally have this important UNESCO/SC...  misinformation\n",
            "8   Climate change is one of the world's most pres...  misinformation\n",
            "9   Can people start questioning the \"Johnson got ...  misinformation\n",
            "10  I’m raising two kids and, ya know, I guess I j...  misinformation\n",
            "14  There is a systemic problem with academic mode...  misinformation\n",
            "15  Every day, I wake up to the news of environmen...  misinformation\n"
          ]
        }
      ],
      "source": [
        "# Extract 10 examples of factual (label == 0)\n",
        "factual_examples = df_climate_inference[df_climate_inference['predicted_label'] == 'factual'].head(10)\n",
        "\n",
        "# Extract 10 examples of misinformation (label == 1)\n",
        "misinfo_examples = df_climate_inference[df_climate_inference['predicted_label'] == 'misinformation'].head(10)\n",
        "\n",
        "print(\"10 Examples of Factual (predicted_label 0):\")\n",
        "print(factual_examples)\n",
        "\n",
        "print(\"\\n10 Examples of Misinformation (predicted_label 1):\")\n",
        "print(misinfo_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline for RNNs etc (need to rework / a lot not needed)\n",
        "'''\n",
        "vocab_idx = vocab_mapping(tokenized_text=climate_tokenised) # is this correct?? ithink not\n",
        " \n",
        "climate_dl = DataLoader(\n",
        "    dataset=list(zip(climate_tokenised,climate_tokenised[\"label\"])), # THIS SHOULD BE BLANK...\n",
        "    batch_size=32, \n",
        "    shuffle=False, \n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "EMBEDDINGS_FILE_PATH_CLIMATE = \"./cache/mapped_pretrained_embeddings_climate.pkl\"\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH_CLIMATE):\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'rb') as f:\n",
        "        embedding_tensor_climate = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings_climate = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings_climate)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# xAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSequenceClassification\n",
        "from captum.attr import IntegratedGradients, Lime\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import HTML, display\n",
        "from matplotlib import cm, colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Model and tokenizer loaded successfully from: ./models/transformer_results\n",
            "Input IDs and embeddings prepared.\n",
            "\n",
            "--- Sample 1 ---\n",
            "\n",
            "Reconstructed Sentence (IG): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (IG):\n",
            "climate (0.0910) change (0.0202) doesn (0.0047) ’ (0.0588) t (0.0972) cause (0.0762) volcanic (0.2702) eruptions (0.1674) . (0.0875) 158 (0.0184) 64 (-0.0012) 76 (0.0100) \n",
            "\n",
            "\n",
            "Reconstructed Sentence (LIME): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (LIME):\n",
            "climate (0.1623) change (0.0000) doesn (-0.0428) ’ (0.0000) t (-0.0575) cause (0.0000) volcanic (0.3155) eruptions (0.2286) . (0.0000) 158 (0.0390) 64 (0.0000) 76 (0.0000) \n",
            "\n",
            "\n",
            "--- Sample 2 ---\n",
            "\n",
            "Reconstructed Sentence (IG): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (IG):\n",
            "climate (0.0869) change (0.0273) doesn (0.0051) ’ (-0.1407) t (-0.0120) cause (-0.0140) volcanic (-0.0036) eruptions (0.0700) . (0.1205) 158 (-0.0547) 64 (0.0316) 76 (0.2475) \n",
            "\n",
            "\n",
            "Reconstructed Sentence (LIME): va tennis ball boy collapses in the tennis court due to climate change . 24 118 159\n",
            "Word-Level Attributions (LIME):\n",
            "va (-0.0070) tennis (0.0196) ball (-0.0194) boy (-0.1523) collapses (-0.0321) in (0.0000) the (0.0000) tennis (0.0173) court (-0.0317) due (0.0000) to (0.0000) climate (0.4382) change (-0.0178) . (0.0000) 24 (0.0001) 118 (0.0222) 159 (0.0000) \n",
            "\n",
            "\n",
            "--- Sample 3 ---\n",
            "\n",
            "Reconstructed Sentence (IG): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (IG):\n",
            "climate (0.1210) change (0.0182) doesn (-0.0078) ’ (0.0198) t (0.0309) cause (0.0459) volcanic (0.0893) eruptions (0.0172) . (0.0303) 158 (0.1079) 64 (0.0226) 76 (0.1735) \n",
            "\n",
            "\n",
            "Reconstructed Sentence (LIME): north america has experienced an average winter , with temperatures and snowfall totals in line with historical trends . do not be fooled . this phenomenon is known as \" as climate change \" . 15 50 158\n",
            "Word-Level Attributions (LIME):\n",
            "north (0.1060) america (0.0000) has (0.0000) experienced (0.0346) an (0.0000) average (0.0000) winter (0.0708) , (0.0000) with (0.0000) temperatures (0.0348) and (0.0000) snowfall (0.1847) totals (-0.0021) in (0.0000) line (0.0000) with (0.0000) historical (0.0000) trends (0.0000) . (0.0000) do (0.0000) not (0.0000) be (0.0000) fooled (-0.0031) . (0.0000) this (0.0000) phenomenon (0.0000) is (0.0000) known (0.0000) as (0.0000) \" (0.0000) as (0.0000) climate (0.0185) change (0.0000) \" (0.0000) . (0.0000) 15 (0.0000) 50 (0.0000) 158 (0.0000) \n",
            "\n",
            "\n",
            "--- Sample 4 ---\n",
            "\n",
            "Reconstructed Sentence (IG): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (IG):\n",
            "climate (0.0001) change (0.0003) doesn (-0.0018) ’ (0.0011) t (0.0011) cause (0.0002) volcanic (0.0006) eruptions (0.0011) . (0.0039) 158 (-0.0005) 64 (0.0009) 76 (-0.0018) \n",
            "\n",
            "\n",
            "Reconstructed Sentence (LIME): they ' re gonna do the same with climate change when it starts to get really bad . quote t @ joey · jan 17 they really want you to fucking live with co . 4 24 127\n",
            "Word-Level Attributions (LIME):\n",
            "they (0.0000) ' (0.0000) re (0.0000) gonna (0.0000) do (0.0000) the (0.0000) same (0.0000) with (0.0000) climate (0.0111) change (0.0000) when (0.0000) it (0.0000) starts (0.0000) to (0.0000) get (0.0000) really (0.0000) bad (0.0000) . (0.0000) quote (0.0000) t (0.0000) @ (0.0000) joey (0.0000) · (0.0000) jan (0.0000) 17 (0.0000) they (0.0000) really (0.0000) want (0.0000) you (0.0000) to (0.0000) fucking (0.0000) live (0.0000) with (0.0000) co (-0.0165) . (0.0000) 4 (0.0000) 24 (0.0000) 127 (0.0000) \n",
            "\n",
            "\n",
            "--- Sample 5 ---\n",
            "\n",
            "Reconstructed Sentence (IG): climate change doesn ’ t cause volcanic eruptions . 158 64 76\n",
            "Word-Level Attributions (IG):\n",
            "climate (0.0137) change (-0.0291) doesn (0.0082) ’ (0.0174) t (0.0090) cause (0.0108) volcanic (0.0127) eruptions (0.0048) . (0.0058) 158 (-0.0013) 64 (0.0008) 76 (0.0043) \n",
            "\n",
            "\n",
            "Reconstructed Sentence (LIME): hello america , who would have ever thought the world could be taken ; by fearing climate change , the common cold , and the flu ? 1 12 22\n",
            "Word-Level Attributions (LIME):\n",
            "hello (-0.1816) america (-0.0440) , (0.0000) who (0.0000) would (0.0000) have (-0.0304) ever (-0.0386) thought (-0.0385) the (0.0000) world (0.0108) could (-0.0126) be (0.0000) taken (0.0000) ; (0.0000) by (0.0000) fearing (0.0000) climate (0.2434) change (0.0000) , (0.0000) the (0.0000) common (0.0000) cold (0.1033) , (0.0000) and (0.0000) the (0.0000) flu (0.0000) ? (0.0000) 1 (0.0000) 12 (0.0000) 22 (0.0000) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Step 1: Set Device === #\n",
        "device = torch.device(\"cpu\")  # Captum doesn't support MPS\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Step 2: Load Tokenized Climate Data === #\n",
        "CLIMATE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "with open(CLIMATE_TOKENISED_DIR, 'rb') as f:\n",
        "    climate_tokenised = pickle.load(f)\n",
        "\n",
        "subset_indices = [1, 2, 3, 4, 5]\n",
        "climate_tokenised_subset = {\n",
        "    \"input_ids\": climate_tokenised[\"input_ids\"][subset_indices],\n",
        "    \"token_type_ids\": climate_tokenised[\"token_type_ids\"][subset_indices],\n",
        "    \"attention_mask\": climate_tokenised[\"attention_mask\"][subset_indices]\n",
        "}\n",
        "\n",
        "# === Step 3: Load Pretrained Model and Tokenizer === #\n",
        "model_path = \"./models/transformer_results\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "bert_uncased_finetuned = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "bert_uncased_finetuned.to(device).eval()  # Set model to evaluation mode\n",
        "print(\"Model and tokenizer loaded successfully from:\", model_path)\n",
        "\n",
        "# === Step 4: Convert Data to Tensors and Prepare Embeddings === #\n",
        "input_ids = torch.clone(climate_tokenised_subset[\"input_ids\"]).detach().to(dtype=torch.long, device=device)\n",
        "attention_mask = torch.clone(climate_tokenised_subset[\"attention_mask\"]).detach().to(dtype=torch.float32, device=device)\n",
        "\n",
        "# Extract embeddings\n",
        "embedding_layer = bert_uncased_finetuned.get_input_embeddings()\n",
        "embeddings = embedding_layer(input_ids).clone().detach().requires_grad_(True).to(device)\n",
        "print(\"Input IDs and embeddings prepared.\")\n",
        "\n",
        "# === Step 5: Define Forward Function === #\n",
        "def forward_func(embeddings, attention_mask=None):\n",
        "    outputs = bert_uncased_finetuned(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
        "    return torch.softmax(outputs.logits, dim=-1)  # Return probabilities\n",
        "\n",
        "# === Step 6: Integrated Gradients Computation === #\n",
        "def compute_integrated_gradients(embeddings, attention_mask, tokenizer, target_class=0, steps=50):\n",
        "    \"\"\"\n",
        "    Compute Integrated Gradients using embeddings and attention mask.\n",
        "    \"\"\"\n",
        "    ig = IntegratedGradients(forward_func)\n",
        "    baseline = torch.zeros_like(embeddings)  # Baseline embeddings\n",
        "    attributions_ig = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "        n_steps=steps,\n",
        "    )\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_ig.sum(dim=-1)[0].detach().cpu().numpy()\n",
        "\n",
        "    # Reconstruct sentence and word-level attributions\n",
        "    sentence, word_attributions = reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "    return sentence, word_attributions\n",
        "\n",
        "# === Step 7: LIME Computation === #\n",
        "\n",
        "def compute_lime(input_ids, attention_mask, tokenizer, model=bert_uncased_finetuned, target_class=0, n_samples=100):\n",
        "\n",
        "    # Define a forward function for Captum\n",
        "    def forward_func(input_ids, attention_mask=None):\n",
        "        input_ids = input_ids.to(dtype=torch.long)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(dtype=torch.float32)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return torch.softmax(outputs.logits, dim=-1)  # Return probabilities\n",
        "    lime = Lime(forward_func)\n",
        "    attributions_lime = lime.attribute(\n",
        "        inputs=input_ids,\n",
        "        n_samples=n_samples,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "    )\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_lime[0].detach().cpu().numpy().flatten()\n",
        "\n",
        "    sentence, word_attributions = reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "    \n",
        "    return sentence, word_attributions\n",
        "\n",
        "\n",
        "# === Step 8: Reconstruct Sentence and Word-Level Attributions === #\n",
        "def reconstruct_sentence(tokens, attributions, tokenizer):\n",
        "    \"\"\"\n",
        "    Reconstruct sentence and aggregate word-level attributions from token attributions.\n",
        "    \"\"\"\n",
        "    word_tokens, word_attributions = [], []\n",
        "    current_word, current_score = \"\", 0.0\n",
        "\n",
        "    for token, score in zip(tokens, attributions):\n",
        "        if token.startswith(\"##\"):\n",
        "            current_score += score\n",
        "        else:\n",
        "            if current_word:  # Save previous word\n",
        "                word_tokens.append(current_word)\n",
        "                word_attributions.append(current_score)\n",
        "            current_word, current_score = token, score\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "        word_tokens.append(current_word)\n",
        "        word_attributions.append(current_score)\n",
        "\n",
        "    # Filter out special tokens like [PAD], [CLS], [SEP]\n",
        "    special_tokens = tokenizer.special_tokens_map.values()\n",
        "    filtered_tokens = [t for t in word_tokens if t not in special_tokens]\n",
        "    filtered_attributions = [\n",
        "        a for t, a in zip(word_tokens, word_attributions) if t not in special_tokens\n",
        "    ]\n",
        "    sentence = \" \".join(filtered_tokens)\n",
        "    return sentence, filtered_attributions\n",
        "\n",
        "# === Step 9: Run and Display Results === #\n",
        "for idx in range(len(climate_tokenised_subset[\"input_ids\"])):\n",
        "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "    \n",
        "    # Get embeddings and attention mask for the sample\n",
        "    sample_embeddings = embeddings[idx].unsqueeze(0)\n",
        "    sample_attention_mask = attention_mask[idx].unsqueeze(0)\n",
        "    \n",
        "    # Integrated Gradients\n",
        "    sentence_ig, word_attributions_ig = compute_integrated_gradients(\n",
        "        embeddings=sample_embeddings,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        steps=50\n",
        "    )\n",
        "    print(f\"\\nReconstructed Sentence (IG): {sentence_ig}\")\n",
        "    print(\"Word-Level Attributions (IG):\")\n",
        "    for word, score in zip(sentence_ig.split(), word_attributions_ig):\n",
        "        print(f\"{word} ({score:.4f}) \", end=\"\")\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    # LIME\n",
        "    sentence_lime, word_attributions_lime = compute_lime(\n",
        "        input_ids=input_ids[idx].unsqueeze(0),\n",
        "        attention_mask=sample_attention_mask,\n",
        "        model=bert_uncased_finetuned,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        n_samples=100\n",
        "    )\n",
        "    print(f\"\\nReconstructed Sentence (LIME): {sentence_lime}\")\n",
        "    print(\"Word-Level Attributions (LIME):\")\n",
        "    for word, score in zip(sentence_lime.split(), word_attributions_lime):\n",
        "        print(f\"{word} ({score:.4f}) \", end=\"\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3>IG Attributions</h3><div style='line-height:1.6;'><span style='background-color:#ff2828; padding:2px; margin:2px;'>climate</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>change</span> <span style='background-color:#ff6464; padding:2px; margin:2px;'>doesn</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>’</span> <span style='background-color:#ff5c5c; padding:2px; margin:2px;'>t</span> <span style='background-color:#ff4848; padding:2px; margin:2px;'>cause</span> <span style='background-color:#ff3434; padding:2px; margin:2px;'>volcanic</span> <span style='background-color:#ff8a8a; padding:2px; margin:2px;'>eruptions</span> <span style='background-color:#ff7e7e; padding:2px; margin:2px;'>.</span> <span style='background-color:#ffcccc; padding:2px; margin:2px;'>158</span> <span style='background-color:#ffb6b6; padding:2px; margin:2px;'>64</span> <span style='background-color:#ff9090; padding:2px; margin:2px;'>76</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>LIME Attributions</h3><div style='line-height:1.6;'><span style='background-color:#3b4cc0; padding:2px; margin:2px;'>hello</span> <span style='background-color:#80a3fa; padding:2px; margin:2px;'>america</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>,</span> <span style='background-color:#7b9ff9; padding:2px; margin:2px;'>who</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>would</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>have</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>ever</span> <span style='background-color:#7b9ff9; padding:2px; margin:2px;'>thought</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>the</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>world</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>could</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>be</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>taken</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>;</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>by</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>fearing</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>climate</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>change</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>,</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>the</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>common</span> <span style='background-color:#f5c0a7; padding:2px; margin:2px;'>cold</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>,</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>and</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>the</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>flu</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>?</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>1</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>12</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>22</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ======== Visualization of Word-Level Attributions (Color-Coded) ======== #\n",
        "\n",
        "def visualize_token_attributions(tokens, attributions, cmap='bwr', title=\"Word-Level Attributions\"):\n",
        "    \"\"\"\n",
        "    Color-code each token by its attribution score and display as HTML in a notebook.\n",
        "    Filters out `[PAD]` tokens during visualization.\n",
        "    \n",
        "    Args:\n",
        "        tokens (list): List of strings (tokens).\n",
        "        attributions (list or np.array): Attribution scores (same length as tokens).\n",
        "        cmap (str): Matplotlib colormap (default: 'bwr' = blue-white-red).\n",
        "        title (str): Title for the visualization.\n",
        "    \"\"\"\n",
        "    # Convert attributions to numpy if not already\n",
        "    attributions = np.array(attributions)\n",
        "    \n",
        "    # Ensure the lengths match\n",
        "    assert len(tokens) == len(attributions), \"Tokens and attributions lengths do not match!\"\n",
        "    \n",
        "    # Filter out [PAD] tokens and their attributions\n",
        "    filtered_tokens, filtered_attributions = [], []\n",
        "    for token, attr in zip(tokens, attributions):\n",
        "        if token not in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]:  # Ignore special tokens\n",
        "            filtered_tokens.append(token)\n",
        "            filtered_attributions.append(attr)\n",
        "    \n",
        "    # Convert filtered attributions to numpy for consistency\n",
        "    filtered_attributions = np.array(filtered_attributions)\n",
        "    \n",
        "    # Normalise attributions for color mapping\n",
        "    norm = colors.Normalize(vmin=filtered_attributions.min(), vmax=filtered_attributions.max())\n",
        "    scalar_map = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "    \n",
        "    # Generate HTML with colored tokens\n",
        "    html_content = f\"<h3>{title}</h3><div style='line-height:1.6;'>\"\n",
        "    for token, score in zip(filtered_tokens, filtered_attributions):\n",
        "        color = colors.rgb2hex(scalar_map.to_rgba(score))\n",
        "        html_content += f\"<span style='background-color:{color}; padding:2px; margin:2px;'>{token}</span> \"\n",
        "    html_content += \"</div>\"\n",
        "    \n",
        "    # Display the HTML content\n",
        "    display(HTML(html_content))\n",
        "\n",
        "# Example: Visualize IG Word-Level Attributions\n",
        "sentence_ig, word_attributions_ig = compute_integrated_gradients(\n",
        "    embeddings=sample_embeddings,\n",
        "    attention_mask=sample_attention_mask,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    target_class=0,\n",
        "    steps=50\n",
        ")\n",
        "tokens_ig = sentence_ig.split()\n",
        "visualize_token_attributions(tokens_ig, word_attributions_ig, cmap='bwr', title=\"IG Attributions\")\n",
        "\n",
        "# Example: Visualize LIME Word-Level Attributions\n",
        "sentence_lime, word_attributions_lime = compute_lime(\n",
        "    input_ids=input_ids[idx].unsqueeze(0),\n",
        "    attention_mask=sample_attention_mask,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    target_class=0,\n",
        "    n_samples=100\n",
        ")\n",
        "tokens_lime = sentence_lime.split()\n",
        "visualize_token_attributions(tokens_lime, word_attributions_lime, cmap='coolwarm', title=\"LIME Attributions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Model and tokenizer loaded successfully from: ./models/transformer_results\n",
            "\n",
            "--- Sample 1 ---\n",
            "Reconstructed Sentence: climate change doesn ’ t cause volcanic eruptions . 158 64 76\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#acacff; padding:2px; margin:2px;'>climate</span> <span style='background-color:#2828ff; padding:2px; margin:2px;'>change</span> <span style='background-color:#0a0aff; padding:2px; margin:2px;'>doesn</span> <span style='background-color:#7070ff; padding:2px; margin:2px;'>’</span> <span style='background-color:#b8b8ff; padding:2px; margin:2px;'>t</span> <span style='background-color:#9292ff; padding:2px; margin:2px;'>cause</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>volcanic</span> <span style='background-color:#ffc0c0; padding:2px; margin:2px;'>eruptions</span> <span style='background-color:#a6a6ff; padding:2px; margin:2px;'>.</span> <span style='background-color:#2424ff; padding:2px; margin:2px;'>158</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>64</span> <span style='background-color:#1414ff; padding:2px; margin:2px;'>76</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (IG): climate (0.0910), change (0.0202), doesn (0.0047), ’ (0.0588), t (0.0972), cause (0.0762), volcanic (0.2702), eruptions (0.1674), . (0.0875), 158 (0.0184), 64 (-0.0012), 76 (0.0100)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#f5c2aa; padding:2px; margin:2px;'>climate</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>change</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>doesn</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>’</span> <span style='background-color:#3b4cc0; padding:2px; margin:2px;'>t</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>cause</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>volcanic</span> <span style='background-color:#dedcdb; padding:2px; margin:2px;'>eruptions</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>.</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>158</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>64</span> <span style='background-color:#6788ee; padding:2px; margin:2px;'>76</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (LIME): climate (0.1786), change (0.0000), doesn (0.0000), ’ (0.0000), t (-0.0521), cause (0.0000), volcanic (0.3131), eruptions (0.1329), . (0.0000), 158 (0.0000), 64 (0.0000), 76 (0.0000)\n",
            "\n",
            "--- Sample 2 ---\n",
            "Reconstructed Sentence: va tennis ball boy collapses in the tennis court due to climate change . 24 118 159\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#ff1212; padding:2px; margin:2px;'>va</span> <span style='background-color:#7474ff; padding:2px; margin:2px;'>tennis</span> <span style='background-color:#ffd0d0; padding:2px; margin:2px;'>ball</span> <span style='background-color:#ffd2d2; padding:2px; margin:2px;'>boy</span> <span style='background-color:#ffc4c4; padding:2px; margin:2px;'>collapses</span> <span style='background-color:#ff5a5a; padding:2px; margin:2px;'>in</span> <span style='background-color:#ff1010; padding:2px; margin:2px;'>the</span> <span style='background-color:#f0f0ff; padding:2px; margin:2px;'>tennis</span> <span style='background-color:#ff9090; padding:2px; margin:2px;'>court</span> <span style='background-color:#ff1a1a; padding:2px; margin:2px;'>due</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>to</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>climate</span> <span style='background-color:#ffc6c6; padding:2px; margin:2px;'>change</span> <span style='background-color:#ff1212; padding:2px; margin:2px;'>.</span> <span style='background-color:#fffafa; padding:2px; margin:2px;'>24</span> <span style='background-color:#e0e0ff; padding:2px; margin:2px;'>118</span> <span style='background-color:#ffd6d6; padding:2px; margin:2px;'>159</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (IG): va (0.1193), tennis (-0.1407), ball (-0.0120), boy (-0.0140), collapses (-0.0036), in (0.0700), the (0.1205), tennis (-0.0547), court (0.0316), due (0.1146), to (0.1328), climate (-0.2225), change (-0.0050), . (0.1194), 24 (-0.0410), 118 (-0.0658), 159 (-0.0165)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#7699f6; padding:2px; margin:2px;'>va</span> <span style='background-color:#93b5fe; padding:2px; margin:2px;'>tennis</span> <span style='background-color:#7b9ff9; padding:2px; margin:2px;'>ball</span> <span style='background-color:#3b4cc0; padding:2px; margin:2px;'>boy</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>collapses</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>in</span> <span style='background-color:#7699f6; padding:2px; margin:2px;'>the</span> <span style='background-color:#82a6fb; padding:2px; margin:2px;'>tennis</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>court</span> <span style='background-color:#779af7; padding:2px; margin:2px;'>due</span> <span style='background-color:#799cf8; padding:2px; margin:2px;'>to</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>climate</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>change</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>.</span> <span style='background-color:#81a4fb; padding:2px; margin:2px;'>24</span> <span style='background-color:#8caffe; padding:2px; margin:2px;'>118</span> <span style='background-color:#7da0f9; padding:2px; margin:2px;'>159</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (LIME): va (-0.0128), tennis (0.0403), ball (-0.0033), boy (-0.1323), collapses (0.0000), in (0.0004), the (-0.0123), tennis (0.0097), court (0.0000), due (-0.0108), to (-0.0069), climate (0.5120), change (0.0000), . (0.0000), 24 (0.0074), 118 (0.0272), 159 (0.0000)\n",
            "\n",
            "--- Sample 3 ---\n",
            "Reconstructed Sentence: north america has experienced an average winter , with temperatures and snowfall totals in line with historical trends . do not be fooled . this phenomenon is known as \" as climate change \" . 15 50 158\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#ff5252; padding:2px; margin:2px;'>north</span> <span style='background-color:#7272ff; padding:2px; margin:2px;'>america</span> <span style='background-color:#2424ff; padding:2px; margin:2px;'>has</span> <span style='background-color:#7878ff; padding:2px; margin:2px;'>experienced</span> <span style='background-color:#9a9aff; padding:2px; margin:2px;'>an</span> <span style='background-color:#c8c8ff; padding:2px; margin:2px;'>average</span> <span style='background-color:#ffb2b2; padding:2px; margin:2px;'>winter</span> <span style='background-color:#7070ff; padding:2px; margin:2px;'>,</span> <span style='background-color:#9898ff; padding:2px; margin:2px;'>with</span> <span style='background-color:#ff7a7a; padding:2px; margin:2px;'>temperatures</span> <span style='background-color:#8080ff; padding:2px; margin:2px;'>and</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>snowfall</span> <span style='background-color:#8a8aff; padding:2px; margin:2px;'>totals</span> <span style='background-color:#6e6eff; padding:2px; margin:2px;'>in</span> <span style='background-color:#4848ff; padding:2px; margin:2px;'>line</span> <span style='background-color:#7272ff; padding:2px; margin:2px;'>with</span> <span style='background-color:#5252ff; padding:2px; margin:2px;'>historical</span> <span style='background-color:#2020ff; padding:2px; margin:2px;'>trends</span> <span style='background-color:#e4e4ff; padding:2px; margin:2px;'>.</span> <span style='background-color:#3e3eff; padding:2px; margin:2px;'>do</span> <span style='background-color:#3838ff; padding:2px; margin:2px;'>not</span> <span style='background-color:#4444ff; padding:2px; margin:2px;'>be</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>fooled</span> <span style='background-color:#9e9eff; padding:2px; margin:2px;'>.</span> <span style='background-color:#2c2cff; padding:2px; margin:2px;'>this</span> <span style='background-color:#3a3aff; padding:2px; margin:2px;'>phenomenon</span> <span style='background-color:#2e2eff; padding:2px; margin:2px;'>is</span> <span style='background-color:#3e3eff; padding:2px; margin:2px;'>known</span> <span style='background-color:#5c5cff; padding:2px; margin:2px;'>as</span> <span style='background-color:#5050ff; padding:2px; margin:2px;'>\"</span> <span style='background-color:#2222ff; padding:2px; margin:2px;'>as</span> <span style='background-color:#ff9292; padding:2px; margin:2px;'>climate</span> <span style='background-color:#1c1cff; padding:2px; margin:2px;'>change</span> <span style='background-color:#8686ff; padding:2px; margin:2px;'>\"</span> <span style='background-color:#8a8aff; padding:2px; margin:2px;'>.</span> <span style='background-color:#4c4cff; padding:2px; margin:2px;'>15</span> <span style='background-color:#4444ff; padding:2px; margin:2px;'>50</span> <span style='background-color:#3232ff; padding:2px; margin:2px;'>158</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (IG): north (0.1210), america (0.0182), has (-0.0078), experienced (0.0198), an (0.0309), average (0.0459), winter (0.0893), , (0.0172), with (0.0303), temperatures (0.1079), and (0.0226), snowfall (0.1480), totals (0.0256), in (0.0168), line (0.0041), with (0.0179), historical (0.0073), trends (-0.0089), . (0.0556), do (0.0006), not (-0.0013), be (0.0029), fooled (-0.0197), . (0.0326), this (-0.0052), phenomenon (-0.0003), is (-0.0044), known (0.0008), as (0.0108), \" (0.0068), as (-0.0083), climate (0.0996), change (-0.0100), \" (0.0243), . (0.0258), 15 (0.0056), 50 (0.0029), 158 (-0.0030)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#a7c5fe; padding:2px; margin:2px;'>north</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>america</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>has</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>experienced</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>an</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>average</span> <span style='background-color:#e0dbd8; padding:2px; margin:2px;'>winter</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>,</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>with</span> <span style='background-color:#6485ec; padding:2px; margin:2px;'>temperatures</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>and</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>snowfall</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>totals</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>in</span> <span style='background-color:#8caffe; padding:2px; margin:2px;'>line</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>with</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>historical</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>trends</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>.</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>do</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>not</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>be</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>fooled</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>.</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>this</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>phenomenon</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>is</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>known</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>as</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>\"</span> <span style='background-color:#5470de; padding:2px; margin:2px;'>as</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>climate</span> <span style='background-color:#3b4cc0; padding:2px; margin:2px;'>change</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>\"</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>.</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>15</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>50</span> <span style='background-color:#4257c9; padding:2px; margin:2px;'>158</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (LIME): north (0.0409), america (0.0000), has (0.0000), experienced (0.0000), an (0.0000), average (0.0000), winter (0.0668), , (0.0000), with (0.0000), temperatures (0.0148), and (0.0000), snowfall (0.1328), totals (0.0000), in (0.0000), line (0.0305), with (0.0000), historical (0.0000), trends (0.0000), . (0.0000), do (0.0000), not (0.0000), be (0.0000), fooled (0.0000), . (0.0000), this (0.0000), phenomenon (0.0000), is (0.0000), known (0.0000), as (0.0000), \" (0.0000), as (0.0081), climate (0.0000), change (-0.0033), \" (0.0000), . (0.0000), 15 (0.0000), 50 (0.0000), 158 (0.0000)\n",
            "\n",
            "--- Sample 4 ---\n",
            "Reconstructed Sentence: they ' re gonna do the same with climate change when it starts to get really bad . quote t @ joey · jan 17 they really want you to fucking live with co . 4 24 127\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#ff9292; padding:2px; margin:2px;'>they</span> <span style='background-color:#ff8e8e; padding:2px; margin:2px;'>'</span> <span style='background-color:#ffb8b8; padding:2px; margin:2px;'>re</span> <span style='background-color:#ff8080; padding:2px; margin:2px;'>gonna</span> <span style='background-color:#ff7e7e; padding:2px; margin:2px;'>do</span> <span style='background-color:#ff9090; padding:2px; margin:2px;'>the</span> <span style='background-color:#ff8a8a; padding:2px; margin:2px;'>same</span> <span style='background-color:#ff7e7e; padding:2px; margin:2px;'>with</span> <span style='background-color:#ff4646; padding:2px; margin:2px;'>climate</span> <span style='background-color:#ff9e9e; padding:2px; margin:2px;'>change</span> <span style='background-color:#ff8484; padding:2px; margin:2px;'>when</span> <span style='background-color:#ffa2a2; padding:2px; margin:2px;'>it</span> <span style='background-color:#ffaaaa; padding:2px; margin:2px;'>starts</span> <span style='background-color:#ff8686; padding:2px; margin:2px;'>to</span> <span style='background-color:#ff9c9c; padding:2px; margin:2px;'>get</span> <span style='background-color:#ffa2a2; padding:2px; margin:2px;'>really</span> <span style='background-color:#ff9e9e; padding:2px; margin:2px;'>bad</span> <span style='background-color:#ff3a3a; padding:2px; margin:2px;'>.</span> <span style='background-color:#ffdcdc; padding:2px; margin:2px;'>quote</span> <span style='background-color:#ffeaea; padding:2px; margin:2px;'>t</span> <span style='background-color:#ff9696; padding:2px; margin:2px;'>@</span> <span style='background-color:#eeeeff; padding:2px; margin:2px;'>joey</span> <span style='background-color:#ff8686; padding:2px; margin:2px;'>·</span> <span style='background-color:#ffc2c2; padding:2px; margin:2px;'>jan</span> <span style='background-color:#ffa8a8; padding:2px; margin:2px;'>17</span> <span style='background-color:#ffd0d0; padding:2px; margin:2px;'>they</span> <span style='background-color:#ffb2b2; padding:2px; margin:2px;'>really</span> <span style='background-color:#ff9494; padding:2px; margin:2px;'>want</span> <span style='background-color:#ffc2c2; padding:2px; margin:2px;'>you</span> <span style='background-color:#ff6c6c; padding:2px; margin:2px;'>to</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>fucking</span> <span style='background-color:#ff9696; padding:2px; margin:2px;'>live</span> <span style='background-color:#ff7272; padding:2px; margin:2px;'>with</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>co</span> <span style='background-color:#ff4e4e; padding:2px; margin:2px;'>.</span> <span style='background-color:#ff8686; padding:2px; margin:2px;'>4</span> <span style='background-color:#ff8686; padding:2px; margin:2px;'>24</span> <span style='background-color:#ff9898; padding:2px; margin:2px;'>127</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (IG): they (0.0001), ' (0.0003), re (-0.0018), gonna (0.0011), do (0.0011), the (0.0002), same (0.0006), with (0.0011), climate (0.0039), change (-0.0005), when (0.0009), it (-0.0007), starts (-0.0011), to (0.0008), get (-0.0004), really (-0.0006), bad (-0.0005), . (0.0046), quote (-0.0036), t (-0.0043), @ (-0.0001), joey (-0.0062), · (0.0008), jan (-0.0023), 17 (-0.0010), they (-0.0029), really (-0.0015), want (0.0000), you (-0.0023), to (0.0021), fucking (0.0075), live (-0.0000), with (0.0017), co (-0.0182), . (0.0036), 4 (0.0008), 24 (0.0007), 127 (-0.0002)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#ea7b60; padding:2px; margin:2px;'>they</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>'</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>re</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>gonna</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>do</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>the</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>same</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>with</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>climate</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>change</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>when</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>it</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>starts</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>to</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>get</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>really</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>bad</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>.</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>quote</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>t</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>@</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>joey</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>·</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>jan</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>17</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>they</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>really</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>want</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>you</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>to</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>fucking</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>live</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>with</span> <span style='background-color:#3b4cc0; padding:2px; margin:2px;'>co</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>.</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>4</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>24</span> <span style='background-color:#ea7b60; padding:2px; margin:2px;'>127</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (LIME): they (0.0000), ' (0.0000), re (0.0000), gonna (0.0000), do (0.0000), the (0.0000), same (0.0000), with (0.0000), climate (0.0043), change (0.0000), when (0.0000), it (0.0000), starts (0.0000), to (0.0000), get (0.0000), really (0.0000), bad (0.0000), . (0.0000), quote (0.0000), t (0.0000), @ (0.0000), joey (0.0000), · (0.0000), jan (0.0000), 17 (0.0000), they (0.0000), really (0.0000), want (0.0000), you (0.0000), to (0.0000), fucking (0.0000), live (0.0000), with (0.0000), co (-0.0193), . (0.0000), 4 (0.0000), 24 (0.0000), 127 (0.0000)\n",
            "\n",
            "--- Sample 5 ---\n",
            "Reconstructed Sentence: hello america , who would have ever thought the world could be taken ; by fearing climate change , the common cold , and the flu ? 1 12 22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#ff2828; padding:2px; margin:2px;'>hello</span> <span style='background-color:#0000ff; padding:2px; margin:2px;'>america</span> <span style='background-color:#ff6464; padding:2px; margin:2px;'>,</span> <span style='background-color:#ff0000; padding:2px; margin:2px;'>who</span> <span style='background-color:#ff5c5c; padding:2px; margin:2px;'>would</span> <span style='background-color:#ff4848; padding:2px; margin:2px;'>have</span> <span style='background-color:#ff3434; padding:2px; margin:2px;'>ever</span> <span style='background-color:#ff8a8a; padding:2px; margin:2px;'>thought</span> <span style='background-color:#ff7e7e; padding:2px; margin:2px;'>the</span> <span style='background-color:#ffcccc; padding:2px; margin:2px;'>world</span> <span style='background-color:#ffb6b6; padding:2px; margin:2px;'>could</span> <span style='background-color:#ffbebe; padding:2px; margin:2px;'>be</span> <span style='background-color:#ff9090; padding:2px; margin:2px;'>taken</span> <span style='background-color:#ff5e5e; padding:2px; margin:2px;'>;</span> <span style='background-color:#ffcccc; padding:2px; margin:2px;'>by</span> <span style='background-color:#ff8888; padding:2px; margin:2px;'>fearing</span> <span style='background-color:#ffe0e0; padding:2px; margin:2px;'>climate</span> <span style='background-color:#f8f8ff; padding:2px; margin:2px;'>change</span> <span style='background-color:#ff4848; padding:2px; margin:2px;'>,</span> <span style='background-color:#ffeeee; padding:2px; margin:2px;'>the</span> <span style='background-color:#acacff; padding:2px; margin:2px;'>common</span> <span style='background-color:#ff4c4c; padding:2px; margin:2px;'>cold</span> <span style='background-color:#ff5c5c; padding:2px; margin:2px;'>,</span> <span style='background-color:#ff9494; padding:2px; margin:2px;'>and</span> <span style='background-color:#ffb2b2; padding:2px; margin:2px;'>the</span> <span style='background-color:#5e5eff; padding:2px; margin:2px;'>flu</span> <span style='background-color:#ff9090; padding:2px; margin:2px;'>?</span> <span style='background-color:#8c8cff; padding:2px; margin:2px;'>1</span> <span style='background-color:#9a9aff; padding:2px; margin:2px;'>12</span> <span style='background-color:#3030ff; padding:2px; margin:2px;'>22</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (IG): hello (0.0137), america (-0.0291), , (0.0082), who (0.0174), would (0.0090), have (0.0108), ever (0.0127), thought (0.0048), the (0.0058), world (-0.0013), could (0.0008), be (0.0001), taken (0.0042), ; (0.0089), by (-0.0013), fearing (0.0050), climate (-0.0029), change (-0.0065), , (0.0108), the (-0.0042), common (-0.0134), cold (0.0104), , (0.0090), and (0.0038), the (0.0011), flu (-0.0205), ? (0.0043), 1 (-0.0162), 12 (-0.0150), 22 (-0.0247)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='line-height:1.6;'><span style='background-color:#3b4cc0; padding:2px; margin:2px;'>hello</span> <span style='background-color:#84a7fc; padding:2px; margin:2px;'>america</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>,</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>who</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>would</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>have</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>ever</span> <span style='background-color:#779af7; padding:2px; margin:2px;'>thought</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>the</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>world</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>could</span> <span style='background-color:#abc8fd; padding:2px; margin:2px;'>be</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>taken</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>;</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>by</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>fearing</span> <span style='background-color:#b40426; padding:2px; margin:2px;'>climate</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>change</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>,</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>the</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>common</span> <span style='background-color:#f2c9b4; padding:2px; margin:2px;'>cold</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>,</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>and</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>the</span> <span style='background-color:#e3d9d3; padding:2px; margin:2px;'>flu</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>?</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>1</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>12</span> <span style='background-color:#b6cefa; padding:2px; margin:2px;'>22</span> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-Level Attributions (LIME): hello (-0.0958), america (-0.0376), , (0.0000), who (0.0000), would (0.0000), have (0.0000), ever (0.0000), thought (-0.0465), the (0.0000), world (0.0000), could (0.0000), be (-0.0077), taken (0.0000), ; (0.0000), by (0.0000), fearing (0.0000), climate (0.1645), change (0.0000), , (0.0000), the (0.0000), common (0.0000), cold (0.0625), , (0.0000), and (0.0000), the (0.0000), flu (0.0408), ? (0.0000), 1 (0.0000), 12 (0.0000), 22 (0.0000)\n"
          ]
        }
      ],
      "source": [
        "# === Workflow Settings === #\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import cm, colors\n",
        "from captum.attr import IntegratedGradients, Lime\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pickle\n",
        "\n",
        "# === Setup === #\n",
        "device = torch.device(\"cpu\")  # Captum doesn't support MPS\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenized climate data\n",
        "CLIMATE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "with open(CLIMATE_TOKENISED_DIR, 'rb') as f:\n",
        "    climate_tokenised = pickle.load(f)\n",
        "\n",
        "subset_indices = [1, 2, 3, 4, 5]\n",
        "climate_tokenised_subset = {\n",
        "    \"input_ids\": climate_tokenised[\"input_ids\"][subset_indices],\n",
        "    \"token_type_ids\": climate_tokenised[\"token_type_ids\"][subset_indices],\n",
        "    \"attention_mask\": climate_tokenised[\"attention_mask\"][subset_indices]\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"./models/transformer_results\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "bert_uncased_finetuned = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "bert_uncased_finetuned.to(device).eval()\n",
        "print(\"Model and tokenizer loaded successfully from:\", model_path)\n",
        "\n",
        "# Prepare data tensors and embeddings\n",
        "input_ids = torch.clone(climate_tokenised_subset[\"input_ids\"]).detach().to(dtype=torch.long, device=device)\n",
        "attention_mask = torch.clone(climate_tokenised_subset[\"attention_mask\"]).detach().to(dtype=torch.float32, device=device)\n",
        "# Extract embeddings directly from input_ids\n",
        "embedding_layer = bert_uncased_finetuned.get_input_embeddings()\n",
        "embeddings = embedding_layer(input_ids).clone().detach().requires_grad_(True).to(device)\n",
        "# Derive tokens from input_ids for reconstruction\n",
        "tokens_list = [bert_tokenizer.convert_ids_to_tokens(ids.cpu().numpy()) for ids in input_ids]\n",
        "\n",
        "\n",
        "# === Helper Functions === #\n",
        "\n",
        "# Reconstruct sentence and attributions\n",
        "def reconstruct_sentence(tokens, attributions, tokenizer):\n",
        "    word_tokens, word_attributions = [], []\n",
        "    current_word, current_score = \"\", 0.0\n",
        "\n",
        "    for token, score in zip(tokens, attributions):\n",
        "        if token.startswith(\"##\"):\n",
        "            current_score += score\n",
        "        else:\n",
        "            if current_word:  # Save previous word\n",
        "                word_tokens.append(current_word)\n",
        "                word_attributions.append(current_score)\n",
        "            current_word, current_score = token, score\n",
        "\n",
        "    if current_word:\n",
        "        word_tokens.append(current_word)\n",
        "        word_attributions.append(current_score)\n",
        "\n",
        "    special_tokens = tokenizer.special_tokens_map.values()\n",
        "    filtered_tokens = [t for t in word_tokens if t not in special_tokens]\n",
        "    filtered_attributions = [\n",
        "        a for t, a in zip(word_tokens, word_attributions) if t not in special_tokens\n",
        "    ]\n",
        "    sentence = \" \".join(filtered_tokens)\n",
        "    return sentence, filtered_attributions\n",
        "\n",
        "# Visualize token attributions\n",
        "def visualize_token_attributions(tokens, attributions, cmap='bwr', title=None):\n",
        "    attributions = np.array(attributions)\n",
        "    assert len(tokens) == len(attributions), \"Tokens and attributions lengths do not match!\"\n",
        "    norm = colors.Normalize(vmin=attributions.min(), vmax=attributions.max())\n",
        "    scalar_map = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "    html_content = f\"<div style='line-height:1.6;'>\"\n",
        "    for token, score in zip(tokens, attributions):\n",
        "        color = colors.rgb2hex(scalar_map.to_rgba(score))\n",
        "        html_content += f\"<span style='background-color:{color}; padding:2px; margin:2px;'>{token}</span> \"\n",
        "    html_content += \"</div>\"\n",
        "    display(HTML(html_content))\n",
        "\n",
        "# Compute Integrated Gradients\n",
        "def compute_integrated_gradients(embeddings, attention_mask, sample_input_ids, tokenizer, target_class=0, steps=50):\n",
        "    def forward_func(embeddings, attention_mask=None):\n",
        "        outputs = bert_uncased_finetuned(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
        "        return torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    ig = IntegratedGradients(forward_func)\n",
        "    baseline = torch.zeros_like(embeddings)\n",
        "    attributions_ig = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "        n_steps=steps,\n",
        "    )\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample_input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_ig.sum(dim=-1)[0].detach().cpu().numpy()\n",
        "    return reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "\n",
        "\n",
        "def compute_lime(sample_input_ids, attention_mask, tokenizer, target_class=0, n_samples=100):\n",
        "    def forward_func(input_ids, attention_mask=None):\n",
        "        outputs = bert_uncased_finetuned(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    lime = Lime(forward_func)\n",
        "    attributions_lime = lime.attribute(\n",
        "        inputs=sample_input_ids,\n",
        "        n_samples=n_samples,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "    )\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample_input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_lime[0].detach().cpu().numpy().flatten()\n",
        "    return reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "\n",
        "# === Main Execution Loop === #\n",
        "for idx in range(len(climate_tokenised_subset[\"input_ids\"])):\n",
        "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "    \n",
        "    # Prepare single-sample embeddings, attention mask, and input_ids\n",
        "    sample_embeddings = embeddings[idx].unsqueeze(0)\n",
        "    sample_attention_mask = attention_mask[idx].unsqueeze(0)\n",
        "    sample_input_ids = input_ids[idx].unsqueeze(0)\n",
        "    \n",
        "    # IG\n",
        "    sentence_ig, word_attributions_ig = compute_integrated_gradients(\n",
        "        embeddings=sample_embeddings,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        sample_input_ids=sample_input_ids,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        steps=50\n",
        "    )\n",
        "    print(f\"Reconstructed Sentence: {sentence_ig}\")\n",
        "    visualize_token_attributions(sentence_ig.split(), word_attributions_ig, cmap='bwr', title=\"IG Attributions\")\n",
        "    print(f\"Word-Level Attributions (IG): {', '.join([f'{word} ({score:.4f})' for word, score in zip(sentence_ig.split(), word_attributions_ig)])}\")\n",
        "\n",
        "    # LIME\n",
        "    sentence_lime, word_attributions_lime = compute_lime(\n",
        "        sample_input_ids=sample_input_ids,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        n_samples=100\n",
        "    )\n",
        "    visualize_token_attributions(sentence_lime.split(), word_attributions_lime, cmap='coolwarm', title=\"LIME Attributions\")\n",
        "    print(f\"Word-Level Attributions (LIME): {', '.join([f'{word} ({score:.4f})' for word, score in zip(sentence_lime.split(), word_attributions_lime)])}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
