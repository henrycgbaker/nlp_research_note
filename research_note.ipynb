{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import warnings\n",
        "import sys\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W5n3aNK6TvsQ",
        "outputId": "3d42ab7b-7a93-4d6b-b948-94acd1764080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "850e0fac-7e02-455c-820b-77035ce993cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nsys.path.append('./aux_scripts')\\nfrom  misinfo_tokenizer import (get_trained_tokenizer,\\n                                batch_tokenize,\\n                                #vocab_mapping,\\n                                custom_analyzer\\n                                )\\nfrom data_loader_helpers import (#Collator,\\n                                 embedding_mapping_fasttext\\n                                 )\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# download pretrained embeddings -----------------------------------------------\n",
        "# for local\n",
        "#fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "# for Colab\n",
        "# !pip install datasets fasttext evaluate\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/Othercomputers/My MacBook Pro/Documents/repositories/nlp/nlp_research_note')\n",
        "#ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "ft = fasttext.load_model(ft_path)\n",
        "\n",
        "# download spacy model for tokenization ----------------------------------------\n",
        "cache_path = './cache/'\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "os.environ['SPACY_DATA'] = cache_path\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# load helper functions & scripts ----------------------------------------------\n",
        "'''\n",
        "sys.path.append('./aux_scripts')\n",
        "from  misinfo_tokenizer import (get_trained_tokenizer,\n",
        "                                batch_tokenize,\n",
        "                                #vocab_mapping,\n",
        "                                custom_analyzer\n",
        "                                )\n",
        "from data_loader_helpers import (#Collator,\n",
        "                                 embedding_mapping_fasttext\n",
        "                                 )\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd7tkDtkpkE",
        "outputId": "bf93912c-46bc-4a76-d5ec-20a88bb636c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found in cache. Downloading...\n",
            "Cache Directory: \n",
            "./cache/huggingface/datasets\n",
            "\n",
            "External Structure: \n",
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "\n",
            "Internal Structure: \n",
            "Dataset({\n",
            "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label'],\n",
            "    num_rows: 92394\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"./cache/huggingface/datasets\")\n",
        "dataset_path = os.path.join(hf_cache_dir, \"roupenminassian\", \"twitter-misinformation\")\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found in cache: {dataset_path}\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\", cache_dir=hf_cache_dir)\n",
        "else:\n",
        "    print(f\"Dataset not found in cache. Downloading...\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "\n",
        "print(f'Cache Directory: \\n{hf_cache_dir}')\n",
        "print(f'\\nExternal Structure: \\n{ds.shape}')\n",
        "print(f'\\nInternal Structure: \\n{ds[\"train\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "mt7Fa1Y6kpkF",
        "outputId": "95cb04b7-3a39-450f-8edc-5774de6bd47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape (5000, 2) \n",
            "\n",
            "\n",
            "0: factual, 1: misinformation\n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.646\n",
            "1    0.354\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.6548\n",
            "1    0.3452\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62905</th>\n",
              "      <td>A sudden there was a flood on the road, and th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48977</th>\n",
              "      <td>No food, no FEMA: Hurricane Michael’s survivor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20691</th>\n",
              "      <td>President Trump visits Florida hospital, prai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32672</th>\n",
              "      <td>During my 2nd week at @sacbee_news, I covered ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70612</th>\n",
              "      <td>Irma is a 5 category hurricane, and your prior...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "62905  A sudden there was a flood on the road, and th...      0\n",
              "48977  No food, no FEMA: Hurricane Michael’s survivor...      0\n",
              "20691   President Trump visits Florida hospital, prai...      1\n",
              "32672  During my 2nd week at @sacbee_news, I covered ...      0\n",
              "70612  Irma is a 5 category hurricane, and your prior...      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATA PARTITIONING =====================================================================\n",
        "ds_cloned = ds.copy()\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "df_misinfo_test = df_misinfo_test.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print('\\n0: factual, 1: misinformation\\n')\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickle files not found. Running tokenization...\n",
            "No pre-fitted tokenizer found or no file specified. Creating a new one...\n",
            "Saving fitted tokenizer to './cache/misinfo_tokenizer.pkl'...\n",
            "Tokenizing Train Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Tokenizing Test Data in Batches...\n",
            "Tokenizing batch 157 of 157...\n",
            "Train inputs tokenised: 5000\n",
            "Test inputs tokenised: 5000\n"
          ]
        }
      ],
      "source": [
        "# DEFINE TOKENIZATION FLOW =====================================================================\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", \n",
        "                 disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "    \"\"\"\n",
        "    Uses the custom_tokenizer, then replaces out-of-vocabulary tokens with <unk>.\n",
        "    \"\"\"\n",
        "    tokens = custom_tokenizer(text)\n",
        "    vocab = trained_tokenizer.vocabulary_\n",
        "    return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "def get_trained_tokenizer(text_series, tokenizer_file=None, min_df=3):\n",
        "    \"\"\"\n",
        "    1) Checks if a previously fitted tokenizer exists in tokenizer_file.\n",
        "    2) If not, create a new CountVectorizer, fit it on 'text_series'.\n",
        "    3) Save the fitted tokenizer if tokenizer_file is provided.\n",
        "    4) Return the tokenizer.\n",
        "    \"\"\"\n",
        "    # If a tokenizer file path is given and exists, load it\n",
        "    if tokenizer_file and os.path.exists(tokenizer_file):\n",
        "        print(f\"Tokenizer file '{tokenizer_file}' found. Loading it...\")\n",
        "        with open(tokenizer_file, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        # Otherwise, create a new one and fit\n",
        "        print(\"No pre-fitted tokenizer found or no file specified. Creating a new one...\")\n",
        "        tokenizer = CountVectorizer(\n",
        "            analyzer=\"word\",\n",
        "            tokenizer=custom_tokenizer,  # We define custom_tokenizer for splitting\n",
        "            lowercase=False,\n",
        "            min_df=min_df\n",
        "        )\n",
        "        tokenizer.fit(text_series)\n",
        "        \n",
        "        # Save the tokenizer if a path was provided\n",
        "        if tokenizer_file:\n",
        "            print(f\"Saving fitted tokenizer to '{tokenizer_file}'...\")\n",
        "            with open(tokenizer_file, 'wb') as f:\n",
        "                pickle.dump(tokenizer, f)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def batch_tokenize(text_series, batch_size, analyzer_func):\n",
        "    \"\"\"\n",
        "    Tokenizes a Pandas Series of text in batches to avoid memory issues.\n",
        "    \"\"\"\n",
        "    tokenized_result = []\n",
        "    total = len(text_series)\n",
        "    num_batches = (total // batch_size) + (1 if total % batch_size != 0 else 0)\n",
        "    \n",
        "    for batch_idx in range(0, total, batch_size):\n",
        "        \n",
        "        # Print progress every 200 batches or at the last batch\n",
        "        if (batch_idx // batch_size + 1) % 200 == 0 or (batch_idx + batch_size >= total):\n",
        "            print(f'Tokenizing batch {batch_idx // batch_size + 1} of {num_batches}...')\n",
        "        \n",
        "        batch_texts = text_series[batch_idx : batch_idx + batch_size]\n",
        "        for text in batch_texts:\n",
        "            tokenized_result.append(analyzer_func(text))\n",
        "    \n",
        "    return tokenized_result\n",
        "\n",
        "# TOKENIZATION ==========================================================================\n",
        "\n",
        "TOKENIZER_DIR = './cache/misinfo_tokenizer.pkl'\n",
        "TRAIN_TOKENISED_DIR = './cache/misinfo_train_tokenised.pkl'\n",
        "TEST_TOKENISED_DIR = './cache/misinfo_test_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(TRAIN_TOKENISED_DIR) and os.path.exists(TEST_TOKENISED_DIR):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load pre-saved tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_train_tokenised = pickle.load(f)\n",
        "    with open(TEST_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_test_tokenised = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    # 1) Train tokenizer\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=TOKENIZER_DIR,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # Build the default analyzer from our tokenizer\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    # 2) Tokenize train data in batches using the built analyzer (trained on train set)\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokenised = batch_tokenize(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        32,\n",
        "        misinfo_tokenizer_analyzer\n",
        "    )\n",
        "    \n",
        "    # 3) Tokenize test data in batches using custom_analyzer (which replaces OOV tokens with <unk>)\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokenised = batch_tokenize(\n",
        "        df_misinfo_test[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer)\n",
        "    )\n",
        "\n",
        "    # Optionally, save the tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokenised, f)\n",
        "    with open(TEST_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokenised, f)\n",
        "\n",
        "print(\"Train inputs tokenised:\", len(misinfo_train_tokenised))\n",
        "print(\"Test inputs tokenised:\", len(misinfo_test_tokenised))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 46425\n",
            "Vocab example: [('<pad>', 0), ('<unk>', 1), ('the', 2), (',', 3), ('.', 4), ('to', 5), ('of', 6), ('and', 7), ('a', 8), ('in', 9)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokenised)\n",
        "\n",
        "print(f\"Vocab size: {len(vocab_idx)}\")\n",
        "print(f\"Vocab example: {list(vocab_idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(data, include_lengths=True):\n",
        "    text_list, label_list, lengths = [], [], []\n",
        "    for _text, _label in data:\n",
        "        # Integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    # Padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    if include_lengths:\n",
        "        return padded_text_list, label_list, lengths\n",
        "    else:\n",
        "        return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300 # too long for classic RNN\n",
        "batch_size = 32\n",
        "\n",
        "# standard dls with collate_fn\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "# dls w/o collate_fn for CNNs:\n",
        "train_dl_cnn = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))\n",
        "\n",
        "test_dl_cnn = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                         batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d06330da-b6a8-40c7-a9b4-f4bda78aaa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\n",
            "Saved embeddings to ./cache/mapped_pretrained_embeddings.pkl. Shape: torch.Size([46425, 300])\n"
          ]
        }
      ],
      "source": [
        "# EMBEDDING MAPPING =====================================================================\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "EMBEDDINGS_FILE_PATH = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH):\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_dl, test_dl, use_lengths=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss_train\": [],\n",
        "        \"loss_test\": [],\n",
        "        \"accuracy_train\": [],\n",
        "        \"accuracy_test\": [],\n",
        "        \"f1_train\": [],\n",
        "        \"f1_test\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch in tqdm(train_dl, desc=\"Training\", leave=False):\n",
        "            if use_lengths:\n",
        "                x_batch, y_batch, lengths = batch\n",
        "                x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                pred = model(x_batch, lengths)[:, 0]  # Include lengths for RNNs/LSTMs\n",
        "            else:\n",
        "                x_batch, y_batch = batch\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                pred = model(x_batch)[:, 0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item() * y_batch.size(0)\n",
        "            preds = (pred >= 0.5).float()\n",
        "            train_correct += (preds == y_batch).float().sum().item()\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_train\"].append(train_loss / len(train_dl.dataset))\n",
        "        metrics[\"accuracy_train\"].append(train_correct / len(train_dl.dataset))\n",
        "        metrics[\"f1_train\"].append(f1_score(all_train_labels, all_train_preds))\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss, test_correct = 0, 0\n",
        "        all_test_preds, all_test_labels = [], []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dl, desc=\"Evaluating\", leave=False):\n",
        "                if use_lengths:\n",
        "                    x_batch, y_batch, lengths = batch\n",
        "                    x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                    pred = model(x_batch, lengths)[:, 0]\n",
        "                else:\n",
        "                    x_batch, y_batch = batch\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                    pred = model(x_batch)[:, 0]\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # Metrics\n",
        "                test_loss += loss.item() * y_batch.size(0)\n",
        "                preds = (pred >= 0.5).float()\n",
        "                test_correct += (preds == y_batch).float().sum().item()\n",
        "                all_test_preds.extend(preds.cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_test\"].append(test_loss / len(test_dl.dataset))\n",
        "        metrics[\"accuracy_test\"].append(test_correct / len(test_dl.dataset))\n",
        "        metrics[\"f1_test\"].append(f1_score(all_test_labels, all_test_preds))\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Loss: {metrics['loss_train'][-1]:.4f}, Accuracy: {metrics['accuracy_train'][-1]:.3f}, F1: {metrics['f1_train'][-1]:.3f}\")\n",
        "        print(f\"    Test  - Loss: {metrics['loss_test'][-1]:.4f}, Accuracy: {metrics['accuracy_test'][-1]:.3f}, F1: {metrics['f1_test'][-1]:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8qQ1LhWHrXP-",
        "outputId": "3469629f-d284-41b2-fac6-f3c8c480c93a"
      },
      "outputs": [],
      "source": [
        "# MODEL BUILDING ================================================================\n",
        "\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = TextClassificationModel(embedding_tensor=embedding_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 4\u001b[0m hist_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/cnn_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_cnn, f)\n",
            "Cell \u001b[0;32mIn[41], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     76\u001b[0m preds \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     77\u001b[0m test_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "hist_cnn = train(model_cnn, num_epochs, train_dl_cnn, test_dl_cnn, use_lengths=False)\n",
        "\n",
        "with open(\"./models/train_hist/cnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_cnn, f)\n",
        "    \n",
        "torch.save(model_cnn, \"./models/cnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.5884, Accuracy: 0.666, F1: 0.168\n",
            "    Test  - Loss: 0.5511, Accuracy: 0.746, F1: 0.617\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     30\u001b[0m model_rnn \u001b[38;5;241m=\u001b[39m RNNTextClassificationModel(embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor)\n\u001b[0;32m---> 31\u001b[0m hist_rnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fluctuating f1 scores, exploding gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/rnn_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_rnn, f)\n",
            "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_rnn = train(model_rnn, num_epochs, train_dl, test_dl, use_lengths=True) # fluctuating f1 scores, exploding gradients\n",
        "\n",
        "with open(\"./models/train_hist/rnn_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_rnn, f)\n",
        "\n",
        "torch.save(model_rnn, \"./models/rnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     29\u001b[0m model_lstm \u001b[38;5;241m=\u001b[39m LSTMTextClassificationModel(embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor)\n\u001b[0;32m---> 31\u001b[0m hist_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/train_hist/lstm_hist.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(hist_lstm, f)\n",
            "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl, use_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_lstm = train(model_lstm, num_epochs=10, train_dl=train_dl, test_dl=test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm, f)\n",
        "    \n",
        "torch.save(model_lstm, \"./models/lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.6151, Accuracy: 0.646, F1: 0.001\n",
            "    Test  - Loss: 0.5594, Accuracy: 0.655, F1: 0.000\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.5095, Accuracy: 0.699, F1: 0.350\n",
            "    Test  - Loss: 0.4939, Accuracy: 0.717, F1: 0.372\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.4619, Accuracy: 0.739, F1: 0.528\n",
            "    Test  - Loss: 0.4533, Accuracy: 0.755, F1: 0.536\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.4459, Accuracy: 0.767, F1: 0.580\n",
            "    Test  - Loss: 0.6065, Accuracy: 0.685, F1: 0.172\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.4907, Accuracy: 0.731, F1: 0.458\n",
            "    Test  - Loss: 0.4248, Accuracy: 0.805, F1: 0.727\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.3840, Accuracy: 0.816, F1: 0.721\n",
            "    Test  - Loss: 0.3956, Accuracy: 0.821, F1: 0.755\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.3288, Accuracy: 0.853, F1: 0.776\n",
            "    Test  - Loss: 0.3073, Accuracy: 0.870, F1: 0.799\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.3954, Accuracy: 0.796, F1: 0.681\n",
            "    Test  - Loss: 0.3181, Accuracy: 0.861, F1: 0.792\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.3182, Accuracy: 0.867, F1: 0.810\n",
            "    Test  - Loss: 0.4231, Accuracy: 0.796, F1: 0.737\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.2768, Accuracy: 0.877, F1: 0.812\n",
            "    Test  - Loss: 0.3157, Accuracy: 0.832, F1: 0.694\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm_stacked = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_lstm_stacked = train(model_lstm_stacked, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/lstm_stacked_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_lstm_stacked, f)\n",
        "\n",
        "torch.save(model_lstm_stacked, \"./models/lstm_stacked_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Summary:\n",
            "    Train - Loss: 0.4323, Accuracy: 0.742, F1: 0.447\n",
            "    Test  - Loss: 0.2439, Accuracy: 0.889, F1: 0.813\n",
            "Epoch 2/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 Summary:\n",
            "    Train - Loss: 0.1747, Accuracy: 0.918, F1: 0.876\n",
            "    Test  - Loss: 0.1507, Accuracy: 0.928, F1: 0.887\n",
            "Epoch 3/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 Summary:\n",
            "    Train - Loss: 0.1223, Accuracy: 0.944, F1: 0.917\n",
            "    Test  - Loss: 0.1579, Accuracy: 0.940, F1: 0.913\n",
            "Epoch 4/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 Summary:\n",
            "    Train - Loss: 0.1041, Accuracy: 0.952, F1: 0.931\n",
            "    Test  - Loss: 0.1719, Accuracy: 0.910, F1: 0.855\n",
            "Epoch 5/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 Summary:\n",
            "    Train - Loss: 0.0892, Accuracy: 0.959, F1: 0.941\n",
            "    Test  - Loss: 0.1305, Accuracy: 0.944, F1: 0.917\n",
            "Epoch 6/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 Summary:\n",
            "    Train - Loss: 0.0820, Accuracy: 0.965, F1: 0.949\n",
            "    Test  - Loss: 0.1294, Accuracy: 0.945, F1: 0.917\n",
            "Epoch 7/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 Summary:\n",
            "    Train - Loss: 0.0916, Accuracy: 0.960, F1: 0.942\n",
            "    Test  - Loss: 0.1187, Accuracy: 0.948, F1: 0.924\n",
            "Epoch 8/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 Summary:\n",
            "    Train - Loss: 0.0672, Accuracy: 0.969, F1: 0.956\n",
            "    Test  - Loss: 0.1367, Accuracy: 0.938, F1: 0.906\n",
            "Epoch 9/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 Summary:\n",
            "    Train - Loss: 0.0805, Accuracy: 0.963, F1: 0.947\n",
            "    Test  - Loss: 0.1324, Accuracy: 0.945, F1: 0.919\n",
            "Epoch 10/10 Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Evaluating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 Summary:\n",
            "    Train - Loss: 0.0668, Accuracy: 0.971, F1: 0.958\n",
            "    Test  - Loss: 0.1481, Accuracy: 0.940, F1: 0.910\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_bi_lstm = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_bi_lstm = train(model_bi_lstm, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "\n",
        "with open(\"./models/train_hist/bi_lstm_hist.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hist_bi_lstm, f)\n",
        "\n",
        "torch.save(model_bi_lstm, \"./models/bi_lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "\n",
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "Unique label values in training data: {0, 1}\n",
            "Class name mapping: <bound method ClassLabel.int2str of ClassLabel(names=['factual', 'misinfo'], id=None)>\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n",
            "{'text': ' President Trump visits Florida hospital, praises first responders following school shooting:  It s very sad something like that could happen, but the job the doctors did, the nurses, the hospital, the first responders, law enforcement   really incredible. A White House statement said that the Trumps were visiting  to pay their respects and thank the medical professionals for their life-saving assistance  in response to shooting.NEW: \"The job they ve done is incredible,\" Pres. Trump says of doctors, first responders as he and first lady Melania Trump meet Parkland shooting victims at Broward Health North Hospital https://t.co/n6Ltn0H0nn pic.twitter.com/gKN8aHbRz4  CBS News (@CBSNews) February 17, 2018POTUS AND FLOTUS THEN MET WITH FLORIDA LAW ENFORCEMENT:After President Trump and First Lady Melania visited with victims, families and the incredible medical teams at Broward Health North   they headed to thank the amazing law enforcement officers at the @BrowardSheriff s Department, where a roundtable is now underway. pic.twitter.com/JNYrVovGl6  Dan Scavino Jr. (@Scavino45) February 17, 2018THE PRESIDENT BRIEFLY SPOKE WITH REPORTERS AFTER THE MEETING WITH LAW ENFORCEMENT:Moments ago, @POTUS and @FLOTUS met with Florida law enforcement officials at the @browardsheriff\\'s Office. pic.twitter.com/jbMU6aIkUA  Fox News (@FoxNews) February 17, 2018At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making  incredible recovery.  And says doctors and first responders had done an incredible job.At Broward Health North Hospital, Pres and Mrs Trump visited with school shooting patients and medical personnel. Pres said students making \"incredible recovery.\" And says doctors and first responders had done an incredible job. pic.twitter.com/iLomsQXknf  Mark Knoller (@markknoller) February 17, 2018The president briefly spoke with reporters:\"It\\'s very sad something like that could happen,\" Pres Trump said of the Parkland school shooting after visiting wounded students in the hospital. Hails work of medical staff and first responders. Declines to respond to question about gun control. pic.twitter.com/hlvMPwhK1M  Mark Knoller (@markknoller) February 17, 2018PRESIDENT TRUMP POSTED PHOTOS FROM HIS VISIT:Our entire Nation, w/one heavy heart, continues to pray for the victims & their families in Parkland, FL. To teachers, law enforcement, first responders & medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG pic.twitter.com/ti791dENTy  Donald J. Trump (@realDonaldTrump) February 17, 2018', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['factual', 'misinfo']),\n",
        "})\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.reset_index(drop=True)\n",
        "df_misinfo_test = df_misinfo_test.reset_index(drop=True)\n",
        "\n",
        "# Convert train and test data to Hugging Face Dataset\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(dataset_train[2]) \n",
        "\n",
        "# Check the unique values of the 'label' column to ensure the classes are correct\n",
        "unique_labels = set(dataset_train['label'])\n",
        "print(\"Unique label values in training data:\", unique_labels)\n",
        "\n",
        "# Check the mapping of integer labels to class names\n",
        "print(\"Class name mapping:\", dataset_train.features['label'].int2str)\n",
        "\n",
        "# Create a Hugging Face DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "\n",
        "# Print the DatasetDict to check its contents\n",
        "print(f'\\n {dataset_dict}')\n",
        "\n",
        "print(dataset_train[2]) \n",
        "print(dataset_dict['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3992.74 examples/s]\n",
            "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4752.76 examples/s]\n",
            "  3%|▎         | 156/4710 [00:43<20:46,  3.65it/s]\n",
            "  3%|▎         | 157/4710 [01:00<20:46,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1574542075395584, 'eval_accuracy': 0.9654, 'eval_f1': 0.9487103468722206, 'eval_runtime': 16.9765, 'eval_samples_per_second': 294.525, 'eval_steps_per_second': 9.248, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            "  7%|▋         | 314/4710 [02:02<20:36,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14587999880313873, 'eval_accuracy': 0.9716, 'eval_f1': 0.9589832466782207, 'eval_runtime': 17.6708, 'eval_samples_per_second': 282.952, 'eval_steps_per_second': 8.885, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 10%|█         | 471/4710 [03:06<19:45,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.18625576794147491, 'eval_accuracy': 0.966, 'eval_f1': 0.9497635933806147, 'eval_runtime': 18.5116, 'eval_samples_per_second': 270.101, 'eval_steps_per_second': 8.481, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 500/4710 [03:14<19:36,  3.58it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0182, 'grad_norm': 0.07430180162191391, 'learning_rate': 4.469214437367304e-05, 'epoch': 3.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 13%|█▎        | 628/4710 [04:08<18:42,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.23330871760845184, 'eval_accuracy': 0.9602, 'eval_f1': 0.9403298350824588, 'eval_runtime': 17.7036, 'eval_samples_per_second': 282.428, 'eval_steps_per_second': 8.868, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 17%|█▋        | 785/4710 [05:10<17:58,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1902061551809311, 'eval_accuracy': 0.9676, 'eval_f1': 0.9526315789473684, 'eval_runtime': 17.8866, 'eval_samples_per_second': 279.539, 'eval_steps_per_second': 8.778, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 785/4710 [05:10<25:52,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 310.5572, 'train_samples_per_second': 483.003, 'train_steps_per_second': 15.166, 'train_loss': 0.014178895039163577, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./models/transformer_results/tokenizer_config.json',\n",
              " './models/transformer_results/special_tokens_map.json',\n",
              " './models/transformer_results/vocab.txt',\n",
              " './models/transformer_results/added_tokens.json',\n",
              " './models/transformer_results/tokenizer.json')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return bert_tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./transformer_results/bert_uncased\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=False,\n",
        "                                  use_cpu=False)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    bert_uncased,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer after training\n",
        "trainer.save_model(\"./models/transformer_results\")  \n",
        "bert_tokenizer.save_pretrained(\"./models/transformer_results\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "F1 Score: 0.9589832466782207\n",
            "Accuracy: 0.9716\n"
          ]
        }
      ],
      "source": [
        "def batch_predict(model, tokenizer, texts, batch_size=16, device='mps', max_length=512):\n",
        "    \"\"\"\n",
        "    Predict labels for a batch of texts using the specified model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "        model: The pre-trained model (e.g., BERT).\n",
        "        tokenizer: The tokenizer associated with the pre-trained model.\n",
        "        texts: List of input texts to predict.\n",
        "        batch_size: Number of samples per batch.\n",
        "        device: Device to use ('mps', 'cuda', or 'cpu').\n",
        "        max_length: Maximum sequence length for tokenization.\n",
        "\n",
        "    Returns:\n",
        "        List of predicted labels.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for prediction\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            # Tokenize the batch with truncation and padding\n",
        "            tokenized_batch = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Move tokenized inputs to the same device as the model\n",
        "            tokenized_batch = {key: value.to(device) for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(**tokenized_batch)\n",
        "\n",
        "            # Apply softmax to logits and determine predicted labels\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "            # Collect predictions\n",
        "            predictions.extend(predicted_labels.cpu().numpy())  # Move predictions to CPU before storing\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# USAGE\n",
        "# set device\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare dataset\n",
        "disinfo_test_texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "\n",
        "# predictions in batches\n",
        "predicted_labels = batch_predict(bert_uncased, bert_tokenizer, disinfo_test_texts, batch_size=16, device=device)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Climate Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from '/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv'...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserScreenName</th>\n",
              "      <th>UserName</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Text</th>\n",
              "      <th>Embedded_text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Retweets</th>\n",
              "      <th>Image link</th>\n",
              "      <th>Tweet URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lauren Boebert</td>\n",
              "      <td>@laurenboebert</td>\n",
              "      <td>2022-01-17T23:32:38.000Z</td>\n",
              "      <td>Lauren Boebert\\n@laurenboebert\\n·\\nJan 18</td>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1,683</td>\n",
              "      <td>2,259</td>\n",
              "      <td>11.7K</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/laurenboebert/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Catherine</td>\n",
              "      <td>@catherine___c</td>\n",
              "      <td>2022-01-17T22:54:02.000Z</td>\n",
              "      <td>Catherine\\n@catherine___c\\n·\\nJan 17</td>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>158</td>\n",
              "      <td>64</td>\n",
              "      <td>762</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/catherine___c/status/14832...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>king Keith</td>\n",
              "      <td>@KaConfessor</td>\n",
              "      <td>2022-01-17T23:51:41.000Z</td>\n",
              "      <td>king Keith\\n@KaConfessor\\n·\\nJan 18</td>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24</td>\n",
              "      <td>118</td>\n",
              "      <td>159</td>\n",
              "      <td>['https://pbs.twimg.com/ext_tw_video_thumb/148...</td>\n",
              "      <td>https://twitter.com/KaConfessor/status/1483225...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PETRIFIED CLIMATE PARENT</td>\n",
              "      <td>@climate_parent</td>\n",
              "      <td>2022-01-17T21:42:04.000Z</td>\n",
              "      <td>PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...</td>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>50</td>\n",
              "      <td>158</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/climate_parent/status/1483...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thomas Speight</td>\n",
              "      <td>@Thomas_Sp8</td>\n",
              "      <td>2022-01-17T21:10:40.000Z</td>\n",
              "      <td>Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17</td>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "      <td>🅾</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>127</td>\n",
              "      <td>['https://pbs.twimg.com/profile_images/1544171...</td>\n",
              "      <td>https://twitter.com/Thomas_Sp8/status/14831850...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             UserScreenName         UserName                 Timestamp  \\\n",
              "0            Lauren Boebert   @laurenboebert  2022-01-17T23:32:38.000Z   \n",
              "1                 Catherine   @catherine___c  2022-01-17T22:54:02.000Z   \n",
              "2                king Keith     @KaConfessor  2022-01-17T23:51:41.000Z   \n",
              "3  PETRIFIED CLIMATE PARENT  @climate_parent  2022-01-17T21:42:04.000Z   \n",
              "4            Thomas Speight      @Thomas_Sp8  2022-01-17T21:10:40.000Z   \n",
              "\n",
              "                                                Text  \\\n",
              "0          Lauren Boebert\\n@laurenboebert\\n·\\nJan 18   \n",
              "1               Catherine\\n@catherine___c\\n·\\nJan 17   \n",
              "2                king Keith\\n@KaConfessor\\n·\\nJan 18   \n",
              "3  PETRIFIED CLIMATE PARENT\\n@climate_parent\\n·\\n...   \n",
              "4             Thomas Speight\\n@Thomas_Sp8\\n·\\nJan 17   \n",
              "\n",
              "                                       Embedded_text Emojis Comments  Likes  \\\n",
              "0  The only solution I’ve ever heard the Left pro...    NaN    1,683  2,259   \n",
              "1  Climate change doesn’t cause volcanic eruption...    NaN      158     64   \n",
              "2  Vaccinated tennis ball boy collapses in the te...    NaN       24    118   \n",
              "3  North America has experienced an average winte...    NaN       15     50   \n",
              "4  They're gonna do the same with Climate Change ...      🅾        4     24   \n",
              "\n",
              "  Retweets                                         Image link  \\\n",
              "0    11.7K                                                 []   \n",
              "1      762                                                 []   \n",
              "2      159  ['https://pbs.twimg.com/ext_tw_video_thumb/148...   \n",
              "3      158                                                 []   \n",
              "4      127  ['https://pbs.twimg.com/profile_images/1544171...   \n",
              "\n",
              "                                           Tweet URL  \n",
              "0  https://twitter.com/laurenboebert/status/14832...  \n",
              "1  https://twitter.com/catherine___c/status/14832...  \n",
              "2  https://twitter.com/KaConfessor/status/1483225...  \n",
              "3  https://twitter.com/climate_parent/status/1483...  \n",
              "4  https://twitter.com/Thomas_Sp8/status/14831850...  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of inference dataset: (9050, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>North America has experienced an average winte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They're gonna do the same with Climate Change ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The only solution I’ve ever heard the Left pro...\n",
              "1  Climate change doesn’t cause volcanic eruption...\n",
              "2  Vaccinated tennis ball boy collapses in the te...\n",
              "3  North America has experienced an average winte...\n",
              "4  They're gonna do the same with Climate Change ..."
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
        "print(f\"Shape of inference dataset: {df_climate_inference.shape}\")\n",
        "df_climate_inference.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized climate tweets pkl file found. Loading data...\n",
            "Using device: mps\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m             predictions\u001b[38;5;241m.\u001b[39mextend(predicted_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m---> 45\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_climate_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_uncased\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclimate_tokenised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Replace 0 with 'factual' and 1 with 'misinformation' in the 'predicted_label' column\u001b[39;00m\n\u001b[1;32m     48\u001b[0m df_climate_inference[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_climate_inference[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmisinformation\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
            "Cell \u001b[0;32mIn[62], line 38\u001b[0m, in \u001b[0;36mpredict_climate_tweets\u001b[0;34m(model, tokenized_texts, batch_size, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_input_ids \u001b[38;5;241m=\u001b[39m tokenized_texts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i \u001b[38;5;241m+\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m batch_attention_mask \u001b[38;5;241m=\u001b[39m tokenized_texts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i \u001b[38;5;241m+\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1108\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m   1102\u001b[0m             attention_mask,\n\u001b[1;32m   1103\u001b[0m             input_shape,\n\u001b[1;32m   1104\u001b[0m             embedding_output,\n\u001b[1;32m   1105\u001b[0m             past_key_values_length,\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1108\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:444\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    441\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TRANSFORMER INFERENCE ================================================================\n",
        "\n",
        "# Tokenize climate tweets\n",
        "\n",
        "CLIMATE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(CLIMATE_TOKENISED_DIR):\n",
        "    print(\"Tokenized climate tweets pkl file found. Loading data...\")\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'rb') as f:\n",
        "        climate_tokenised = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle file not found. Tokenizing climate tweets...\")\n",
        "    climate_tokenised = bert_tokenizer(\n",
        "        list(df_climate_inference[\"text\"]),  # Use raw text data\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with open(CLIMATE_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(climate_tokenised, f)\n",
        "    print(\"Tokenized climate tweets and saved to file.\")\n",
        "\n",
        "# Predict using fine-tuned BERT model\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def predict_climate_tweets(model, tokenized_texts, batch_size=32, device='mps'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(tokenized_texts['input_ids']), batch_size):\n",
        "            batch_input_ids = tokenized_texts['input_ids'][i:i + batch_size].to(device)\n",
        "            batch_attention_mask = tokenized_texts['attention_mask'][i:i + batch_size].to(device)\n",
        "\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "predicted_labels = predict_climate_tweets(bert_uncased, climate_tokenised, batch_size=32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text predicted_label\n",
            "0  The only solution I’ve ever heard the Left pro...  misinformation\n",
            "1  Climate change doesn’t cause volcanic eruption...         factual\n",
            "2  Vaccinated tennis ball boy collapses in the te...  misinformation\n",
            "3  North America has experienced an average winte...         factual\n",
            "4  They're gonna do the same with Climate Change ...  misinformation\n",
            "Predictions saved to /Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv.\n"
          ]
        }
      ],
      "source": [
        "# Replace 0 with 'factual' and 1 with 'misinformation' in the 'predicted_label' column\n",
        "df_climate_inference['predicted_label'] = df_climate_inference['predicted_label'].replace({0: 'factual', 1: 'misinformation'})\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(df_climate_inference.head())\n",
        "\n",
        "# Save predictions to CSV\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv\"\n",
        "df_climate_inference.to_csv(output_path_climate, index=False)\n",
        "print(f\"Predictions saved to {output_path_climate}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribution of Factual vs Misinformation:\n",
            "predicted_label\n",
            "1    6011\n",
            "0    3039\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHCCAYAAAAO4dYCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIeklEQVR4nO3deVgVdf//8dcRZRE94AaIomB2q7jkmmCLmSQZppaWprkvadCdWm7l7XbfZrflWi5ZKZaaS4uZO7l2G24U5pJ+szRMBSoFXEFhfn90MT+PoAIhh5rn47rOdXk+85mZ95xzxvNi5jNzbIZhGAIAALCwEs4uAAAAwNkIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRCj2xo8fL5vNViTreuihh/TQQw+Zz7dt2yabzaaPP/64SNbfu3dvBQYGFsm6CurChQvq37+//Pz8ZLPZNGTIEGeXVOSK8jN5J0RHR8tms+nEiRP5ms9ms2n8+PEFWue1a9c0YsQIBQQEqESJEurYsWOBllMcFfT1RPFCIEKRyv6PI/vh7u4uf39/hYeHa9asWTp//nyhrOf06dMaP3684uPjC2V5hak415YXr732mqKjozV48GB9+OGH6tGjx037BgYGOrzf1z+uXLlSqHUtXbpUM2bMKNRlFncPPfSQbDab7r777lynx8TEmK93UYX6m1mwYIHeeOMNde7cWYsWLdLQoUOdWk9BvPbaa1q1apWzy8AdUtLZBcCaJk6cqKCgIF29elWJiYnatm2bhgwZomnTpmn16tVq0KCB2XfMmDEaNWpUvpZ/+vRpTZgwQYGBgWrYsGGe59u0aVO+1lMQt6rt3XffVVZW1h2v4c/YsmWLQkJCNG7cuDz1b9iwoV566aUc7a6uroVa19KlS3Xw4EHLHbFyd3fXsWPHtGfPHt17770O05YsWSJ3d/cc4bNHjx7q2rWr3Nzc8rWuy5cvq2TJgn1tbNmyRVWqVNH06dMLNH9x8Nprr6lz5845jm4V9PVE8UIgglO0bdtWTZs2NZ+PHj1aW7ZsUbt27dS+fXt9//338vDwkCSVLFmywP8J59WlS5dUunTpQv+Szq9SpUo5df15kZycrODg4Dz3r1Klip599tk7WJG13XXXXbp27Zo++ugjh0B05coVffbZZ4qIiNAnn3ziMI+Li4tcXFzyvS53d/cC15mcnCxvb+8Cz3+jrKwsZWRk/KmaCktBX08UL5wyQ7Hx8MMP61//+pd+/vlnLV682GzPbbxGTEyM7r//fnl7e6tMmTKqVauWXnnlFUl/jPtp1qyZJKlPnz7mKYPo6GhJf5xmqFevnuLi4vTggw+qdOnS5rw3jiHKlpmZqVdeeUV+fn7y9PRU+/btdfLkSYc+gYGB6t27d455r1/m7WrLbQzRxYsX9dJLLykgIEBubm6qVauW3nzzTRmG4dDPZrMpKipKq1atUr169eTm5qa6detqw4YNub/gN0hOTla/fv3k6+srd3d33XPPPVq0aJE5PXs81fHjx7V27Vqz9j8zbmLhwoV6+OGH5ePjIzc3NwUHB2vu3Lm59l2/fr1atmypsmXLym63q1mzZlq6dKmkP17jtWvX6ueffzbryn4dbza+I3t7tm3bZrZ99dVXeuqpp1StWjW5ubkpICBAQ4cO1eXLl/O9bVFRUSpTpowuXbqUY9ozzzwjPz8/ZWZmSpL27dun8PBwVaxYUR4eHgoKClLfvn3zvK5nnnlGy5cvdzi6+MUXX+jSpUt6+umnc/TP7TXJSw03jiHK3jePHTum3r17y9vbW15eXurTp4+53SdOnJDNZtPWrVt16NAh8/3Jft3z+/lesmSJ6tatKzc3N23YsMHclv/973/65z//qUqVKsnb21vPPfecMjIylJKSop49e6pcuXIqV66cRowYkWPZb775plq0aKEKFSrIw8NDTZo0yXGK0Waz6eLFi1q0aJG5Ddn7+80+Y3PmzDFr9ff3V2RkpFJSUhz6ZP9/dPjwYbVq1UqlS5dWlSpVNGXKlBzvG+4sjhChWOnRo4deeeUVbdq0SQMGDMi1z6FDh9SuXTs1aNBAEydOlJubm44dO6adO3dKkurUqaOJEydq7NixGjhwoB544AFJUosWLcxl/P7772rbtq26du2qZ599Vr6+vresa9KkSbLZbBo5cqSSk5M1Y8YMhYWFKT4+3jySlRd5qe16hmGoffv22rp1q/r166eGDRtq48aNGj58uE6dOpXj9MP//vc/ffrpp3r++edVtmxZzZo1S506dVJCQoIqVKhw07ouX76shx56SMeOHVNUVJSCgoK0cuVK9e7dWykpKXrxxRdVp04dffjhhxo6dKiqVq1qngarVKnSLbf56tWr+u233xzaSpcurdKlS2vu3LmqW7eu2rdvr5IlS+qLL77Q888/r6ysLEVGRpr9o6Oj1bdvX9WtW1ejR4+Wt7e3vv32W23YsEHdunXTq6++qtTUVP3yyy/ma1KmTJlb1pWblStX6tKlSxo8eLAqVKigPXv26K233tIvv/yilStX5mtZXbp00ezZs7V27Vo99dRTZvulS5f0xRdfqHfv3nJxcVFycrLatGmjSpUqadSoUfL29taJEyf06aef5nld3bp10/jx47Vt2zY9/PDDkv44hdi6dWv5+Pjcdv4/W8PTTz+toKAgTZ48Wd98843ee+89+fj46L///a8qVaqkDz/8UJMmTdKFCxc0efJkSX/sC/n9fG/ZskUrVqxQVFSUKlasqMDAQHMs3gsvvCA/Pz9NmDBBu3bt0vz58+Xt7a2vv/5a1apV02uvvaZ169bpjTfeUL169dSzZ09zuTNnzlT79u3VvXt3ZWRkaNmyZXrqqae0Zs0aRURESJI+/PBD9e/fX/fee68GDhwo6Y+jczczfvx4TZgwQWFhYRo8eLCOHj2quXPnau/evdq5c6fD0eBz587p0Ucf1ZNPPqmnn35aH3/8sUaOHKn69eurbdu2eXoPUAgMoAgtXLjQkGTs3bv3pn28vLyMRo0amc/HjRtnXP9RnT59uiHJ+PXXX2+6jL179xqSjIULF+aY1rJlS0OSMW/evFyntWzZ0ny+detWQ5JRpUoVIy0tzWxfsWKFIcmYOXOm2Va9enWjV69et13mrWrr1auXUb16dfP5qlWrDEnGf/7zH4d+nTt3Nmw2m3Hs2DGzTZLh6urq0LZ//35DkvHWW2/lWNf1ZsyYYUgyFi9ebLZlZGQYoaGhRpkyZRy2vXr16kZERMQtl3d9X0k5HuPGjTMMwzAuXbqUY57w8HCjRo0a5vOUlBSjbNmyRvPmzY3Lly879M3KyjL/HRER4fDaZcv+zB0/ftyhPfu93bp1q9mWWz2TJ082bDab8fPPP5ttN34mc5OVlWVUqVLF6NSpk0N79mdnx44dhmEYxmeffXbbfeJmWrZsadStW9cwDMNo2rSp0a9fP8MwDOPcuXOGq6ursWjRInM7V65cac5342uS1xquf+8M4/+/Dn379nXo98QTTxgVKlS4aa3Z8vv5LlGihHHo0CGHvtnbEh4e7vB5CA0NNWw2mzFo0CCz7dq1a0bVqlUd9kfDyPm+Z2RkGPXq1TMefvhhh3ZPT89c9/EbX8/k5GTD1dXVaNOmjZGZmWn2e/vttw1JxoIFCxxeF0nGBx98YLalp6cbfn5+OT47uLM4ZYZip0yZMre82ix7HMLnn39e4AHIbm5u6tOnT5779+zZU2XLljWfd+7cWZUrV9a6desKtP68WrdunVxcXPTPf/7Tof2ll16SYRhav369Q3tYWJjDX60NGjSQ3W7XTz/9dNv1+Pn56ZlnnjHbSpUqpX/+85+6cOGCtm/fXuBtaN68uWJiYhwe2X+dX390LTU1Vb/99ptatmypn376SampqZL+OD16/vx5jRo1Ksd4kcK+9P36ei5evKjffvtNLVq0kGEY+vbbb/O1LJvNpqeeekrr1q3ThQsXzPbly5erSpUquv/++yX9/8/zmjVrdPXq1QLX3q1bN3366afKyMjQxx9/LBcXFz3xxBN5mvfP1jBo0CCH5w888IB+//13paWl3XK+/H6+W7ZsedPxa/369XP4PDRv3lyGYahfv35mm4uLi5o2bZpjf7j+fT937pxSU1P1wAMP6Jtvvrll/Tfz5ZdfKiMjQ0OGDFGJEv//a3bAgAGy2+1au3atQ/8yZco4jLNzdXXVvffee9v9FoWLQIRi58KFCw7h40ZdunTRfffdp/79+8vX11ddu3bVihUr8hWOqlSpkq8B1Dde1myz2VSzZs07ft+Rn3/+Wf7+/jlejzp16pjTr1etWrUcyyhXrpzOnTt32/XcfffdDv9532o9+VGxYkWFhYU5PGrUqCFJ2rlzp8LCwuTp6Slvb29VqlTJHM+VHYh+/PFHSVK9evUKXENeJSQkqHfv3ipfvrzKlCmjSpUqqWXLlg715EeXLl10+fJlrV69WtIfn+1169bpqaeeMr+8W7ZsqU6dOmnChAmqWLGiOnTooIULFyo9PT1f6+ratatSU1O1fv16LVmyRO3atbvlfnS9P1vDjZ+7cuXKSVKePnf5+XwHBQXluQYvLy9JUkBAQI72G+tas2aNQkJC5O7urvLly6tSpUqaO3dugd7z6+uuVauWQ7urq6tq1KiRY7uqVq2aI9znZb9F4SIQoVj55ZdflJqaqpo1a960j4eHh3bs2KEvv/xSPXr00HfffacuXbrokUceMQep3k5+xv3k1c2OVuS1psJwsytdjBsGkRYHP/74o1q3bq3ffvtN06ZN09q1axUTE2Pen6awbj+Q1/clMzNTjzzyiNauXauRI0dq1apViomJMQe8F6SekJAQBQYGasWKFZL+GOh8+fJldenSxaG+jz/+WLGxsYqKitKpU6fUt29fNWnSxOHI0u1UrlxZDz30kKZOnaodO3aoW7dueZ73z9ZQVJ+7W+23N6sht/br6/rqq6/Uvn17ubu7a86cOVq3bp1iYmLUrVu3Ittv/kr77d8ZgQjFyocffihJCg8Pv2W/EiVKqHXr1po2bZoOHz6sSZMmacuWLdq6daukwj+V8sMPPzg8NwxDx44dc7girFy5cjmuIJFy/pWbn9qqV6+u06dP5ziFeOTIEXN6Yahevbp++OGHHF/6hb2e633xxRdKT0/X6tWr9dxzz+mxxx5TWFhYji+97FOABw8evOXybva6Zh+tuPG9ufF9OXDggP7v//5PU6dO1ciRI9WhQweFhYXJ398/P5uVw9NPP60NGzYoLS1Ny5cvV2BgoEJCQnL0CwkJ0aRJk7Rv3z4tWbJEhw4d0rJly/K1rm7duumrr76S3W7XY489lu9aC6OG/Ciqz/etfPLJJ3J3d9fGjRvVt29ftW3bVmFhYbn2zeu+m1330aNHHdozMjJ0/PjxItku5B+BCMXGli1b9O9//1tBQUHq3r37TfudPXs2R1v2DQ6zD/F7enpKyvklWFAffPCBw3/aH3/8sc6cOeNwBchdd92lXbt2KSMjw2xbs2ZNjsvz81PbY489pszMTL399tsO7dOnT5fNZiu0K1Aee+wxJSYmavny5WbbtWvX9NZbb6lMmTLmaaPClP1X8fV/BaempmrhwoUO/dq0aaOyZctq8uTJOW4weP28np6euZ7iyA5UO3bsMNsyMzM1f/7829ZjGIZmzpyZr+26UZcuXZSenq5FixZpw4YNOS6DP3fuXI4jATd+nvOqc+fOGjdunObMmZOvU8KFWUN+FNXn+1ZcXFxks9kcjhieOHEi1ztSe3p65mm/DQsLk6urq2bNmuXwur7//vtKTU01r1xD8cJl93CK9evX68iRI7p27ZqSkpK0ZcsWxcTEqHr16lq9evUtb7Y2ceJE7dixQxEREapevbqSk5M1Z84cVa1a1Ryoetddd8nb21vz5s1T2bJl5enpqebNm99yDMKtlC9fXvfff7/69OmjpKQkzZgxQzVr1nS4NUD//v318ccf69FHH9XTTz+tH3/8UYsXL85xaW5+anv88cfVqlUrvfrqqzpx4oTuuecebdq0SZ9//rmGDBlyy8t+82PgwIF655131Lt3b8XFxSkwMFAff/yxdu7cqRkzZuR5LEp+tGnTRq6urnr88cf13HPP6cKFC3r33Xfl4+OjM2fOmP3sdrumT5+u/v37q1mzZurWrZvKlSun/fv369KlS+a9kpo0aaLly5dr2LBhatasmcqUKaPHH39cdevWVUhIiEaPHq2zZ8+qfPnyWrZsma5du+ZQT+3atXXXXXfp5Zdf1qlTp2S32/XJJ5/86XEcjRs3Vs2aNfXqq68qPT3d4XSZJC1atEhz5szRE088obvuukvnz5/Xu+++W6CjPF5eXgX6rbHCrCE/iurzfSsRERGaNm2aHn30UXXr1k3JycmaPXu2atasqe+++86hb5MmTfTll19q2rRp8vf3V1BQkJo3b55jmZUqVdLo0aM1YcIEPfroo2rfvr2OHj2qOXPmqFmzZtyotLgq8uvaYGnZl6dmP1xdXQ0/Pz/jkUceMWbOnOlweXe2Gy9x3rx5s9GhQwfD39/fcHV1Nfz9/Y1nnnnG+L//+z+H+T7//HMjODjYKFmypMNl7rld/pvtZpfdf/TRR8bo0aMNHx8fw8PDw4iIiHC4DDvb1KlTjSpVqhhubm7GfffdZ+zbty/HMm9V242X3RuGYZw/f94YOnSo4e/vb5QqVcq4++67jTfeeMPhEmPD+OOy5MjIyBw13ex2ADdKSkoy+vTpY1SsWNFwdXU16tevn+utAfJ72f2t+q5evdpo0KCB4e7ubgQGBhr//e9/jQULFuR6mfzq1auNFi1aGB4eHobdbjfuvfde46OPPjKnX7hwwejWrZvh7e1tSHJ4HX/88UcjLCzMcHNzM3x9fY1XXnnFiImJyXHZ/eHDh42wsDCjTJkyRsWKFY0BAwaYty64/rXIy2X313v11VcNSUbNmjVzTPvmm2+MZ555xqhWrZrh5uZm+Pj4GO3atTP27dt32+Xe6rOcLS+X3ee1Bt3ksvsbb4GR260Oblbrn/183+xWHjerrVevXoanp6dD2/vvv2/cfffdhpubm1G7dm1j4cKFub7HR44cMR588EHDw8PDkGTuVze7tcPbb79t1K5d2yhVqpTh6+trDB482Dh37pxDn5u9Lrn9X4A7y2YYjNoCAADWxhgiAABgeQQiAABgeQQiAABgeQQiAABgeQQiAABgeQQiAABgedyYMQ+ysrJ0+vRplS1bttB/EgIAANwZhmHo/Pnz8vf3z/Hj1TciEOXB6dOnc/xiMgAA+Gs4efKkqlatess+BKI8yP7ZgpMnT8putzu5GgAAkBdpaWkKCAjI088PEYjyIPs0md1uJxABAPAXk5fhLgyqBgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAgAAluf0QHTq1Ck9++yzqlChgjw8PFS/fn3t27fPnG4YhsaOHavKlSvLw8NDYWFh+uGHHxyWcfbsWXXv3l12u13e3t7q16+fLly44NDnu+++0wMPPCB3d3cFBARoypQpRbJ9AACg+HNqIDp37pzuu+8+lSpVSuvXr9fhw4c1depUlStXzuwzZcoUzZo1S/PmzdPu3bvl6emp8PBwXblyxezTvXt3HTp0SDExMVqzZo127NihgQMHmtPT0tLUpk0bVa9eXXFxcXrjjTc0fvx4zZ8/v0i3FwAAFE82wzAMZ6181KhR2rlzp7766qtcpxuGIX9/f7300kt6+eWXJUmpqany9fVVdHS0unbtqu+//17BwcHau3evmjZtKknasGGDHnvsMf3yyy/y9/fX3Llz9eqrryoxMVGurq7muletWqUjR47cts60tDR5eXkpNTWVX7sHAOAvIj/f3049QrR69Wo1bdpUTz31lHx8fNSoUSO9++675vTjx48rMTFRYWFhZpuXl5eaN2+u2NhYSVJsbKy8vb3NMCRJYWFhKlGihHbv3m32efDBB80wJEnh4eE6evSozp07l6Ou9PR0paWlOTwAAMDfV0lnrvynn37S3LlzNWzYML3yyivau3ev/vnPf8rV1VW9evVSYmKiJMnX19dhPl9fX3NaYmKifHx8HKaXLFlS5cuXd+gTFBSUYxnZ064/RSdJkydP1oQJEwpvQ//CAketdXYJKEInXo9wdgkA4BROPUKUlZWlxo0b67XXXlOjRo00cOBADRgwQPPmzXNmWRo9erRSU1PNx8mTJ51aDwAAuLOcGogqV66s4OBgh7Y6deooISFBkuTn5ydJSkpKcuiTlJRkTvPz81NycrLD9GvXruns2bMOfXJbxvXruJ6bm5vsdrvDAwAA/H05NRDdd999Onr0qEPb//3f/6l69eqSpKCgIPn5+Wnz5s3m9LS0NO3evVuhoaGSpNDQUKWkpCguLs7ss2XLFmVlZal58+Zmnx07dujq1atmn5iYGNWqVSvH6TIAAGA9Tg1EQ4cO1a5du/Taa6/p2LFjWrp0qebPn6/IyEhJks1m05AhQ/Sf//xHq1ev1oEDB9SzZ0/5+/urY8eOkv44ovToo49qwIAB2rNnj3bu3KmoqCh17dpV/v7+kqRu3brJ1dVV/fr106FDh7R8+XLNnDlTw4YNc9amAwCAYsSpg6qbNWumzz77TKNHj9bEiRMVFBSkGTNmqHv37mafESNG6OLFixo4cKBSUlJ0//33a8OGDXJ3dzf7LFmyRFFRUWrdurVKlCihTp06adasWeZ0Ly8vbdq0SZGRkWrSpIkqVqyosWPHOtyrCAAAWJdT70P0V2Hl+xBxlZm1cJUZgL+Tv8x9iAAAAIoDAhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8pwai8ePHy2azOTxq165tTr9y5YoiIyNVoUIFlSlTRp06dVJSUpLDMhISEhQREaHSpUvLx8dHw4cP17Vr1xz6bNu2TY0bN5abm5tq1qyp6Ojootg8AADwF+H0I0R169bVmTNnzMf//vc/c9rQoUP1xRdfaOXKldq+fbtOnz6tJ5980pyemZmpiIgIZWRk6Ouvv9aiRYsUHR2tsWPHmn2OHz+uiIgItWrVSvHx8RoyZIj69++vjRs3Ful2AgCA4quk0wsoWVJ+fn452lNTU/X+++9r6dKlevjhhyVJCxcuVJ06dbRr1y6FhIRo06ZNOnz4sL788kv5+vqqYcOG+ve//62RI0dq/PjxcnV11bx58xQUFKSpU6dKkurUqaP//e9/mj59usLDw4t0WwEAQPHk9CNEP/zwg/z9/VWjRg11795dCQkJkqS4uDhdvXpVYWFhZt/atWurWrVqio2NlSTFxsaqfv368vX1NfuEh4crLS1Nhw4dMvtcv4zsPtnLyE16errS0tIcHgAA4O/LqYGoefPmio6O1oYNGzR37lwdP35cDzzwgM6fP6/ExES5urrK29vbYR5fX18lJiZKkhITEx3CUPb07Gm36pOWlqbLly/nWtfkyZPl5eVlPgICAgpjcwEAQDHl1FNmbdu2Nf/doEEDNW/eXNWrV9eKFSvk4eHhtLpGjx6tYcOGmc/T0tIIRQAA/I05/ZTZ9by9vfWPf/xDx44dk5+fnzIyMpSSkuLQJykpyRxz5Ofnl+Oqs+znt+tjt9tvGrrc3Nxkt9sdHgAA4O+rWAWiCxcu6Mcff1TlypXVpEkTlSpVSps3bzanHz16VAkJCQoNDZUkhYaG6sCBA0pOTjb7xMTEyG63Kzg42Oxz/TKy+2QvAwAAwKmB6OWXX9b27dt14sQJff3113riiSfk4uKiZ555Rl5eXurXr5+GDRumrVu3Ki4uTn369FFoaKhCQkIkSW3atFFwcLB69Oih/fv3a+PGjRozZowiIyPl5uYmSRo0aJB++uknjRgxQkeOHNGcOXO0YsUKDR061JmbDgAAihGnjiH65Zdf9Mwzz+j3339XpUqVdP/992vXrl2qVKmSJGn69OkqUaKEOnXqpPT0dIWHh2vOnDnm/C4uLlqzZo0GDx6s0NBQeXp6qlevXpo4caLZJygoSGvXrtXQoUM1c+ZMVa1aVe+99x6X3AMAAJPNMAzD2UUUd2lpafLy8lJqaqrlxhMFjlrr7BJQhE68HuHsEgCg0OTn+7tYjSECAABwBgIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwvGITiF5//XXZbDYNGTLEbLty5YoiIyNVoUIFlSlTRp06dVJSUpLDfAkJCYqIiFDp0qXl4+Oj4cOH69q1aw59tm3bpsaNG8vNzU01a9ZUdHR0EWwRAAD4qygWgWjv3r1655131KBBA4f2oUOH6osvvtDKlSu1fft2nT59Wk8++aQ5PTMzUxEREcrIyNDXX3+tRYsWKTo6WmPHjjX7HD9+XBEREWrVqpXi4+M1ZMgQ9e/fXxs3biyy7QMAAMWb0wPRhQsX1L17d7377rsqV66c2Z6amqr3339f06ZN08MPP6wmTZpo4cKF+vrrr7Vr1y5J0qZNm3T48GEtXrxYDRs2VNu2bfXvf/9bs2fPVkZGhiRp3rx5CgoK0tSpU1WnTh1FRUWpc+fOmj59ulO2FwAAFD9OD0SRkZGKiIhQWFiYQ3tcXJyuXr3q0F67dm1Vq1ZNsbGxkqTY2FjVr19fvr6+Zp/w8HClpaXp0KFDZp8blx0eHm4uAwAAoKQzV75s2TJ988032rt3b45piYmJcnV1lbe3t0O7r6+vEhMTzT7Xh6Hs6dnTbtUnLS1Nly9floeHR451p6enKz093XyelpaW/40DAAB/GU47QnTy5Em9+OKLWrJkidzd3Z1VRq4mT54sLy8v8xEQEODskgAAwB3ktEAUFxen5ORkNW7cWCVLllTJkiW1fft2zZo1SyVLlpSvr68yMjKUkpLiMF9SUpL8/PwkSX5+fjmuOst+frs+drs916NDkjR69Gilpqaaj5MnTxbGJgMAgGLKaYGodevWOnDggOLj481H06ZN1b17d/PfpUqV0ubNm815jh49qoSEBIWGhkqSQkNDdeDAASUnJ5t9YmJiZLfbFRwcbPa5fhnZfbKXkRs3NzfZ7XaHBwAA+Pty2hiismXLql69eg5tnp6eqlChgtner18/DRs2TOXLl5fdbtcLL7yg0NBQhYSESJLatGmj4OBg9ejRQ1OmTFFiYqLGjBmjyMhIubm5SZIGDRqkt99+WyNGjFDfvn21ZcsWrVixQmvXri3aDQYAAMWWUwdV38706dNVokQJderUSenp6QoPD9ecOXPM6S4uLlqzZo0GDx6s0NBQeXp6qlevXpo4caLZJygoSGvXrtXQoUM1c+ZMVa1aVe+9957Cw8OdsUkAAKAYshmGYTi7iOIuLS1NXl5eSk1Ntdzps8BRHEmzkhOvRzi7BAAoNPn5/nb6fYgAAACcjUAEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsr0CBqEaNGvr9999ztKekpKhGjRp/uigAAICiVKBAdOLECWVmZuZoT09P16lTp/50UQAAAEWpZH46r1692vz3xo0b5eXlZT7PzMzU5s2bFRgYWGjFAQAAFIV8BaKOHTtKkmw2m3r16uUwrVSpUgoMDNTUqVMLrTgAAICikK9AlJWVJUkKCgrS3r17VbFixTtSFAAAQFHKVyDKdvz48cKuAwAAwGkKFIgkafPmzdq8ebOSk5PNI0fZFixY8KcLAwAAKCoFCkQTJkzQxIkT1bRpU1WuXFk2m62w6wIAACgyBQpE8+bNU3R0tHr06FHY9QAAABS5At2HKCMjQy1atCjsWgAAAJyiQIGof//+Wrp0aWHXAgAA4BQFOmV25coVzZ8/X19++aUaNGigUqVKOUyfNm1aoRQHAABQFAoUiL777js1bNhQknTw4EGHaQywBgAAfzUFCkRbt24t7DoAAACcpkBjiAAAAP5OCnSEqFWrVrc8NbZly5YCFwQAAFDUChSIsscPZbt69ari4+N18ODBHD/6CgAAUNwVKBBNnz491/bx48frwoULf6ogAACAolaoY4ieffZZfscMAAD85RRqIIqNjZW7u3thLhIAAOCOK9ApsyeffNLhuWEYOnPmjPbt26d//etfhVIYAABAUSlQIPLy8nJ4XqJECdWqVUsTJ05UmzZtCqUwAACAolKgQLRw4cLCrgMAAMBp/tQYori4OC1evFiLFy/Wt99+m+/5586dqwYNGshut8tutys0NFTr1683p1+5ckWRkZGqUKGCypQpo06dOikpKclhGQkJCYqIiFDp0qXl4+Oj4cOH69q1aw59tm3bpsaNG8vNzU01a9ZUdHR0gbYXAAD8PRXoCFFycrK6du2qbdu2ydvbW5KUkpKiVq1aadmyZapUqVKellO1alW9/vrruvvuu2UYhhYtWqQOHTro22+/Vd26dTV06FCtXbtWK1eulJeXl6KiovTkk09q586dkqTMzExFRETIz89PX3/9tc6cOaOePXuqVKlSeu211yRJx48fV0REhAYNGqQlS5Zo8+bN6t+/vypXrqzw8PCCbD4AAPibsRmGYeR3pi5duuinn37SBx98oDp16kiSDh8+rF69eqlmzZr66KOPClxQ+fLl9cYbb6hz586qVKmSli5dqs6dO0uSjhw5ojp16ig2NlYhISFav3692rVrp9OnT8vX11eSNG/ePI0cOVK//vqrXF1dNXLkSK1du9bhR2i7du2qlJQUbdiwIU81paWlycvLS6mpqbLb7QXetr+iwFFrnV0CitCJ1yOcXQKKEPu3tVhx/87P93eBTplt2LBBc+bMMcOQJAUHB2v27NkOp7zyIzMzU8uWLdPFixcVGhqquLg4Xb16VWFhYWaf2rVrq1q1aoqNjZX0x2X+9evXN8OQJIWHhystLU2HDh0y+1y/jOw+2cvITXp6utLS0hweAADg76tAgSgrK0ulSpXK0V6qVCllZWXla1kHDhxQmTJl5ObmpkGDBumzzz5TcHCwEhMT5erqap6Sy+br66vExERJUmJiokMYyp6ePe1WfdLS0nT58uVca5o8ebK8vLzMR0BAQL62CQAA/LUUKBA9/PDDevHFF3X69Gmz7dSpUxo6dKhat26dr2XVqlVL8fHx2r17twYPHqxevXrp8OHDBSmr0IwePVqpqanm4+TJk06tBwAA3FkFGlT99ttvq3379goMDDSPnpw8eVL16tXT4sWL87UsV1dX1axZU5LUpEkT7d27VzNnzlSXLl2UkZGhlJQUh6NESUlJ8vPzkyT5+flpz549DsvLvgrt+j43XpmWlJQku90uDw+PXGtyc3OTm5tbvrYDAAD8dRUoEAUEBOibb77Rl19+qSNHjkiS6tSpk2OsTkFkZWUpPT1dTZo0UalSpbR582Z16tRJknT06FElJCQoNDRUkhQaGqpJkyYpOTlZPj4+kqSYmBjZ7XYFBwebfdatW+ewjpiYGHMZAAAA+QpEW7ZsUVRUlHbt2iW73a5HHnlEjzzyiCQpNTVVdevW1bx58/TAAw/kaXmjR49W27ZtVa1aNZ0/f15Lly7Vtm3btHHjRnl5ealfv34aNmyYypcvL7vdrhdeeEGhoaEKCQmRJLVp00bBwcHq0aOHpkyZosTERI0ZM0aRkZHmEZ5Bgwbp7bff1ogRI9S3b19t2bJFK1as0Nq1XF0BAAD+kK9ANGPGDA0YMCDXS9e8vLz03HPPadq0aXkORMnJyerZs6fOnDkjLy8vNWjQQBs3bjRD1vTp01WiRAl16tRJ6enpCg8P15w5c8z5XVxctGbNGg0ePFihoaHy9PRUr169NHHiRLNPUFCQ1q5dq6FDh2rmzJmqWrWq3nvvPe5BBAAATPm6D1H16tW1YcMGh8vtr3fkyBG1adNGCQkJhVZgccB9iGAVVrxPiZWxf1uLFffvO3YfoqSkpFwvt89WsmRJ/frrr/lZJAAAgNPlKxBVqVLF4Y7PN/ruu+9UuXLlP10UAABAUcpXIHrsscf0r3/9S1euXMkx7fLlyxo3bpzatWtXaMUBAAAUhXwNqh4zZow+/fRT/eMf/1BUVJRq1aol6Y+xQ7Nnz1ZmZqZeffXVO1IoAADAnZKvQOTr66uvv/5agwcP1ujRo5U9Httmsyk8PFyzZ8/O8TMZAAAAxV2+b8xYvXp1rVu3TufOndOxY8dkGIbuvvtulStX7k7UBwAAcMcV6E7VklSuXDk1a9asMGsBAABwigL9uCsAAMDfCYEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYHoEIAABYnlMD0eTJk9WsWTOVLVtWPj4+6tixo44ePerQ58qVK4qMjFSFChVUpkwZderUSUlJSQ59EhISFBERodKlS8vHx0fDhw/XtWvXHPps27ZNjRs3lpubm2rWrKno6Og7vXkAAOAvwqmBaPv27YqMjNSuXbsUExOjq1evqk2bNrp48aLZZ+jQofriiy+0cuVKbd++XadPn9aTTz5pTs/MzFRERIQyMjL09ddfa9GiRYqOjtbYsWPNPsePH1dERIRatWql+Ph4DRkyRP3799fGjRuLdHsBAEDxZDMMw3B2Edl+/fVX+fj4aPv27XrwwQeVmpqqSpUqaenSpercubMk6ciRI6pTp45iY2MVEhKi9evXq127djp9+rR8fX0lSfPmzdPIkSP166+/ytXVVSNHjtTatWt18OBBc11du3ZVSkqKNmzYcNu60tLS5OXlpdTUVNnt9juz8cVU4Ki1zi4BRejE6xHOLgFFiP3bWqy4f+fn+7tYjSFKTU2VJJUvX16SFBcXp6tXryosLMzsU7t2bVWrVk2xsbGSpNjYWNWvX98MQ5IUHh6utLQ0HTp0yOxz/TKy+2QvAwAAWFtJZxeQLSsrS0OGDNF9992nevXqSZISExPl6uoqb29vh76+vr5KTEw0+1wfhrKnZ0+7VZ+0tDRdvnxZHh4eDtPS09OVnp5uPk9LS/vzGwgAAIqtYnOEKDIyUgcPHtSyZcucXYomT54sLy8v8xEQEODskgAAwB1ULAJRVFSU1qxZo61bt6pq1apmu5+fnzIyMpSSkuLQPykpSX5+fmafG686y35+uz52uz3H0SFJGj16tFJTU83HyZMn//Q2AgCA4supgcgwDEVFRemzzz7Tli1bFBQU5DC9SZMmKlWqlDZv3my2HT16VAkJCQoNDZUkhYaG6sCBA0pOTjb7xMTEyG63Kzg42Oxz/TKy+2Qv40Zubm6y2+0ODwAA8Pfl1DFEkZGRWrp0qT7//HOVLVvWHPPj5eUlDw8PeXl5qV+/fho2bJjKly8vu92uF154QaGhoQoJCZEktWnTRsHBwerRo4emTJmixMREjRkzRpGRkXJzc5MkDRo0SG+//bZGjBihvn37asuWLVqxYoXWruUKCwAA4OQjRHPnzlVqaqoeeughVa5c2XwsX77c7DN9+nS1a9dOnTp10oMPPig/Pz99+umn5nQXFxetWbNGLi4uCg0N1bPPPquePXtq4sSJZp+goCCtXbtWMTExuueeezR16lS99957Cg8PL9LtBQAAxVOxug9RccV9iGAVVrxPiZWxf1uLFffvv+x9iAAAAJyBQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzPqYFox44devzxx+Xv7y+bzaZVq1Y5TDcMQ2PHjlXlypXl4eGhsLAw/fDDDw59zp49q+7du8tut8vb21v9+vXThQsXHPp89913euCBB+Tu7q6AgABNmTLlTm8aAAD4C3FqILp48aLuuecezZ49O9fpU6ZM0axZszRv3jzt3r1bnp6eCg8P15UrV8w+3bt316FDhxQTE6M1a9Zox44dGjhwoDk9LS1Nbdq0UfXq1RUXF6c33nhD48eP1/z58+/49gEAgL+Gks5cedu2bdW2bdtcpxmGoRkzZmjMmDHq0KGDJOmDDz6Qr6+vVq1apa5du+r777/Xhg0btHfvXjVt2lSS9NZbb+mxxx7Tm2++KX9/fy1ZskQZGRlasGCBXF1dVbduXcXHx2vatGkOwQkAAFhXsR1DdPz4cSUmJiosLMxs8/LyUvPmzRUbGytJio2Nlbe3txmGJCksLEwlSpTQ7t27zT4PPvigXF1dzT7h4eE6evSozp07l+u609PTlZaW5vAAAAB/X8U2ECUmJkqSfH19Hdp9fX3NaYmJifLx8XGYXrJkSZUvX96hT27LuH4dN5o8ebK8vLzMR0BAwJ/fIAAAUGwV20DkTKNHj1Zqaqr5OHnypLNLAgAAd1CxDUR+fn6SpKSkJIf2pKQkc5qfn5+Sk5Mdpl+7dk1nz5516JPbMq5fx43c3Nxkt9sdHgAA4O+r2AaioKAg+fn5afPmzWZbWlqadu/erdDQUElSaGioUlJSFBcXZ/bZsmWLsrKy1Lx5c7PPjh07dPXqVbNPTEyMatWqpXLlyhXR1gAAgOLMqYHowoULio+PV3x8vKQ/BlLHx8crISFBNptNQ4YM0X/+8x+tXr1aBw4cUM+ePeXv76+OHTtKkurUqaNHH31UAwYM0J49e7Rz505FRUWpa9eu8vf3lyR169ZNrq6u6tevnw4dOqTly5dr5syZGjZsmJO2GgAAFDdOvex+3759atWqlfk8O6T06tVL0dHRGjFihC5evKiBAwcqJSVF999/vzZs2CB3d3dzniVLligqKkqtW7dWiRIl1KlTJ82aNcuc7uXlpU2bNikyMlJNmjRRxYoVNXbsWC65BwAAJpthGIaziyju0tLS5OXlpdTUVMuNJwoctdbZJaAInXg9wtkloAixf1uLFffv/Hx/F9sxRAAAAEWFQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACyPQAQAACzPUoFo9uzZCgwMlLu7u5o3b649e/Y4uyQAAFAMWCYQLV++XMOGDdO4ceP0zTff6J577lF4eLiSk5OdXRoAAHAyywSiadOmacCAAerTp4+Cg4M1b948lS5dWgsWLHB2aQAAwMksEYgyMjIUFxensLAws61EiRIKCwtTbGysEysDAADFQUlnF1AUfvvtN2VmZsrX19eh3dfXV0eOHMnRPz09Xenp6ebz1NRUSVJaWtqdLbQYykq/5OwSUISs+Bm3MvZva7Hi/p29zYZh3LavJQJRfk2ePFkTJkzI0R4QEOCEaoCi4zXD2RUAuFOsvH+fP39eXl5et+xjiUBUsWJFubi4KCkpyaE9KSlJfn5+OfqPHj1aw4YNM59nZWXp7NmzqlChgmw22x2vF86VlpamgIAAnTx5Una73dnlAChE7N/WYhiGzp8/L39//9v2tUQgcnV1VZMmTbR582Z17NhR0h8hZ/PmzYqKisrR383NTW5ubg5t3t7eRVApihO73c5/mMDfFPu3ddzuyFA2SwQiSRo2bJh69eqlpk2b6t5779WMGTN08eJF9enTx9mlAQAAJ7NMIOrSpYt+/fVXjR07VomJiWrYsKE2bNiQY6A1AACwHssEIkmKiorK9RQZcD03NzeNGzcux2lTAH997N+4GZuRl2vRAAAA/sYscWNGAACAWyEQAQAAyyMQAQAAyyMQAQAAy7PUVWYAAGv57bfftGDBAsXGxioxMVGS5OfnpxYtWqh3796qVKmSkytEccFVZgCAv6W9e/cqPDxcpUuXVlhYmHnfuaSkJG3evFmXLl3Sxo0b1bRpUydXiuKAQATcxsmTJzVu3DgtWLDA2aUAyIeQkBDdc889mjdvXo7foTQMQ4MGDdJ3332n2NhYJ1WI4oRABNzG/v371bhxY2VmZjq7FAD54OHhoW+//Va1a9fOdfqRI0fUqFEjXb58uYgrQ3HEGCJY3urVq285/aeffiqiSgAUJj8/P+3Zs+emgWjPnj38fBNMBCJYXseOHWWz2XSrg6U3Hm4HUPy9/PLLGjhwoOLi4tS6descY4jeffddvfnmm06uEsUFp8xgeVWqVNGcOXPUoUOHXKfHx8erSZMmnDID/oKWL1+u6dOnKy4uztyHXVxc1KRJEw0bNkxPP/20kytEcUEgguW1b99eDRs21MSJE3Odvn//fjVq1EhZWVlFXBmAwnL16lX99ttvkqSKFSuqVKlSTq4IxQ2nzGB5w4cP18WLF286vWbNmtq6dWsRVgSgsJUqVUqVK1d2dhkoxjhCBAAALI+f7gAAAJZHIAIAAJZHIAIAAJZHIALugOjoaHl7e//p5dhsNq1ateqWfX7//Xf5+PjoxIkTf3p9fwe9e/dWx44dnV3GbZ04cUI2m03x8fGF2vd68+fPV0BAgEqUKKEZM2YUqE5nCAwMLPR6u3btqqlTpxbqMvH3QiACcvFX+VKVpEmTJqlDhw4KDAw02xISEhQREaHSpUvLx8dHw4cP17Vr1/K13OjoaNlsthyP995770/XPH78eDVs2PBPL6ewzJ8/Xw899JDsdrtsNptSUlIKtJzs12jXrl0O7enp6apQoYJsNpu2bdsmSQoICNCZM2dUr1692y43P32zpaWlKSoqSiNHjtSpU6c0cODAfG1LUbjZHw579+4t9HrHjBmjSZMmKTU1tVCXi78PAhHwF3bp0iW9//776tevn9mWmZmpiIgIZWRk6Ouvv9aiRYsUHR2tsWPH5nv5drtdZ86ccXh07969MDehWLh06ZIeffRRvfLKK396WQEBAVq4cKFD22effaYyZco4tLm4uMjPz08lS97+7if56ZstISFBV69eVUREhCpXrqzSpUvned7rXb16tUDz/RmVKlUqcL03U69ePd11111avHhxoS4Xfx8EIqAApk2bpvr168vT01MBAQF6/vnndeHChRz9Vq1apbvvvlvu7u4KDw/XyZMnHaZ//vnnaty4sdzd3VWjRg1NmDAhX0dy1q1bJzc3N4WEhJhtmzZt0uHDh7V48WI1bNhQbdu21b///W/Nnj1bGRkZ+dpOm80mPz8/h4eHh4c2bNig+++/X97e3qpQoYLatWunH3/80WHeX375Rc8884zKly8vT09PNW3aVLt371Z0dLQmTJig/fv3m0dUoqOjcz0tlJKS4nBUJTMzU/369VNQUJA8PDxUq1YtzZw5M1/blJshQ4Zo1KhRDq9jQfXq1UvLli1z+MHQBQsWqFevXg79btzec+fOqXv37qpUqZI8PDx09913m8Hqxr7btm2TzWbT5s2b1bRpU5UuXVotWrTQ0aNHJf1x5KV+/fqSpBo1ashms5mnVOfOnau77rpLrq6uqlWrlj788EOHumw2m+bOnav27dvL09NTkyZNMo/oLViwQNWqVVOZMmX0/PPPKzMzU1OmTJGfn598fHw0adIkh2Xdaj/Ztm2b+vTpo9TUVPNzMH78eEk5T5klJCSoQ4cOKlOmjOx2u55++mklJSWZ07Pr+/DDDxUYGCgvLy917dpV58+fd6jn8ccf17Jly/LyNsKCCERAAZQoUUKzZs3SoUOHtGjRIm3ZskUjRoxw6HPp0iVNmjRJH3zwgXbu3KmUlBR17drVnP7VV1+pZ8+eevHFF3X48GG98847io6OzvGlcitfffWVmjRp4tAWGxur+vXrO/xoZXh4uNLS0nTo0CFJ//8LNjto5NfFixc1bNgw7du3T5s3b1aJEiX0xBNPmHfzvnDhglq2bKlTp05p9erV2r9/v0aMGKGsrCx16dJFL730kurWrWsederSpUue1puVlaWqVatq5cqVOnz4sMaOHatXXnlFK1asKNB25MdDDz2k3r1737ZfkyZNFBgYqE8++UTSH1/mO3bsUI8ePW4537/+9S8dPnxY69ev1/fff6+5c+eqYsWKt5zn1Vdf1dSpU7Vv3z6VLFlSffv2lSR16dJFX375paQ/fsD0zJkzCggI0GeffaYXX3xRL730kg4ePKjnnntOffr0yXHj0fHjx+uJJ57QgQMHzGX++OOPWr9+vTZs2KCPPvpI77//viIiIvTLL79o+/bt+u9//6sxY8Zo9+7d5nJutZ+0aNFCM2bMcDgK+fLLL+fYxqysLHXo0EFnz57V9u3bFRMTo59++inHZ+bHH3/UqlWrtGbNGq1Zs0bbt2/X66+/7tDn3nvv1Z49e5Senn7L1xUWZQDIoVevXkaHDh3y3H/lypVGhQoVzOcLFy40JBm7du0y277//ntDkrF7927DMAyjdevWxmuvveawnA8//NCoXLmy+VyS8dlnn910vR06dDD69u3r0DZgwACjTZs2Dm0XL140JBnr1q0zDMMwfvnlF6NWrVpmLbnJ3gZPT0/z4evrm2vfX3/91ZBkHDhwwDAMw3jnnXeMsmXLGr///nuu/ceNG2fcc889Dm3Hjx83JBnffvut2Xbu3DlDkrF169ab1hkZGWl06tTJfJ7f9+56W7duNSQZ586dyzGtR48exqhRo245f/b7NWPGDKNVq1aGYRjGhAkTjCeeeCLHtty4vY8//rjRp0+fXJd7Y9/sOr/88kuzz9q1aw1JxuXLlw3DMIxvv/3WkGQcP37c7NOiRQtjwIABDst+6qmnjMcee8xhG4YMGeLQZ9y4cUbp0qWNtLQ0sy08PNwIDAw0MjMzzbZatWoZkydPvunrk9t+4uXllaNf9erVjenTpxuGYRibNm0yXFxcjISEBHP6oUOHDEnGnj17blrf8OHDjebNmzssd//+/YYk48SJEzetEdbFT3cABfDll19q8uTJOnLkiNLS0nTt2jVduXJFly5dMsc+lCxZUs2aNTPnqV27try9vfX999/r3nvv1f79+7Vz506HI0KZmZk5lnMrly9flru7e77rr1Klio4cOXLbfmXLltU333xjPi9R4o+Dyj/88IPGjh2r3bt367fffjOPDCUkJKhevXqKj49Xo0aNVL58+XzXdjuzZ8/WggULlJCQoMuXLysjI6NIBmh/8MEHee777LPPatSoUfrpp58UHR2tWbNm3XaewYMHq1OnTvrmm2/Upk0bdezYUS1atLjlPA0aNDD/nf2zFMnJyapWrVqu/b///vscg5Xvu+++HKcdmzZtmmPewMBAlS1b1nzu6+srFxcX8zOR3ZacnGw+z8t+cjvff/+9AgICFBAQYLYFBweb+1L2PnZjfZUrV3aoRZI8PDwk/XH0FrgRp8yAfDpx4oTatWunBg0a6JNPPlFcXJxmz54tSfkao3PhwgVNmDBB8fHx5uPAgQP64Ycf8hxyKlasqHPnzjm0+fn5OYyvkGQ+9/Pzy3N90h8BqGbNmuajRo0akv4Yi3H27Fm9++672r17t3maJHv7s7948rsuSTKu+zWhGwf0Llu2TC+//LL69eunTZs2KT4+Xn369Mn32Kg7LXtcVb9+/XTlyhW1bdv2tvO0bdtWP//8s4YOHarTp0+rdevWuZ5Cut71P1Bqs9kkqVB+hNjT0/OW68peX25t2esvrP0kr25VS7azZ89K+mPQNnAjAhGQT3FxccrKytLUqVMVEhKif/zjHzp9+nSOfteuXdO+ffvM50ePHlVKSorq1KkjSWrcuLGOHj3qEDiyH9f/1X0rjRo10uHDhx3aQkNDdeDAAYe/jmNiYmS32xUcHFyQTXbw+++/6+jRoxozZoxat26tOnXq5AhlDRo0UHx8vPkFdCNXV1dlZmY6tGV/SZ05c8Zsu/G+Ozt37lSLFi30/PPPq1GjRqpZs2aOwdzFRd++fbVt2zb17NlTLi4ueZqnUqVK6tWrlxYvXqwZM2Zo/vz5hVpTnTp1tHPnToe2nTt3Fsrn4kZ52U9y+xzkVvPJkycdLkg4fPiwUlJS8l33wYMHVbVq1duOzYI1ccoMuInU1NQcX8gVKlRQzZo1dfXqVb311lt6/PHHtXPnTs2bNy/H/KVKldILL7ygWbNmqWTJkoqKilJISIjuvfdeSdLYsWPVrl07VatWTZ07d1aJEiW0f/9+HTx4UP/5z3/yVGN4eLhGjx6tc+fOqVy5cpKkNm3aKDg4WD169NCUKVOUmJioMWPGKDIyUm5ubpKkU6dOqXXr1vrggw/MevKqXLlyqlChgubPn6/KlSsrISFBo0aNcujzzDPP6LXXXlPHjh01efJkVa5cWd9++638/f0VGhqqwMBAHT9+XPHx8apatarKli0rDw8PhYSE6PXXX1dQUJCSk5M1ZswYh+Xefffd+uCDD7Rx40YFBQXpww8/1N69exUUFJSvbbhRYmKiEhMTdezYMUnSgQMHVLZsWVWrVs087dezZ09VqVJFkydPztMyH330Uf3666+y2+156j927Fg1adJEdevWVXp6utasWWOG58IyfPhwPf3002rUqJHCwsL0xRdf6NNPPzUHYBemvOwngYGBunDhgjZv3qx77rlHpUuXznEqLSwsTPXr11f37t01Y8YMXbt2Tc8//7xatmyZ66m9W/nqq6/Upk2bP71t+HviCBFwE9u2bVOjRo0cHhMmTNA999yjadOm6b///a/q1aunJUuW5PolWbp0aY0cOVLdunXTfffdpzJlymj58uXm9PDwcK1Zs0abNm1Ss2bNFBISounTp6t69ep5rrF+/fpq3Lixw1VWLi4uWrNmjVxcXBQaGqpnn31WPXv21MSJE80+V69e1dGjRws0lqJEiRJatmyZ4uLiVK9ePQ0dOlRvvPGGQx9XV1dt2rRJPj4+euyxx1S/fn29/vrr5pGSTp066dFHH1WrVq1UqVIlffTRR5L+uDz92rVratKkiYYMGZIjGD733HN68skn1aVLFzVv3ly///67nn/++VvWm32DyVuZN2+eGjVqpAEDBkiSHnzwQTVq1EirV682+yQkJDgcvbodm82mihUrytXVNU/9XV1dNXr0aDVo0EAPPvigXFxcCv0S8Y4dO2rmzJl68803VbduXb3zzjtauHChHnrooUJdj6Q87SctWrTQoEGD1KVLF1WqVElTpkzJsRybzabPP/9c5cqV04MPPqiwsDDVqFHDYV/KiytXrmjVqlXmewzcyGZcf8IewF/O2rVrNXz4cB08eDDPp9qsZNy4cdq+fXuBbzGAv4e5c+fqs88+06ZNm5xdCoopTpkBf3ERERH64YcfdOrUKYcrcfCH9evX6+2333Z2GXCyUqVK6a233nJ2GSjGOEIEAAAsj+PrAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8ghEAADA8v4fsdXnYfewVgMAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count occurrences of each label\n",
        "label_counts = df_climate_inference['predicted_label'].value_counts()\n",
        "\n",
        "print(\"Distribution of Factual vs Misinformation:\")\n",
        "print(label_counts)\n",
        "\n",
        "# If you want to visualise this distribution:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Factual vs Misinformation')\n",
        "plt.xlabel('Label (0: Factual, 1: Misinformation)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Examples of Factual (predicted_label 0):\n",
            "                                                 text  predicted_label\n",
            "1   Climate change doesn’t cause volcanic eruption...                0\n",
            "3   North America has experienced an average winte...                0\n",
            "5   HELLO AMERICA,\\n\\nWho would have ever thought ...                0\n",
            "6   fucking hell this weather makes me really fuck...                0\n",
            "11  Ronald Reagan (1989): \"Because changes in the ...                0\n",
            "12  #Geopolitics could have a material impact on #...                0\n",
            "13  The climate crisis is front page news in South...                0\n",
            "17  ... don't be the person that only thinks about...                0\n",
            "18  Is this guy real? BBC News - Climate change: W...                0\n",
            "22  Toronto Ontario; the city that thought snow pl...                0\n",
            "\n",
            "10 Examples of Misinformation (predicted_label 1):\n",
            "                                                 text  predicted_label\n",
            "0   The only solution I’ve ever heard the Left pro...                1\n",
            "2   Vaccinated tennis ball boy collapses in the te...                1\n",
            "4   They're gonna do the same with Climate Change ...                1\n",
            "7   Great to finally have this important UNESCO/SC...                1\n",
            "8   Climate change is one of the world's most pres...                1\n",
            "9   Can people start questioning the \"Johnson got ...                1\n",
            "10  I’m raising two kids and, ya know, I guess I j...                1\n",
            "14  There is a systemic problem with academic mode...                1\n",
            "15  Every day, I wake up to the news of environmen...                1\n",
            "16  I saw my Amish friend Elton today. \"Nice day.\"...                1\n"
          ]
        }
      ],
      "source": [
        "# Extract 10 examples of factual (label == 0)\n",
        "factual_examples = df_climate_inference[df_climate_inference['predicted_label'] == 0].head(10)\n",
        "\n",
        "# Extract 10 examples of misinformation (label == 1)\n",
        "misinfo_examples = df_climate_inference[df_climate_inference['predicted_label'] == 1].head(10)\n",
        "\n",
        "print(\"10 Examples of Factual (predicted_label 0):\")\n",
        "print(factual_examples)\n",
        "\n",
        "print(\"\\n10 Examples of Misinformation (predicted_label 1):\")\n",
        "print(misinfo_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline for RNNs etc (need to rework / a lot not needed)\n",
        "'''\n",
        "vocab_idx = vocab_mapping(tokenized_text=climate_tokenised) # is this correct?? ithink not\n",
        " \n",
        "climate_dl = DataLoader(\n",
        "    dataset=list(zip(climate_tokenised,climate_tokenised[\"label\"])), # THIS SHOULD BE BLANK...\n",
        "    batch_size=32, \n",
        "    shuffle=False, \n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "EMBEDDINGS_FILE_PATH_CLIMATE = \"./cache/mapped_pretrained_embeddings_climate.pkl\"\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH_CLIMATE):\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'rb') as f:\n",
        "        embedding_tensor_climate = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings_climate = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings_climate)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
