{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W5n3aNK6TvsQ",
        "outputId": "3d42ab7b-7a93-4d6b-b948-94acd1764080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "850e0fac-7e02-455c-820b-77035ce993cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(4372) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# download pretrained embeddings -----------------------------------------------\n",
        "# for local\n",
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "# for Colab\n",
        "# !pip install datasets fasttext evaluate\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/Othercomputers/My MacBook Pro/Documents/repositories/nlp/nlp_research_note')\n",
        "#ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "ft = fasttext.load_model(ft_path)\n",
        "\n",
        "# download spacy model for tokenization ----------------------------------------\n",
        "cache_path = './cache/'\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "os.environ['SPACY_DATA'] = cache_path\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# load helper functions & scripts ----------------------------------------------\n",
        "sys.path.append('./aux_scripts')\n",
        "from  misinfo_tokenizer import (get_trained_tokenizer,\n",
        "                                batch_tokenize,\n",
        "                                #vocab_mapping,\n",
        "                                custom_analyzer\n",
        "                                )\n",
        "from data_loader_helpers import (#Collator,\n",
        "                                 embedding_mapping_fasttext\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd7tkDtkpkE",
        "outputId": "bf93912c-46bc-4a76-d5ec-20a88bb636c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found in cache. Downloading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(4395) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache Directory: \n",
            "./cache/huggingface/datasets\n",
            "\n",
            "External Structure: \n",
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "\n",
            "Internal Structure: \n",
            "Dataset({\n",
            "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label'],\n",
            "    num_rows: 92394\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"./cache/huggingface/datasets\")\n",
        "dataset_path = os.path.join(hf_cache_dir, \"roupenminassian\", \"twitter-misinformation\")\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset found in cache: {dataset_path}\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\", cache_dir=hf_cache_dir)\n",
        "else:\n",
        "    print(f\"Dataset not found in cache. Downloading...\")\n",
        "    ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "\n",
        "print(f'Cache Directory: \\n{hf_cache_dir}')\n",
        "print(f'\\nExternal Structure: \\n{ds.shape}')\n",
        "print(f'\\nInternal Structure: \\n{ds[\"train\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "mt7Fa1Y6kpkF",
        "outputId": "95cb04b7-3a39-450f-8edc-5774de6bd47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRE-BALANCING:\n",
            "\n",
            "Train shape (92394, 2) \n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.652737\n",
            "1    0.347263\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.659686\n",
            "1    0.340314\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Post-BALANCING:\n",
            "\n",
            "Train shape (64170, 2) \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fire breaks should have been in California dec...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hurricane Michael's Survivors were wondering w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WASHINGTON (Reuters) - U.S. Secretary of State...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In the midst of preparing for the hurricane it...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Italian newspaper reports that Pope Francis ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Fire breaks should have been in California dec...      0\n",
              "1  Hurricane Michael's Survivors were wondering w...      0\n",
              "2  WASHINGTON (Reuters) - U.S. Secretary of State...      0\n",
              "3  In the midst of preparing for the hurricane it...      0\n",
              "4  Italian newspaper reports that Pope Francis ha...      1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATA PARTITIONING =====================================================================\n",
        "ds_cloned = ds.copy()\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "print(\"PRE-BALANCING:\\n\")\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "# balance train split -------------------------------------------------------------------\n",
        "\n",
        "balancer = RandomUnderSampler(random_state=42, sampling_strategy = 'majority')\n",
        "df_misinfo_train_balanced = pd.concat(balancer.fit_resample(X = df_misinfo_train.iloc[:,[0]],\n",
        "                                                           y = df_misinfo_train.iloc[:,[1]]),\n",
        "                                     axis=1).sample(frac = 1).reset_index(drop=True)\n",
        "df_misinfo_train_balanced.value_counts(\"label\")/df_misinfo_train_balanced.shape[0]\n",
        "\n",
        "print(\"\\nPost-BALANCING:\\n\")\n",
        "print(f\"Train shape {df_misinfo_train_balanced.shape} \\n\")\n",
        "df_misinfo_train_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFX1GRNej28H",
        "outputId": "14e28b9c-0c97-495f-9ae3-ce15f7db7ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized text pkl files found: loading data...\n",
            "Done! Train tokens count: 92394\n",
            "Test tokens count: 10267\n",
            "Train tokens example: [['The', 'tsunami', 'has', 'started', 'President', 'Obama', 's', 'Kenyan', 'half', '-', 'brother', 'wants', 'to', 'make', 'America', 'great', 'again', '  ', 'so', 'he', 's', 'voting', 'for', 'Donald', 'Trump', '.', 'I', 'like', 'Donald', 'Trump', 'because', 'he', 'speaks', 'from', 'the', 'heart', ',', ' ', 'Malik', 'Obama', 'told', 'The', 'Post', 'from', 'his', 'home', 'in', 'the', 'rural', 'village', 'of', 'Kogelo', '.', ' ', 'Make', 'America', 'Great', 'Again', 'is', 'a', 'great', 'slogan', '.', 'I', 'would', 'like', 'to', 'meet', 'him', '.', 'Obama', ',', '58', ',', 'a', 'longtime', 'Democrat', ',', 'said', 'his', ' ', 'deep', 'disappointment', ' ', 'in', 'his', 'brother', 'Barack', 's', 'administration', 'has', 'led', 'him', 'to', 'recently', 'switch', 'allegiance', 'to', ' ', 'the', 'party', 'of', 'Lincoln', '.', 'The', 'last', 'straw', ',', 'he', 'said', ',', 'came', 'earlier', 'this', 'month', 'when', 'FBI', 'Director', 'James', 'Comey', 'recommended', 'not', 'prosecuting', 'Democratic', 'presidential', 'candidate', 'Hillary', 'Clinton', 'over', 'her', 'use', 'of', 'a', 'private', 'e', '-', 'mail', 'servers', 'while', 'secretary', 'of', 'state', '.', 'She', 'should', 'have', 'known', 'better', 'as', 'the', 'custodian', 'of', 'classified', 'information', ',', ' ', 'said', 'Obama', '.', 'He', 's', 'also', 'annoyed', 'that', 'Clinton', 'and', 'President', 'Obama', 'killed', 'Libyan', 'leader', 'Moammar', 'Khadafy', ',', 'whom', 'he', 'called', 'one', 'of', 'his', 'best', 'friends', '.', 'Malik', 'Obama', 'dedicated', 'his', '2012', 'biography', 'of', 'his', 'late', 'father', 'to', 'Khadafy', 'and', 'others', 'who', 'were', ' ', 'making', 'this', 'world', 'a', 'better', 'place', '.', 'I', 'still', 'feel', 'that', 'getting', 'rid', 'of', 'Khadafy', 'didn', 't', 'make', 'things', 'any', 'better', 'in', 'Libya', ',', ' ', 'he', 'said', '.', ' ', 'My', 'brother', 'and', 'the', 'secretary', 'of', 'state', 'disappointed', 'me', 'in', 'that', 'regard', '.', 'But', 'what', 'bothers', 'him', 'even', 'more', 'is', 'the', 'Democratic', 'Party', 's', 'support', 'of', 'same', '-', 'sex', 'marriage', '.', 'Obama', 'plans', 'to', 'trek', 'back', 'to', 'the', 'US', 'to', 'vote', 'for', 'Trump', 'in', 'November', '.', 'Obama', 'used', 'to', 'live', 'in', 'Maryland', ',', 'where', 'he', 'worked', 'for', 'many', 'years', 'as', 'an', 'accountant', 'and', 'is', 'registered', 'to', 'vote', 'there', ',', 'public', 'records', 'show', '.', 'Mr.', 'Trump', 'is', 'providing', 'something', 'new', 'and', 'something', 'fresh', ',', ' ', 'he', 'said', '.', 'For', 'entire', 'story', ':', 'NYP'], ['The', 'only', 'reality', 'show', 'Donald', 'Trump', 'should', 'have', 'ever', 'been', 'featured', 'in', 'is', 'The', 'Biggest', 'Loser', 'because', 'he', 'just', 'got', 'his', 'ass', 'handed', 'to', 'him', 'in', 'court', '.', 'Two', 'years', 'ago', ',', 'Trump', 'National', 'Doral', 'Miami', 'golf', 'resort', 'signed', 'a', 'contract', 'worth', '$', '200,000', 'for', 'a', 'local', 'business', 'called', 'The', 'Paint', 'Spot', 'to', 'provide', 'paint', 'used', 'to', 'renovate', 'the', 'golf', 'course', '.', 'Well', ',', 'guess', 'who', 'tried', 'to', 'stiff', 'The', 'Paint', 'Spot', 'of', 'the', 'final', '$', '<unk>', 'payment', 'in', 'the', '<unk>', ',', 'that', 'would', 'be', 'Republican', 'nominee', 'Donald', 'J.', 'Trump', '.', 'Trump', 'and', 'his', 'company', 'refused', 'to', 'honor', 'the', 'contract', 'by', 'not', 'paying', 'the', 'final', 'payment', ',', 'saying', 'that', 'they', 've', ' ', 'paid', 'enough', ' ', 'for', 'the', 'paint', '.', 'In', 'other', 'words', ',', 'Trump', 'negotiated', 'a', 'deal', 'that', 'ended', 'up', 'costing', 'him', 'more', 'in', 'the', 'end', ',', 'just', 'like', 'the', 'kinds', 'of', 'deals', 'he', 'wants', 'to', 'negotiate', 'for', 'America', 'with', 'the', 'rest', 'of', 'the', 'world', '.', 'Anyway', ',', 'Paint', 'Spot', 'owner', 'Juan', 'Carlos', 'Enriquez', 'filed', 'suit', 'against', 'Trump', 'in', 'court', ',', 'and', 'Judge', 'Jorge', '<unk>', 'just', 'slapped', 'Trump', 'and', 'his', 'company', 'with', 'a', '$', '300,000', 'hit', 'to', 'cover', 'The', 'Paint', 'Shop', 's', 'attorney', 'and', 'court', 'fees', ',', 'nearly', 'ten', 'times', 'more', 'money', 'than', 'the', '$', '<unk>', 'owed', '.', 'And', 'Trump', 'STILL', 'hasn', 't', 'paid', 'that', 'debt', ',', 'so', 'how', 'are', 'we', 'supposed', 'to', 'trust', 'him', 'to', 'pay', 'down', 'the', 'debt', 'of', 'en', 'entire', 'nation', 'if', 'he', 'can', 't', 'meet', 'his', 'own', 'obligations', '?', 'I', 'm', 'happy', 'I', 'have', 'a', 'judgment', ',', ' ', 'Enriquez', 'told', 'the', 'Miami', 'Herald', '.', ' ', 'But', 'he', '[', 'Trump', ']', 'hasn', 't', 'paid', 'yet', '.', 'You', 'know', 'how', 'he', 'says', 'he', 'll', 'surround', 'himself', 'with', 'the', 'greatest', 'people', 'if', 'he', 'is', 'president', '?', 'In', 'this', 'case', ',', 'he', 'might', 'not', 'be', 'surrounded', 'by', 'the', 'right', 'people', '.', 'This', 'isn', 't', 'even', 'the', 'first', 'fine', 'Trump', 'has', 'been', 'ordered', 'to', 'pay', 'this', 'month', '.', 'On', 'the', 'same', 'day', 'as', 'his', 'coronation', 'as', 'the', 'Republican', 'Party', 'nominee', 'last', 'week', ',', 'the', 'National', 'Labor', 'Relations', 'Board', 'slapped', 'Trump', 'with', 'an', '$', '<unk>', 'fine', 'for', 'treating', 'employees', 'like', 'shit', 'because', 'they', 'tried', 'to', 'join', 'a', 'labor', 'union', '.', 'As', 'it', 'turns', 'out', ',', 'one', 'employee', 'was', 'wrongfully', 'fired', 'and', 'the', 'other', 'was', 'retaliated', 'against', 'by', 'denying', 'them', 'the', 'promotion', 'they', 'had', 'earned', '.', 'For', 'someone', 'who', 'claims', 'to', 'be', 'the', ' ', 'law', 'and', 'order', ' ', 'candidate', ',', 'Trump', 'sure', 'does', 'break', 'the', 'law', 'a', 'lot', '.', 'Featured', 'Image', ':', 'Christopher', 'Furlong', '/', 'Getty', 'Images'], ['No', 'Food', ',', 'No', 'FEMA', ':', 'Hurricane', 'Michael', '’s', 'Survivors', 'Are', 'Furious', 'https://thebea.st/2CIXNoz', '\\xa0'], ['WASHINGTON', '(', 'Reuters', ')', '-', 'Here', 'are', 'some', 'of', 'the', 'highlights', 'of', 'the', 'Reuters', 'interview', 'with', 'U.S.', 'President', 'Donald', 'Trump', 'on', 'Thursday', '.', '“', 'There', '’s', 'a', 'chance', 'that', 'we', 'could', 'end', 'up', 'having', 'a', 'major', ',', 'major', ',', 'conflict', 'with', 'North', 'Korea', ',', 'absolutely', '.', '”', 'QUESTION', ':', 'Is', 'that', 'your', 'biggest', 'global', 'worry', 'at', 'this', 'point', '?', '“', 'Yes', ',', 'I', 'would', 'say', 'that', '’s', 'true', ',', 'yes', '.', '...', 'North', 'Korea', 'would', 'be', 'certainly', 'that', '.', '”', 'ON', 'GETTING', 'SOUTH', 'KOREA', 'TO', 'PAY', 'FOR', 'THAAD', 'MISSILE', 'DEFENSE', 'SYSTEM', '“', 'On', 'the', 'THAAD', 'system', ',', 'it', '’s', 'about', 'a', 'billion', 'dollars', '.', 'I', 'said', ',', ' ', '‘', 'Why', 'are', 'we', 'paying', '?', 'Why', 'are', 'we', 'paying', 'a', 'billion', 'dollars', '?', 'We', '’re', 'protecting', '.', 'Why', 'are', 'we', 'paying', 'a', 'billion', 'dollars', '?', '’', 'So', 'I', 'informed', 'South', 'Korea', 'it', 'would', 'be', 'appropriate', 'if', 'they', 'paid', '.', 'Nobody', '’s', 'going', 'to', 'do', 'that', '.', 'Why', 'are', 'we', 'paying', 'a', 'billion', 'dollars', '?', 'It', '’s', 'a', 'billion', 'dollar', 'system', '.', 'It', '’s', 'phenomenal', '.', 'It', '’s', 'the', 'most', 'incredible', 'equipment', 'you', '’ve', 'ever', 'seen', '-', 'shoots', 'missiles', 'right', 'out', 'of', 'the', 'sky', '.', 'And', 'it', 'protects', 'them', 'and', 'I', 'want', 'to', 'protect', 'them', '.', 'We', '’re', 'going', 'to', 'protect', 'them', '.', 'But', 'they', 'should', 'pay', 'for', 'that', ',', 'and', 'they', 'understand', 'that', '.', '”', 'ON', 'WHETHER', 'THE', 'WAR', 'AGAINST', '<unk>', '<unk>', 'WILL', 'EVER', 'END', '“', 'Yours', 'is', 'the', 'toughest', 'question', '.', 'Because', 'at', 'what', 'point', 'does', 'it', 'end', '?', 'But', 'we', 'ca', 'n’t', 'let', 'them', 'come', 'over', 'here', '.', 'I', 'have', 'to', 'say', ',', 'there', 'is', 'an', 'end', '.', 'And', 'it', 'has', 'to', 'be', 'humiliation', '.', 'There', 'is', 'an', 'end', '.', 'Otherwise', 'it', '’s', 'really', 'tough', '.', 'But', 'there', 'is', 'an', 'end', '.', 'We', 'are', 'really', 'eradicating', 'some', 'very', 'bad', 'people', '.', 'When', 'you', 'take', 'a', 'look', 'at', 'what', '’s', 'going', 'on', 'with', 'the', 'cutting', 'off', 'of', 'the', 'heads', '.', 'We', 'have', 'n’t', 'seen', 'that', 'since', 'Medieval', 'times', '.', 'Right', '?', '”', ' ', 'ON', 'CHINESE', 'PRESIDENT', 'XI', '’S', 'EFFORTS', 'TO', '<unk>', 'IN', 'NORTH', 'KOREA', '“', 'He', 'certainly', 'does', 'n’t', 'want', 'to', 'see', 'turmoil', 'and', 'death', '.', 'He', 'does', 'n’t', 'want', 'to', 'see', 'it', '.', 'He', '’s', 'a', 'good', 'man', '.', 'He', '’s', 'a', 'very', 'good', 'man', 'and', 'I', 'got', 'to', 'know', 'him', 'very', 'well', '...', 'We', '’ll', 'see', 'how', 'it', 'all', 'works', 'out', '.', 'I', 'know', 'he', 'would', 'like', 'to', 'be', 'able', 'to', 'do', 'something', '.', 'Perhaps', 'it', '’s', 'possible', 'that', 'he', 'ca', 'n’t', '.', 'But', 'I', 'think', 'he', '’d', 'like', 'to', 'be', 'able', 'to', 'do', 'something', '.', '”', '“', 'He', '’s', '27', 'years', 'old', ',', 'his', 'father', 'dies', ',', 'took', 'over', 'a', 'regime', ',', 'so', 'say', 'what', 'you', 'want', 'but', 'that', '’s', 'not', 'easy', ',', 'especially', 'at', 'that', 'age', '.', 'You', 'know', 'you', 'have', 'plenty', 'of', 'generals', 'in', 'there', 'and', 'plenty', 'of', 'other', 'people', 'that', 'would', 'like', 'to', 'do', 'what', 'he', '’s', 'doing', '.', 'So', 'I', '’ve', 'said', 'this', 'before', 'and', 'I', '’ve', ',', 'I', '’m', 'just', 'telling', 'you', ',', 'and', 'I', '’m', 'not', 'giving', 'him', 'credit', 'or', 'not', 'giving', 'him', 'credit', '.', 'I', '’m', 'just', 'saying', 'that', '’s', 'a', 'very', 'hard', 'thing', 'to', 'do', '.', '”', '“', 'As', 'to', 'whether', 'or', 'not', 'he', '’s', 'rational', ',', 'I', 'have', 'no', 'opinion', 'on', 'it', '.', 'I', 'hope', 'he', '’s', 'rational', '.', '”', '“', 'I', 'get', 'a', 'call', 'from', 'Mexico', 'yesterday', ',', '‘', 'We', 'hear', 'you', '’re', 'going', 'to', 'terminate', 'NAFTA', '.', '’', 'I', 'said', 'that', '’s', 'right', '.', 'They', 'said', ',', '‘', 'Is', 'there', 'any', 'way', 'we', 'can', 'do', 'something', 'without', 'you', '–', 'without', 'termination', '?', '’', 'I', 'said', ',', '‘', 'What', 'do', 'you', 'want', 'to', 'do', '?', '’', 'He', 'said', ',', '‘', 'Well', ',', 'we', '’d', 'like', 'to', 'negotiate', '.', '’', 'I', 'said', 'we', '’ll', 'think', 'about', 'it', '.', 'Then', 'I', 'get', 'a', 'call', ',', 'and', 'they', 'call', 'me', ',', 'I', 'get', 'a', 'call', 'from', 'Justin', 'Trudeau', 'and', 'he', 'said', ',', '‘', 'We', '’d', 'like', 'to', 'see', 'if', 'we', 'can', 'work', 'something', 'out', ',', '’', 'and', 'I', 'said', 'that', '’s', 'fine', '.', 'Because', 'I', '’ve', 'always', '-', 'I', '’ve', 'been', 'very', 'consistent', '.', 'It', '’s', 'much', 'less', 'disruptive', 'if', 'we', 'can', 'make', 'a', 'fair', 'trade', 'deal', 'than', 'if', 'we', 'terminate', '.', '”', '“', 'It', '’s', 'unacceptable', '.', 'It', '’s', 'a', 'horrible', 'deal', 'made', 'by', 'Hillary', '.', 'It', '’s', 'a', 'horrible', 'deal', '.', 'And', 'we', '’re', 'going', 'to', 'renegotiate', 'that', 'deal', ',', 'or', 'terminate', 'it', '.', '”', ' ', 'QUESTION', ':', 'When', 'will', 'you', 'announce', 'it', '?', '“', 'Very', 'soon', '.', 'I', '’m', 'announcing', 'it', 'now', '.', '”', '“', 'By', 'the', 'way', ',', 'with', 'South', 'Korea', ',', 'just', 'so', 'you', 'know', '.', 'They', '’re', 'ready', 'for', 'it', '.', 'Mike', 'Pence', 'was', 'representing', 'me', ',', 'he', 'was', 'just', 'over', 'there', ',', 'he', '’s', 'told', 'them', '.', 'And', 'we', 'have', 'the', 'five', '-', 'year', 'anniversary', 'coming', 'up', 'very', 'shortly', '.', 'And', 'we', 'thought', 'that', 'would', 'be', 'a', 'good', 'time', 'to', 'start', '...', 'It', '’s', 'a', 'great', 'deal', 'for', 'South', 'Korea', '.', 'It', '’s', 'a', 'terrible', 'deal', 'for', 'us', '.', '”', '“', 'Frankly', ',', 'Saudi', 'Arabia', 'has', 'not', 'treated', 'us', 'fairly', ',', 'because', 'we', 'are', 'losing', 'a', 'tremendous', 'amount', 'of', 'money', 'in', 'defending', 'Saudi', 'Arabia', '.', '”', '“', 'Well', ',', 'my', 'problem', 'is', 'that', 'I', '’ve', 'established', 'a', 'very', 'good', 'personal', 'relationship', 'with', '(', 'Chinese', ')', 'President', 'Xi', '.', 'And', 'I', 'really', 'feel', 'that', 'he', 'is', 'doing', 'everything', 'in', 'his', 'power', 'to', 'help', 'us', 'with', 'a', 'big', 'situation', ',', 'so', 'I', 'would', 'n’t', 'want', 'to', 'be', 'causing', 'difficulty', 'right', 'now', 'for', 'him', '...', 'So', 'I', 'would', 'certainly', 'want', 'to', 'speak', 'to', 'him', 'first', '.', '”', '“', 'If', 'there', '’s', 'closure', ',', 'there', '’s', 'closure', '.', 'We', '’ll', 'see', 'what', 'happens', '.', 'If', 'there', '’s', 'a', 'shutdown', '.', 'It', '’s', 'the', 'Democrats', '’', 'fault', '.', 'Not', 'our', 'fault', '.', 'It', '’s', 'the', 'Democrats', '’', 'fault', '.', 'Maybe', 'they', '’d', 'like', 'to', 'see', 'a', 'shutdown', '.', '”', ' ', 'ON', 'TRUMP', '’S', 'PLAN', 'TO', '<unk>', '<unk>', 'TO', '<unk>', 'TAX', 'CUTS', '“', 'We', 'will', 'do', 'trade', 'deals', 'that', 'are', 'going', 'to', 'make', 'up', 'for', 'a', 'tremendous', 'amount', 'of', 'the', 'deficit', '.', 'We', 'are', 'going', 'to', 'be', 'doing', 'trade', 'deals', 'that', 'are', 'going', 'to', 'be', 'much', 'better', 'trade', 'deals', '...', ' ', '“', 'There', 'will', 'be', 'other', 'ways', 'that', 'we', 'are', 'going', 'to', 'raise', 'revenues', '.', 'But', 'we', 'are', 'going', 'to', 'run', 'the', 'country', 'properly', ',', 'and', 'we', 'are', 'going', 'to', 'be', 'reimbursed', 'when', 'we', 'do', 'things', '.', 'Why', 'should', 'we', 'be', 'paying', 'for', 'somebody', 'else', '’s', 'military', '?', '”', 'ON', 'MIDDLE', 'EAST', 'PEACE', 'AND', 'POSSIBLE', 'TRIP', 'TO', 'ISRAEL', ',', 'SAUDI', 'ARABIA', '“', 'It', '’s', 'a', 'possibility', ',', 'we', '’re', 'talking', 'to', 'both', '.', 'It', '’s', 'a', 'possibility', ',', 'but', 'I', 'want', 'to', 'see', 'peace', 'with', 'Israel', 'and', 'the', 'Palestinians', '.', 'There', 'is', 'no', 'reason', 'there', '’s', 'not', 'peace', 'between', 'Israel', 'and', 'the', 'Palestinians', '-', 'none', 'whatsoever', '.', 'So', 'we', '’re', 'looking', 'at', 'that', 'and', 'we', '’re', 'also', 'looking', 'at', 'the', 'potential', 'of', 'going', 'to', 'Saudi', 'Arabia', '.', '”']]\n"
          ]
        }
      ],
      "source": [
        "# TOKENISATION ==========================================================================\n",
        "\n",
        "tokenizer_file = './cache/misinfo_tokenizer.pkl'\n",
        "train_tokens_file = './cache/misinfo_train_tokens.pkl'\n",
        "test_tokens_file = './cache/misinfo_test_tokens.pkl'\n",
        "\n",
        "if os.path.exists(train_tokens_file) and os.path.exists(test_tokens_file):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load pre-saved tokenized data\n",
        "    with open(train_tokens_file, 'rb') as f:\n",
        "        misinfo_train_tokens = pickle.load(f)\n",
        "    with open(test_tokens_file, 'rb') as f:\n",
        "        misinfo_test_tokens = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    # 1) Get the trained tokenizer (will create if it doesn't exist)\n",
        "    #    'df_misinfo_train[\"text\"]' is used to fit the vocabulary\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=tokenizer_file,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # 2) Tokenize train data in batches (using custom_analyzer)\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokens = batch_tokenize(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, misinfo_tokenizer)  \n",
        "    )\n",
        "\n",
        "    # 3) Tokenize test data in batches (using custom_analyzer)\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokens = batch_tokenize(\n",
        "        df_misinfo_test[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, misinfo_tokenizer)\n",
        "    )\n",
        "\n",
        "    # 4) Save the tokenized train and test data\n",
        "    with open(train_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokens, f)\n",
        "    with open(test_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokens, f)\n",
        "\n",
        "print(\"Done! Train tokens count:\", len(misinfo_train_tokens))\n",
        "print(\"Test tokens count:\", len(misinfo_test_tokens))\n",
        "\n",
        "print(f\"Train tokens example:\", misinfo_train_tokens[1:5]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab indexing...\n",
            "Vocab size: 71957\n",
            "Vocab example: [('<pad>', 0), ('<unk>', 12), ('the', 2), (',', 3), ('.', 4), ('to', 5), ('of', 6), ('and', 7), ('a', 8), ('in', 9)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "print (\"Vocab indexing...\")\n",
        "\n",
        "#getting vocab_mapping() from misinfo_tokenizer.py\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokens)\n",
        "\n",
        "print(f\"Vocab size: {len(vocab_idx)}\")\n",
        "print(f\"Vocab example: {list(vocab_idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating data loaders...\n",
            "Created data loaders!\n",
            "Dimensions:\n",
            "torch.Size([32, 300]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "print(\"Creating data loaders...\")\n",
        "\n",
        "def collate_fn(data):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in data:\n",
        "        # integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "    label_list = torch.tensor(label_list)\n",
        "    # padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokens,\n",
        "                                         df_misinfo_train_balanced[\"label\"])),\n",
        "                        batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokens,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Created data loaders!\")\n",
        "\n",
        "print(\"Dimensions:\")\n",
        "for x, y in train_dl:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d06330da-b6a8-40c7-a9b4-f4bda78aaa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mapping pretrained fasttext embeddings to vocabulary indices...\n",
            "Emebddings pre-exists: loaded embeddings from ./cache/mapped_pretrained_embeddings.pkl. Shape: torch.Size([71957, 300])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "print(\"Mapping pretrained fasttext embeddings to vocabulary indices...\")\n",
        "\n",
        "# Define the file path for the pickle file\n",
        "embeddings_file_path = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "# for Hertie GPU:\n",
        "#pickle_file_path = \"/workspace/workspace/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "# Check if the pickle file already exists\n",
        "if os.path.exists(embeddings_file_path):\n",
        "    # If the file exists, load it from the pickle file\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {embeddings_file_path}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    # If the file does not exist, proceed with creating the embeddings and save them\n",
        "    # Load pre-trained FastText model\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "\n",
        "    # Map pretrained FastText embeddings to vocabulary indices\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    print(\"Mapped!\")\n",
        "\n",
        "    # Convert mapped embeddings to a tensor\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save the embeddings to a pickle file\n",
        "    with open(embeddings_file_path, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {embeddings_file_path}. Shape: {embedding_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_dl, test_dl, use_lengths=False):\n",
        "    \"\"\"Trains and evaluates a binary classification model (CNN, RNN, LSTM, etc.).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): PyTorch model\n",
        "        num_epochs (int): Number of epochs\n",
        "        train_dl (DataLoader): DataLoader for training\n",
        "        test_dl (DataLoader): DataLoader for testing/validation\n",
        "        use_lengths (bool): Whether to compute and pass lengths to the model (for LSTMs, RNNs, etc.)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Create an optimizer (Adam by default)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_hist_train = [0] * num_epochs\n",
        "    accuracy_hist_train = [0] * num_epochs\n",
        "    f1_hist_train = [0] * num_epochs\n",
        "    loss_hist_test = [0] * num_epochs\n",
        "    accuracy_hist_test = [0] * num_epochs\n",
        "    f1_hist_test = [0] * num_epochs\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ### ------------------ TRAINING LOOP ------------------ ###\n",
        "        model.train()\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_dl):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # (1) Optionally compute sequence lengths, if required:\n",
        "            if use_lengths:\n",
        "                lengths = torch.sum(x_batch != 0, dim=1)\n",
        "                pred = model(x_batch, lengths)[:, 0]\n",
        "            else:\n",
        "                pred = model(x_batch)[:, 0]\n",
        "\n",
        "            # (2) Compute loss\n",
        "            loss = nn.BCEWithLogitsLoss()(pred, y_batch.float())\n",
        "\n",
        "            # (3) Backprop and update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # (4) Bookkeeping: accumulate metrics\n",
        "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
        "\n",
        "            # Binary accuracy: (pred >= 0.5) vs actual\n",
        "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "            accuracy_hist_train[epoch] += is_correct.sum()\n",
        "\n",
        "            # Store predictions & labels for F1\n",
        "            all_train_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            # (5) Print batch progress\n",
        "            if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(train_dl):\n",
        "                print(f\"    Batch {batch_idx+1}/{len(train_dl)}: Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Compute average training metrics for the epoch\n",
        "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        f1_hist_train[epoch] = f1_score(all_train_labels, all_train_preds)\n",
        "\n",
        "        ### ------------------ EVALUATION LOOP ------------------ ###\n",
        "        model.eval()\n",
        "        all_test_preds = []\n",
        "        all_test_labels = []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x_batch, y_batch) in enumerate(test_dl):\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                # Optionally compute lengths for test set\n",
        "                if use_lengths:\n",
        "                    lengths = torch.sum(x_batch != 0, dim=1)\n",
        "                    pred = model(x_batch, lengths)[:, 0]\n",
        "                else:\n",
        "                    pred = model(x_batch)[:, 0]\n",
        "\n",
        "                loss = nn.BCEWithLogitsLoss()(pred, y_batch.float())\n",
        "\n",
        "                # Bookkeeping\n",
        "                loss_hist_test[epoch] += loss.item() * y_batch.size(0)\n",
        "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "                accuracy_hist_test[epoch] += is_correct.sum()\n",
        "                all_test_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "                # Print batch progress\n",
        "                if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(test_dl):\n",
        "                    print(f\"    Batch {batch_idx + 1}/{len(test_dl)}: Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Compute average test metrics for the epoch\n",
        "        loss_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        accuracy_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        f1_hist_test[epoch] = f1_score(all_test_labels, all_test_preds)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Accuracy: {accuracy_hist_train[epoch]:.3f}, F1: {f1_hist_train[epoch]:.3f}\")\n",
        "        print(f\"    Test  - Accuracy: {accuracy_hist_test[epoch]:.3f}, F1: {f1_hist_test[epoch]:.3f}\")\n",
        "\n",
        "    return (loss_hist_train, loss_hist_test, accuracy_hist_train,\n",
        "            accuracy_hist_test, f1_hist_train, f1_hist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8qQ1LhWHrXP-",
        "outputId": "3469629f-d284-41b2-fac6-f3c8c480c93a"
      },
      "outputs": [],
      "source": [
        "# STEP 4: MODEL BUILDING ================================================================\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = TextClassificationModel(embedding_tensor=embedding_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2nd try\n",
        "def train(model, num_epochs, train_dl, test_dl):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss() \n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "        \n",
        "    loss_hist_train = [0] * num_epochs\n",
        "    accuracy_hist_train = [0] * num_epochs\n",
        "    f1_hist_train = [0] * num_epochs\n",
        "    loss_hist_test = [0] * num_epochs\n",
        "    accuracy_hist_test = [0] * num_epochs\n",
        "    f1_hist_test = [0] * num_epochs\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        model.train()  \n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        \n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_dl):\n",
        "            \n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            \n",
        "            # forward pass\n",
        "            pred = model(x_batch)[:, 0]  # generate predictions\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()  # compute gradients\n",
        "            optimizer.step()  # update parameters\n",
        "            optimizer.zero_grad()  # reset gradients\n",
        "\n",
        "            # evaluate train\n",
        "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
        "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "            accuracy_hist_train[epoch] += is_correct.sum()\n",
        "            all_train_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            # Print batch progress\n",
        "            if (batch_idx + 1) % 500 == 0 or (batch_idx + 1) == len(train_dl):\n",
        "                print(f\"    Batch {batch_idx + 1}/{len(train_dl)}: \"\n",
        "                      f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        f1_hist_train[epoch] = f1_score(all_train_labels, all_train_preds)\n",
        "\n",
        "        # evaluate model\n",
        "        model.eval()  # set evaluation mode\n",
        "        all_test_preds = []\n",
        "        all_test_labels = []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x_batch, y_batch) in enumerate(test_dl):\n",
        "                # forward pass\n",
        "                pred = model(x_batch)[:, 0]\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # evaluate test\n",
        "                loss_hist_test[epoch] += loss.item() * y_batch.size(0)\n",
        "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "                accuracy_hist_test[epoch] += is_correct.sum()\n",
        "                all_test_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "                # Print batch progress\n",
        "                if (batch_idx + 1) % 500 == 0 or (batch_idx + 1) == len(test_dl):\n",
        "                    print(f\"    Batch {batch_idx + 1}/{len(test_dl)}: \"\n",
        "                          f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        accuracy_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        f1_hist_test[epoch] = f1_score(all_test_labels, all_test_preds)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Accuracy: {accuracy_hist_train[epoch]:.3f}, F1: {f1_hist_train[epoch]:.3f}\")\n",
        "        print(f\"    Test  - Accuracy: {accuracy_hist_test[epoch]:.3f}, F1: {f1_hist_test[epoch]:.3f}\")\n",
        "\n",
        "    return [loss_hist_train, loss_hist_test, accuracy_hist_train,\n",
        "            accuracy_hist_test, f1_hist_train, f1_hist_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3rd try\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, num_epochs, train_dl, test_dl):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize metrics\n",
        "    metrics = {\n",
        "        \"loss_train\": [],\n",
        "        \"loss_test\": [],\n",
        "        \"accuracy_train\": [],\n",
        "        \"accuracy_test\": [],\n",
        "        \"f1_train\": [],\n",
        "        \"f1_test\": []\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for x_batch, y_batch in tqdm(train_dl, desc=\"Training\", leave=False):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(x_batch)[:, 0]\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item() * y_batch.size(0)\n",
        "            preds = (pred >= 0.5).float()\n",
        "            train_correct += (preds == y_batch).float().sum().item()\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        # Epoch metrics\n",
        "        metrics[\"loss_train\"].append(train_loss / len(train_dl.dataset))\n",
        "        metrics[\"accuracy_train\"].append(train_correct / len(train_dl.dataset))\n",
        "        metrics[\"f1_train\"].append(f1_score(all_train_labels, all_train_preds))\n",
        "\n",
        "        # Evaluate model\n",
        "        model.eval()\n",
        "        test_loss, test_correct = 0, 0\n",
        "        all_test_preds, all_test_labels = [], []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in tqdm(test_dl, desc=\"Evaluating\", leave=False):\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                pred = model(x_batch)[:, 0]\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # Metrics\n",
        "                test_loss += loss.item() * y_batch.size(0)\n",
        "                preds = (pred >= 0.5).float()\n",
        "                test_correct += (preds == y_batch).float().sum().item()\n",
        "                all_test_preds.extend(preds.cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        # Epoch metrics\n",
        "        metrics[\"loss_test\"].append(test_loss / len(test_dl.dataset))\n",
        "        metrics[\"accuracy_test\"].append(test_correct / len(test_dl.dataset))\n",
        "        metrics[\"f1_test\"].append(f1_score(all_test_labels, all_test_preds))\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Loss: {metrics['loss_train'][-1]:.4f}, Accuracy: {metrics['accuracy_train'][-1]:.3f}, F1: {metrics['f1_train'][-1]:.3f}\")\n",
        "        print(f\"    Test  - Loss: {metrics['loss_test'][-1]:.4f}, Accuracy: {metrics['accuracy_test'][-1]:.3f}, F1: {metrics['f1_test'][-1]:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "hist_cnn = train(model_cnn, num_epochs=10, train_dl=train_dl, test_dl=test_dl)\n",
        "torch.save(model_cnn, \"./models/cnn_model_full.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist_rnn = train(model_rnn, num_epochs, train_dl, test_dl, use_lengths=True) # fluctuating f1 scores, exploding gradients\n",
        "torch.save(model_rnn, \"./models/rnn_model_full.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist_lstm = train(model_lstm, num_epochs=10, train_dl=train_dl, test_dl=test_dl, use_lengths=True)\n",
        "torch.save(model_lstm, \"./models/lstm_model_full.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm_stacked = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "hist = train(model_lstm_stacked, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "torch.save(model_lstm_stacked, \"./models/lstm_stacked_model_full.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_bi_lstm = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "hist = train(model_bi_lstm, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "torch.save(model_bi_lstm, \"./models/bi_lstm_model_full.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "\n",
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['factual', 'misinfo']),\n",
        "})\n",
        "\n",
        "# Convert train and test data to Hugging Face Dataset\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(dataset_train[2]) \n",
        "\n",
        "# Check the unique values of the 'label' column to ensure the classes are correct\n",
        "unique_labels = set(dataset_train['label'])\n",
        "print(\"Unique label values in training data:\", unique_labels)\n",
        "\n",
        "# Check the mapping of integer labels to class names\n",
        "print(\"Class name mapping:\", dataset_train.features['label'].int2str)\n",
        "\n",
        "# Create a Hugging Face DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "\n",
        "# Print the DatasetDict to check its contents\n",
        "print(f'\\n {dataset_dict}')\n",
        "\n",
        "print(dataset_train[2]) \n",
        "print(dataset_dict['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return bert_tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./transformer_results\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=False,\n",
        "                                  use_cpu=False)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    bert_uncased,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer after training\n",
        "trainer.save_model(\"./models/transformer_results\")  \n",
        "bert_tokenizer.save_pretrained(\"./models/transformer_results\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict -------------------------------------------------------------------------------\n",
        "bert_uncased.eval()\n",
        "\n",
        "# Helper function to process data in batches\n",
        "def batch_predict(model, tokenizer, texts, batch_size=16, max_length=512):\n",
        "    all_preds = []\n",
        "    # Check if GPU is available and move model to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, len(texts), batch_size):\n",
        "            end = min(start + batch_size, len(texts))\n",
        "            batch_texts = texts[start:end]\n",
        "\n",
        "            # Tokenize the batch of texts\n",
        "            tokenized_batch = tokenizer(batch_texts, truncation=True, padding=\"max_length\",\n",
        "                                        max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "            # Move tensors to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                tokenized_batch = {key: value.cuda() for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(**tokenized_batch)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(predictions, dim=1)\n",
        "            all_preds.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "# Prepare your dataset\n",
        "disinfo_test_texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "# Make predictions in batches\n",
        "predicted_labels = batch_predict(bert_uncased, bert_tokenizer, disinfo_test_texts, batch_size=16)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Climate Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
        "\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
        "df_climate_inference['label'] = None\n",
        "\n",
        "print(f\"Train shape {df_climate_inference.shape} \\n\")\n",
        "df_climate_inference.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TOKENISATION ==========================================================================\n",
        "\n",
        "# Check if the pickle files already exist\n",
        "# local\n",
        "climate_tokens_file = './cache/climate_tokens.pkl'\n",
        "\n",
        "# for Hertie GPU:\n",
        "# climate_tokens_file = '/workspace/workspace/cache/climate_tokens.pkl'\n",
        "\n",
        "\n",
        "if os.path.exists(climate_tokens_file):\n",
        "    print(\"Tokenized climate tweets pkl files found: loading data...\")\n",
        "    # Load the pre-saved tokenized data\n",
        "    with open(climate_tokens_file, 'rb') as f:\n",
        "        climate_tokens = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization on climate tweets...\")\n",
        "\n",
        "    print(\"Loading spaCy model...\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=tokenizer_file,\n",
        "        min_df=3\n",
        "    )\n",
        "    \n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "  \n",
        "    print(\"Tokenizing Climate Data in Batches...\")\n",
        "    climate_tokens = batch_tokenize(\n",
        "        df_climate_inference, \n",
        "        32,\n",
        "        misinfo_tokenizer_analyzer,\n",
        "    )\n",
        "\n",
        "    # Save tokenized train and test data\n",
        "    with open(climate_tokens_file, 'wb') as f:\n",
        "        pickle.dump(climate_tokens, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "climate_tokens[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "print (\"Vocab indexing\")\n",
        "\n",
        "vocab_idx_climate = vocab_mapping(tokenized_text=climate_tokens)\n",
        "\n",
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "print(\"Creating data loaders\")\n",
        "climate_dl = DataLoader(\n",
        "    dataset=list(zip(climate_tokens, climate_tokens[\"label\"])),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=Collator(vocab_idx, max_seq_length)\n",
        ")\n",
        "\n",
        "print(\"Created data loaders!\")\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "# Define the file path for the pickle file\n",
        "# for local:\n",
        "embeddings_file_path = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "# for Hertie GPU:\n",
        "#pickle_file_path = \"/workspace/workspace/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "# Check if the pickle file already exists\n",
        "if os.path.exists(embeddings_file_path):\n",
        "    # If the file exists, load it from the pickle file\n",
        "    with open(embeddings_file_path, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {embeddings_file_path}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    # If the file does not exist, proceed with creating the embeddings and save them\n",
        "    # Load pre-trained FastText model\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    # Map pretrained FastText embeddings to vocabulary indices\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx_climate,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "\n",
        "    # Convert mapped embeddings to a tensor\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save the embeddings to a pickle file\n",
        "    with open(embeddings_file_path, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {embeddings_file_path}. Shape: {embedding_tensor.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
