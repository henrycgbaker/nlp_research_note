{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/henrybaker/miniconda3/envs/nlp_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets fasttext evaluate\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import warnings\n",
        "import sys\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "W5n3aNK6TvsQ",
        "outputId": "3d42ab7b-7a93-4d6b-b948-94acd1764080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "850e0fac-7e02-455c-820b-77035ce993cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(3888) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nsys.path.append('./aux_scripts')\\nfrom  misinfo_tokenizer import (get_trained_tokenizer,\\n                                batch_tokenize,\\n                                #vocab_mapping,\\n                                custom_analyzer\\n                                )\\nfrom data_loader_helpers import (#Collator,\\n                                 embedding_mapping_fasttext\\n                                 )\\n\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download pretrained embeddings -----------------------------------------------\n",
        "# for local\n",
        "#fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "# for Colab\n",
        "# !pip install datasets fasttext evaluate\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/Othercomputers/My MacBook Pro/Documents/repositories/nlp/nlp_research_note')\n",
        "#ft_path = \"./cc.en.300.bin\"\n",
        "\n",
        "ft = fasttext.load_model(ft_path)\n",
        "\n",
        "# download spacy model for tokenization ----------------------------------------\n",
        "cache_path = './cache/'\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "os.environ['SPACY_DATA'] = cache_path\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# load helper functions & scripts ----------------------------------------------\n",
        "'''\n",
        "sys.path.append('./aux_scripts')\n",
        "from  misinfo_tokenizer import (get_trained_tokenizer,\n",
        "                                batch_tokenize,\n",
        "                                #vocab_mapping,\n",
        "                                custom_analyzer\n",
        "                                )\n",
        "from data_loader_helpers import (#Collator,\n",
        "                                 embedding_mapping_fasttext\n",
        "                                 )\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Path to dataset files: /Users/henrybaker/.cache/kagglehub/datasets/saurabhshahane/fake-news-classification/versions/77\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
            "        num_rows: 72134\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(3895) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "misinfo_path = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
        "print(\"Path to dataset files:\", misinfo_path)\n",
        "\n",
        "ds = load_dataset(misinfo_path)\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n",
            "Train shape (5000, 2) \n",
            "\n",
            "(0 = fake and 1 = real).\n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "1    0.51\n",
            "0    0.49\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "1    0.5172\n",
            "0    0.4828\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26093</th>\n",
              "      <td>Share on Twitter The Wildfire is an opinion pl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8593</th>\n",
              "      <td>Military British Defense Secretary Michael Fal...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4551</th>\n",
              "      <td>Missing from much of the news media coverage o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12755</th>\n",
              "      <td>BERLIN (Reuters) - It is  incredibly unlikely ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56423</th>\n",
              "      <td>FRANKFURT (Reuters) - Germany’s largest bank h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "26093  Share on Twitter The Wildfire is an opinion pl...      1\n",
              "8593   Military British Defense Secretary Michael Fal...      1\n",
              "4551   Missing from much of the news media coverage o...      0\n",
              "12755  BERLIN (Reuters) - It is  incredibly unlikely ...      0\n",
              "56423  FRANKFURT (Reuters) - Germany’s largest bank h...      0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DATA PARTITIONING =====================================================================\n",
        "ds_cloned = ds.copy()\n",
        "\n",
        "\n",
        "df = ds_cloned['train'].to_pandas()\n",
        "\n",
        "columns_to_remove = ['Unnamed: 0', 'title']\n",
        "df = df.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "print(df[\"text\"].isnull().sum())  \n",
        "df = df.dropna(subset=[\"text\"])\n",
        "\n",
        "df_misinfo_train, df_misinfo_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "df_misinfo_test = df_misinfo_test.sample(n=5000, random_state=42) # REMOVE THIS\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print('(0 = fake and 1 = real).\\n')\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized text pkl files found: loading data...\n",
            "Train inputs tokenised: 5000\n",
            "Test inputs tokenised: 5000\n"
          ]
        }
      ],
      "source": [
        "# DEFINE TOKENIZATION FLOW =====================================================================\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", \n",
        "                 disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "    \"\"\"\n",
        "    Uses the custom_tokenizer, then replaces out-of-vocabulary tokens with <unk>.\n",
        "    \"\"\"\n",
        "    tokens = custom_tokenizer(text)\n",
        "    vocab = trained_tokenizer.vocabulary_\n",
        "    return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "def get_trained_tokenizer(text_series, tokenizer_file=None, min_df=3):\n",
        "    \"\"\"\n",
        "    1) Checks if a previously fitted tokenizer exists in tokenizer_file.\n",
        "    2) If not, create a new CountVectorizer, fit it on 'text_series'.\n",
        "    3) Save the fitted tokenizer if tokenizer_file is provided.\n",
        "    4) Return the tokenizer.\n",
        "    \"\"\"\n",
        "    # If a tokenizer file path is given and exists, load it\n",
        "    if tokenizer_file and os.path.exists(tokenizer_file):\n",
        "        print(f\"Tokenizer file '{tokenizer_file}' found. Loading it...\")\n",
        "        with open(tokenizer_file, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        # Otherwise, create a new one and fit\n",
        "        print(\"No pre-fitted tokenizer found or no file specified. Creating a new one...\")\n",
        "        tokenizer = CountVectorizer(\n",
        "            analyzer=\"word\",\n",
        "            tokenizer=custom_tokenizer,  # We define custom_tokenizer for splitting\n",
        "            lowercase=False,\n",
        "            min_df=min_df\n",
        "        )\n",
        "        tokenizer.fit(text_series)\n",
        "        \n",
        "        # Save the tokenizer if a path was provided\n",
        "        if tokenizer_file:\n",
        "            print(f\"Saving fitted tokenizer to '{tokenizer_file}'...\")\n",
        "            with open(tokenizer_file, 'wb') as f:\n",
        "                pickle.dump(tokenizer, f)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def batch_tokenize(text_series, batch_size, analyzer_func):\n",
        "    \"\"\"\n",
        "    Tokenizes a Pandas Series of text in batches to avoid memory issues.\n",
        "    \"\"\"\n",
        "    tokenized_result = []\n",
        "    total = len(text_series)\n",
        "    num_batches = (total // batch_size) + (1 if total % batch_size != 0 else 0)\n",
        "    \n",
        "    for batch_idx in range(0, total, batch_size):\n",
        "        \n",
        "        # Print progress every 200 batches or at the last batch\n",
        "        if (batch_idx // batch_size + 1) % 200 == 0 or (batch_idx + batch_size >= total):\n",
        "            print(f'Tokenizing batch {batch_idx // batch_size + 1} of {num_batches}...')\n",
        "        \n",
        "        batch_texts = text_series[batch_idx : batch_idx + batch_size]\n",
        "        for text in batch_texts:\n",
        "            tokenized_result.append(analyzer_func(text))\n",
        "    \n",
        "    return tokenized_result\n",
        "\n",
        "# TOKENIZATION ==========================================================================\n",
        "\n",
        "TOKENIZER_DIR = './cache/misinfo_tokenizer.pkl'\n",
        "TRAIN_TOKENISED_DIR = './cache/misinfo_train_tokenised.pkl'\n",
        "TEST_TOKENISED_DIR = './cache/misinfo_test_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(TRAIN_TOKENISED_DIR) and os.path.exists(TEST_TOKENISED_DIR):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load pre-saved tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_train_tokenised = pickle.load(f)\n",
        "    with open(TEST_TOKENISED_DIR, 'rb') as f:\n",
        "        misinfo_test_tokenised = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    # 1) Train tokenizer\n",
        "    misinfo_tokenizer = get_trained_tokenizer(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        tokenizer_file=TOKENIZER_DIR,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # Build the default analyzer from our tokenizer\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    # 2) Tokenize train data in batches using the built analyzer (trained on train set)\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokenised = batch_tokenize(\n",
        "        df_misinfo_train[\"text\"],\n",
        "        32,\n",
        "        misinfo_tokenizer_analyzer\n",
        "    )\n",
        "    \n",
        "    # 3) Tokenize test data in batches using custom_analyzer (which replaces OOV tokens with <unk>)\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokenised = batch_tokenize(\n",
        "        df_misinfo_test[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer)\n",
        "    )\n",
        "\n",
        "    # Optionally, save the tokenized data\n",
        "    with open(TRAIN_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokenised, f)\n",
        "    with open(TEST_TOKENISED_DIR, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokenised, f)\n",
        "\n",
        "print(\"Train inputs tokenised:\", len(misinfo_train_tokenised))\n",
        "print(\"Test inputs tokenised:\", len(misinfo_test_tokenised))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 83115\n",
            "Vocab example: [('<pad>', 0), ('<unk>', 1), (',', 2), ('the', 3), ('.', 4), ('to', 5), ('of', 6), ('and', 7), ('a', 8), ('in', 9)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokenised)\n",
        "\n",
        "print(f\"Vocab size: {len(vocab_idx)}\")\n",
        "print(f\"Vocab example: {list(vocab_idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(data, include_lengths=True):\n",
        "    text_list, label_list, lengths = [], [], []\n",
        "    for _text, _label in data:\n",
        "        # Integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    # Padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    if include_lengths:\n",
        "        return padded_text_list, label_list, lengths\n",
        "    else:\n",
        "        return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300 # too long for full RNNs\n",
        "batch_size = 32\n",
        "\n",
        "# standard dls with collate_fn\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, \n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=True))\n",
        "\n",
        "# dls w/o collate_fn for CNNs:\n",
        "train_dl_cnn = DataLoader(dataset=list(zip(misinfo_train_tokenised,\n",
        "                                         df_misinfo_train[\"label\"])),\n",
        "                        batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))\n",
        "\n",
        "test_dl_cnn = DataLoader(dataset=list(zip(misinfo_test_tokenised,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                         batch_size=32, shuffle=True,\n",
        "                        collate_fn=lambda x: collate_fn(x, include_lengths=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d06330da-b6a8-40c7-a9b4-f4bda78aaa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emebddings pre-exists: loaded embeddings from ./cache/mapped_pretrained_embeddings.pkl. Shape: torch.Size([83115, 300])\n"
          ]
        }
      ],
      "source": [
        "# EMBEDDING MAPPING =====================================================================\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "EMBEDDINGS_FILE_PATH = \"./cache/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH):\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH}. Shape: {embedding_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, train_dl, test_dl, use_lengths=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss_train\": [],\n",
        "        \"loss_test\": [],\n",
        "        \"accuracy_train\": [],\n",
        "        \"accuracy_test\": [],\n",
        "        \"f1_train\": [],\n",
        "        \"f1_test\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch in tqdm(train_dl, desc=\"Training\", leave=False):\n",
        "            if use_lengths:\n",
        "                x_batch, y_batch, lengths = batch\n",
        "                x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                pred = model(x_batch, lengths)[:, 0]  # Include lengths for RNNs/LSTMs\n",
        "            else:\n",
        "                x_batch, y_batch = batch\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                pred = model(x_batch)[:, 0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Metrics\n",
        "            train_loss += loss.item() * y_batch.size(0)\n",
        "            preds = (pred >= 0.5).float()\n",
        "            train_correct += (preds == y_batch).float().sum().item()\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_train\"].append(train_loss / len(train_dl.dataset))\n",
        "        metrics[\"accuracy_train\"].append(train_correct / len(train_dl.dataset))\n",
        "        metrics[\"f1_train\"].append(f1_score(all_train_labels, all_train_preds))\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss, test_correct = 0, 0\n",
        "        all_test_preds, all_test_labels = [], []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dl, desc=\"Evaluating\", leave=False):\n",
        "                if use_lengths:\n",
        "                    x_batch, y_batch, lengths = batch\n",
        "                    x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
        "                    pred = model(x_batch, lengths)[:, 0]\n",
        "                else:\n",
        "                    x_batch, y_batch = batch\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                    pred = model(x_batch)[:, 0]\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # Metrics\n",
        "                test_loss += loss.item() * y_batch.size(0)\n",
        "                preds = (pred >= 0.5).float()\n",
        "                test_correct += (preds == y_batch).float().sum().item()\n",
        "                all_test_preds.extend(preds.cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        metrics[\"loss_test\"].append(test_loss / len(test_dl.dataset))\n",
        "        metrics[\"accuracy_test\"].append(test_correct / len(test_dl.dataset))\n",
        "        metrics[\"f1_test\"].append(f1_score(all_test_labels, all_test_preds))\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Loss: {metrics['loss_train'][-1]:.4f}, Accuracy: {metrics['accuracy_train'][-1]:.3f}, F1: {metrics['f1_train'][-1]:.3f}\")\n",
        "        print(f\"    Test  - Loss: {metrics['loss_test'][-1]:.4f}, Accuracy: {metrics['accuracy_test'][-1]:.3f}, F1: {metrics['f1_test'][-1]:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8qQ1LhWHrXP-",
        "outputId": "3469629f-d284-41b2-fac6-f3c8c480c93a"
      },
      "outputs": [],
      "source": [
        "# MODEL BUILDING ================================================================\n",
        "\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = TextClassificationModel(embedding_tensor=embedding_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from disk.\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "if os.path.exists(\"./models/cnn_model.pth\"):\n",
        "    model_cnn = torch.load(\"./models/cnn_model.pth\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    print(\"Training the model...\")\n",
        "    num_epochs = 10\n",
        "    hist_cnn = train(model_cnn, num_epochs, train_dl_cnn, test_dl_cnn, use_lengths=False)\n",
        "    print(\"Training the model...\")\n",
        "    with open(\"./models/train_hist/cnn_hist.pkl\", \"wb\") as f:\n",
        "        pickle.dump(hist_cnn, f)\n",
        "    torch.save(model_cnn, \"./models/cnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from disk.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "if os.path.exists(\"./models/rnn_model.pth\"):\n",
        "    model_cnn = torch.load(\"./models/rnn_model.pth\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    print(\"Training the model...\")\n",
        "    hist_rnn = train(model_rnn, num_epochs, train_dl, test_dl, use_lengths=True) \n",
        "    with open(\"./models/train_hist/rnn_hist.pkl\", \"wb\") as f:\n",
        "        pickle.dump(hist_rnn, f)\n",
        "\n",
        "    torch.save(model_rnn, \"./models/rnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from disk.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "if os.path.exists(\"./models/lstm_model.pth\"):\n",
        "    model_cnn = torch.load(\"./models/lstm_model.pth\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    print(\"Training the model...\")\n",
        "    hist_lstm = train(model_lstm, num_epochs=10, train_dl=train_dl, test_dl=test_dl, use_lengths=True)\n",
        "    with open(\"./models/train_hist/lstm_hist.pkl\", \"wb\") as f:\n",
        "        pickle.dump(hist_rnn, f)\n",
        "    torch.save(model_lstm, \"./models/lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from disk.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_lstm_stacked = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "if os.path.exists(\"./models/lstm_stacked_model.pth\"):\n",
        "    model_lstm_stacked = torch.load(\"./models/lstm_stacked_model.pth\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    print(\"Training the model...\")\n",
        "    hist_lstm_stacked = train(model_lstm_stacked, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "    with open(\"./models/train_hist/lstm_stacked_hist.pkl\", \"wb\") as f:\n",
        "        pickle.dump(hist_lstm_stacked, f)\n",
        "    torch.save(model_lstm_stacked, \"./models/lstm_stacked_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from disk.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_bi_lstm = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "\n",
        "if os.path.exists(\"./models/bi_lstm_model.pth\"):\n",
        "    model_bi_lstm = torch.load(\"./models/bi_lstm_model.pth\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    print(\"Training the model...\")\n",
        "    hist_bi_lstm = train(model_bi_lstm, num_epochs, train_dl, test_dl, use_lengths=True)\n",
        "    with open(\"./models/train_hist/bi_lstm_hist.pkl\", \"wb\") as f:\n",
        "        pickle.dump(hist_bi_lstm, f)\n",
        "    torch.save(model_bi_lstm, \"./models/bi_lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "\n",
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Missing from much of the news media coverage of FBI Director James Comey’s testimony last week that   top Hillary Clinton aide Huma Abedin regularly forwarded classified emails to her husband, Anthony Weiner, is that the revelation directly contradicts Abedin’s own claims from last November about how the emails famously made it to Weiner’s laptop computer. [During last Wednesday’s Senate Judiciary Committee hearing on FBI oversight, Comey stated Abedin appears to have engaged in a “regular practice of forwarding  ” to Weiner’s computer, possibly for him to print out.  The testimony, which came during an exchange with Sen. John Kennedy ( ) about classified emails found on Weiner’s computer, went thusly: KENNEDY: OK. Was there classified information on  —   on former Congressman Weiner’s computer? COMEY: Yes. KENNEDY: Who sent it to him? COMEY: His then spouse, Huma Abedin, appears to have had a regular practice of forwarding   to him, for him I think to print out for her so she could then deliver them to the Secretary of State. Those details seem to contradict claims made by Abedin last November through her lawyer, Karen Dunn, immediately following reports that the FBI discovered possible Clinton emails on her estranged husband’s computer. Dunn claimed that Abedin did not know how the emails could have reached Weiner’s device and only learned about the possibility of emails being on Weiner’s laptop via media reports. “She only learned for the first time on Friday, from press reports, of the possibility that a laptop belonging to Mr. Weiner could contain emails of hers. While the FBI has not contacted us about this, Ms. Abedin will continue to be, as she always has been, forthcoming and cooperative,” Dunn said in the statement. Politico further quoted a “source familiar with Abedin’s account” as stating that, according to Politico’s portrayal, that Abedin “has told colleagues she was taken aback when she learned that the FBI found her emails on a laptop belonging to her estranged husband, Anthony Weiner, and doesn’t know how the messages got there. ” That’s a far cry from Comey’s testimony that Abedin appears to have regularly forwarded Clinton emails to Weiner. Weiner was seemingly trusted in Clinton’s inner circle. As Breitbart News reported last November, one Clinton email released at the time had Clinton asking Abedin whether a trusted staff member working for Weiner could deliver a secure cell phone to Clinton. Meanwhile, Comey’s revelation has some calling for a special prosecutor to probe Abedin. Last week, Sen. Richard Blumenthal ( ) was asked on MSNBC’s Morning Joe whether he believes Abedin’s actions were potentially criminal. “If there was classified information and it was improperly passed to a person unauthorized to receive it. Yes, it’s a crime,” he said. “Without knowing what the intentions were and so forth, there is potentially a prosecutable crime. ” “Should it have been prosecuted?” host Joe Scarborough asked. “It still may be potentially. It’s not outside the statute of limitations. Who will decide it? That’s why we need a special prosecutor to review all of this investigative material,” Blumenthal replied. Aaron Klein is Breitbart’s Jerusalem bureau chief and senior investigative reporter. He is a New York Times bestselling author and hosts the popular weekend talk radio program, “Aaron Klein Investigative Radio. ” Follow him on Twitter @AaronKleinShow. Follow him on Facebook. With research by Joshua Klein.', 'label': 0}\n",
            "Unique label values in training data: {0, 1}\n",
            "Class name mapping: <bound method ClassLabel.int2str of ClassLabel(names=['factual', 'misinfo'], id=None)>\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n",
            "{'text': 'Missing from much of the news media coverage of FBI Director James Comey’s testimony last week that   top Hillary Clinton aide Huma Abedin regularly forwarded classified emails to her husband, Anthony Weiner, is that the revelation directly contradicts Abedin’s own claims from last November about how the emails famously made it to Weiner’s laptop computer. [During last Wednesday’s Senate Judiciary Committee hearing on FBI oversight, Comey stated Abedin appears to have engaged in a “regular practice of forwarding  ” to Weiner’s computer, possibly for him to print out.  The testimony, which came during an exchange with Sen. John Kennedy ( ) about classified emails found on Weiner’s computer, went thusly: KENNEDY: OK. Was there classified information on  —   on former Congressman Weiner’s computer? COMEY: Yes. KENNEDY: Who sent it to him? COMEY: His then spouse, Huma Abedin, appears to have had a regular practice of forwarding   to him, for him I think to print out for her so she could then deliver them to the Secretary of State. Those details seem to contradict claims made by Abedin last November through her lawyer, Karen Dunn, immediately following reports that the FBI discovered possible Clinton emails on her estranged husband’s computer. Dunn claimed that Abedin did not know how the emails could have reached Weiner’s device and only learned about the possibility of emails being on Weiner’s laptop via media reports. “She only learned for the first time on Friday, from press reports, of the possibility that a laptop belonging to Mr. Weiner could contain emails of hers. While the FBI has not contacted us about this, Ms. Abedin will continue to be, as she always has been, forthcoming and cooperative,” Dunn said in the statement. Politico further quoted a “source familiar with Abedin’s account” as stating that, according to Politico’s portrayal, that Abedin “has told colleagues she was taken aback when she learned that the FBI found her emails on a laptop belonging to her estranged husband, Anthony Weiner, and doesn’t know how the messages got there. ” That’s a far cry from Comey’s testimony that Abedin appears to have regularly forwarded Clinton emails to Weiner. Weiner was seemingly trusted in Clinton’s inner circle. As Breitbart News reported last November, one Clinton email released at the time had Clinton asking Abedin whether a trusted staff member working for Weiner could deliver a secure cell phone to Clinton. Meanwhile, Comey’s revelation has some calling for a special prosecutor to probe Abedin. Last week, Sen. Richard Blumenthal ( ) was asked on MSNBC’s Morning Joe whether he believes Abedin’s actions were potentially criminal. “If there was classified information and it was improperly passed to a person unauthorized to receive it. Yes, it’s a crime,” he said. “Without knowing what the intentions were and so forth, there is potentially a prosecutable crime. ” “Should it have been prosecuted?” host Joe Scarborough asked. “It still may be potentially. It’s not outside the statute of limitations. Who will decide it? That’s why we need a special prosecutor to review all of this investigative material,” Blumenthal replied. Aaron Klein is Breitbart’s Jerusalem bureau chief and senior investigative reporter. He is a New York Times bestselling author and hosts the popular weekend talk radio program, “Aaron Klein Investigative Radio. ” Follow him on Twitter @AaronKleinShow. Follow him on Facebook. With research by Joshua Klein.', 'label': 0}\n",
            "{'text': 'Missing from much of the news media coverage of FBI Director James Comey’s testimony last week that   top Hillary Clinton aide Huma Abedin regularly forwarded classified emails to her husband, Anthony Weiner, is that the revelation directly contradicts Abedin’s own claims from last November about how the emails famously made it to Weiner’s laptop computer. [During last Wednesday’s Senate Judiciary Committee hearing on FBI oversight, Comey stated Abedin appears to have engaged in a “regular practice of forwarding  ” to Weiner’s computer, possibly for him to print out.  The testimony, which came during an exchange with Sen. John Kennedy ( ) about classified emails found on Weiner’s computer, went thusly: KENNEDY: OK. Was there classified information on  —   on former Congressman Weiner’s computer? COMEY: Yes. KENNEDY: Who sent it to him? COMEY: His then spouse, Huma Abedin, appears to have had a regular practice of forwarding   to him, for him I think to print out for her so she could then deliver them to the Secretary of State. Those details seem to contradict claims made by Abedin last November through her lawyer, Karen Dunn, immediately following reports that the FBI discovered possible Clinton emails on her estranged husband’s computer. Dunn claimed that Abedin did not know how the emails could have reached Weiner’s device and only learned about the possibility of emails being on Weiner’s laptop via media reports. “She only learned for the first time on Friday, from press reports, of the possibility that a laptop belonging to Mr. Weiner could contain emails of hers. While the FBI has not contacted us about this, Ms. Abedin will continue to be, as she always has been, forthcoming and cooperative,” Dunn said in the statement. Politico further quoted a “source familiar with Abedin’s account” as stating that, according to Politico’s portrayal, that Abedin “has told colleagues she was taken aback when she learned that the FBI found her emails on a laptop belonging to her estranged husband, Anthony Weiner, and doesn’t know how the messages got there. ” That’s a far cry from Comey’s testimony that Abedin appears to have regularly forwarded Clinton emails to Weiner. Weiner was seemingly trusted in Clinton’s inner circle. As Breitbart News reported last November, one Clinton email released at the time had Clinton asking Abedin whether a trusted staff member working for Weiner could deliver a secure cell phone to Clinton. Meanwhile, Comey’s revelation has some calling for a special prosecutor to probe Abedin. Last week, Sen. Richard Blumenthal ( ) was asked on MSNBC’s Morning Joe whether he believes Abedin’s actions were potentially criminal. “If there was classified information and it was improperly passed to a person unauthorized to receive it. Yes, it’s a crime,” he said. “Without knowing what the intentions were and so forth, there is potentially a prosecutable crime. ” “Should it have been prosecuted?” host Joe Scarborough asked. “It still may be potentially. It’s not outside the statute of limitations. Who will decide it? That’s why we need a special prosecutor to review all of this investigative material,” Blumenthal replied. Aaron Klein is Breitbart’s Jerusalem bureau chief and senior investigative reporter. He is a New York Times bestselling author and hosts the popular weekend talk radio program, “Aaron Klein Investigative Radio. ” Follow him on Twitter @AaronKleinShow. Follow him on Facebook. With research by Joshua Klein.', 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['factual', 'misinfo']),\n",
        "})\n",
        "\n",
        "df_misinfo_train = df_misinfo_train.reset_index(drop=True)\n",
        "df_misinfo_test = df_misinfo_test.reset_index(drop=True)\n",
        "\n",
        "# Convert train and test data to Hugging Face Dataset\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(dataset_train[2]) \n",
        "\n",
        "# Check the unique values of the 'label' column to ensure the classes are correct\n",
        "unique_labels = set(dataset_train['label'])\n",
        "print(\"Unique label values in training data:\", unique_labels)\n",
        "\n",
        "# Check the mapping of integer labels to class names\n",
        "print(\"Class name mapping:\", dataset_train.features['label'].int2str)\n",
        "\n",
        "# Create a Hugging Face DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "\n",
        "# Print the DatasetDict to check its contents\n",
        "print(f'\\n {dataset_dict}')\n",
        "\n",
        "print(dataset_train[2]) \n",
        "print(dataset_dict['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 157/4710 [00:45<19:57,  3.80it/s]Using the latest cached version of the module from /Users/henrybaker/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Sun Dec 22 00:40:44 2024) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.\n",
            "\n",
            "  3%|▎         | 157/4710 [01:02<19:57,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3986719846725464, 'eval_accuracy': 0.8476, 'eval_f1': 0.8559001512859304, 'eval_runtime': 17.232, 'eval_samples_per_second': 290.157, 'eval_steps_per_second': 9.111, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 313/4710 [01:47<20:08,  3.64it/s]  \n",
            "  7%|▋         | 314/4710 [02:13<20:08,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.20952405035495758, 'eval_accuracy': 0.9256, 'eval_f1': 0.929277566539924, 'eval_runtime': 25.7787, 'eval_samples_per_second': 193.959, 'eval_steps_per_second': 6.09, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 10%|█         | 471/4710 [03:13<19:27,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1440412551164627, 'eval_accuracy': 0.9452, 'eval_f1': 0.9480667172100076, 'eval_runtime': 15.7192, 'eval_samples_per_second': 318.082, 'eval_steps_per_second': 9.988, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 500/4710 [03:21<19:46,  3.55it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3264, 'grad_norm': 1.0890783071517944, 'learning_rate': 4.469214437367304e-05, 'epoch': 3.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 13%|█▎        | 628/4710 [04:16<19:04,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.11397890746593475, 'eval_accuracy': 0.9618, 'eval_f1': 0.9632762930205729, 'eval_runtime': 18.9952, 'eval_samples_per_second': 263.225, 'eval_steps_per_second': 8.265, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 17%|█▋        | 785/4710 [05:16<18:15,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.11012965440750122, 'eval_accuracy': 0.9626, 'eval_f1': 0.9636822684016314, 'eval_runtime': 16.006, 'eval_samples_per_second': 312.383, 'eval_steps_per_second': 9.809, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \n",
            " 20%|██        | 942/4710 [06:17<17:33,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.11685270816087723, 'eval_accuracy': 0.9652, 'eval_f1': 0.9659090909090909, 'eval_runtime': 16.1464, 'eval_samples_per_second': 309.666, 'eval_steps_per_second': 9.724, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 1000/4710 [06:34<19:08,  3.23it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.065, 'grad_norm': 15.168498039245605, 'learning_rate': 3.9384288747346076e-05, 'epoch': 6.37}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            " 23%|██▎       | 1099/4710 [07:18<16:43,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1109643280506134, 'eval_accuracy': 0.9686, 'eval_f1': 0.9698829848455783, 'eval_runtime': 15.9643, 'eval_samples_per_second': 313.198, 'eval_steps_per_second': 9.834, 'epoch': 7.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 27%|██▋       | 1256/4710 [08:19<16:00,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1391686350107193, 'eval_accuracy': 0.9636, 'eval_f1': 0.964299725382503, 'eval_runtime': 16.1086, 'eval_samples_per_second': 310.394, 'eval_steps_per_second': 9.746, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 30%|███       | 1413/4710 [09:20<15:07,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.12772008776664734, 'eval_accuracy': 0.9696, 'eval_f1': 0.97041650447645, 'eval_runtime': 16.58, 'eval_samples_per_second': 301.569, 'eval_steps_per_second': 9.469, 'epoch': 9.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 1500/4710 [09:46<15:22,  3.48it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0228, 'grad_norm': 1.2410106658935547, 'learning_rate': 3.407643312101911e-05, 'epoch': 9.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            " 33%|███▎      | 1570/4710 [10:23<14:48,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14833740890026093, 'eval_accuracy': 0.9652, 'eval_f1': 0.965748031496063, 'eval_runtime': 17.2221, 'eval_samples_per_second': 290.324, 'eval_steps_per_second': 9.116, 'epoch': 10.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 37%|███▋      | 1727/4710 [11:24<13:57,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14136789739131927, 'eval_accuracy': 0.9696, 'eval_f1': 0.9704739704739704, 'eval_runtime': 16.2225, 'eval_samples_per_second': 308.213, 'eval_steps_per_second': 9.678, 'epoch': 11.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 40%|████      | 1884/4710 [12:26<13:20,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.18439048528671265, 'eval_accuracy': 0.961, 'eval_f1': 0.9614090639224223, 'eval_runtime': 16.7099, 'eval_samples_per_second': 299.223, 'eval_steps_per_second': 9.396, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 2000/4710 [12:59<12:53,  3.50it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0096, 'grad_norm': 0.01946254074573517, 'learning_rate': 2.8768577494692145e-05, 'epoch': 12.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            " 43%|████▎     | 2041/4710 [13:27<12:21,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1631179004907608, 'eval_accuracy': 0.9678, 'eval_f1': 0.9684003925417075, 'eval_runtime': 16.2629, 'eval_samples_per_second': 307.449, 'eval_steps_per_second': 9.654, 'epoch': 13.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                     \n",
            " 47%|████▋     | 2198/4710 [14:30<13:29,  3.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.15312129259109497, 'eval_accuracy': 0.9696, 'eval_f1': 0.9703588143525741, 'eval_runtime': 17.4972, 'eval_samples_per_second': 285.76, 'eval_steps_per_second': 8.973, 'epoch': 14.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 2198/4710 [14:30<16:35,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 870.9334, 'train_samples_per_second': 172.229, 'train_steps_per_second': 5.408, 'train_loss': 0.09705758778152952, 'epoch': 14.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# tokenising the texts ================================================================\n",
        "\n",
        "if os.path.exists(\"./data/misinfo_bert_tokenized\"):\n",
        "    tokenized_datasets = DatasetDict.load_from_disk(\"./data/misinfo_bert_tokenized\")\n",
        "    \n",
        "else:\n",
        "    print(\"Tokenizing...\")                                      \n",
        "    \n",
        "    def tokenize_function(dataset):\n",
        "        return bert_tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "        # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets\n",
        "\n",
        "    tokenized_datasets['train'][0]['text']\n",
        "    tokenized_datasets['train'][0]['label']\n",
        "    tokenized_datasets['train'][0]['input_ids']\n",
        "    tokenized_datasets['train'][0]['attention_mask']\n",
        "    \n",
        "    tokenized_datasets.save_to_disk(\"./data/misinfo_bert_tokenized\")\n",
        "\n",
        "# fine-tuning the model ================================================================\n",
        "    \n",
        "if os.path.exists(\"./models/transformer_results/bert_uncased_finetuned.safetensor\"):\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained(\"./models/transformer_results\")\n",
        "    bert_uncased = AutoModelForSequenceClassification.from_pretrained(\"./models/transformer_results\")\n",
        "    print(\"Model loaded from disk.\")\n",
        "else:\n",
        "    \n",
        "    print(\"Fine tuning...\")\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=\"./models/transformer_results/bert_uncased_finetuned\",\n",
        "                                    eval_strategy=\"epoch\",\n",
        "                                    save_strategy=\"epoch\",\n",
        "                                    per_device_train_batch_size=32,\n",
        "                                    per_device_eval_batch_size=32,\n",
        "                                    num_train_epochs=30,\n",
        "                                    load_best_model_at_end=True,\n",
        "                                    metric_for_best_model='f1',\n",
        "                                    disable_tqdm=False,\n",
        "                                    use_cpu=False)\n",
        "\n",
        "    def compute_metrics(eval_preds):\n",
        "        metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "        logits, labels = eval_preds\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        bert_uncased,\n",
        "        training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer after training\n",
        "    trainer.save_model(\"./models/transformer_results\")  \n",
        "    bert_tokenizer.save_pretrained(\"./models/transformer_results\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "F1 Score: 0.9704739704739704\n",
            "Accuracy: 0.9696\n"
          ]
        }
      ],
      "source": [
        "def batch_predict(model, tokenizer, texts, batch_size=16, device='mps', max_length=512):\n",
        "    \"\"\"\n",
        "    Predict labels for a batch of texts using the specified model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "        model: The pre-trained model (e.g., BERT).\n",
        "        tokenizer: The tokenizer associated with the pre-trained model.\n",
        "        texts: List of input texts to predict.\n",
        "        batch_size: Number of samples per batch.\n",
        "        device: Device to use ('mps', 'cuda', or 'cpu').\n",
        "        max_length: Maximum sequence length for tokenization.\n",
        "\n",
        "    Returns:\n",
        "        List of predicted labels.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for prediction\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            # Tokenize the batch with truncation and padding\n",
        "            tokenized_batch = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Move tokenized inputs to the same device as the model\n",
        "            tokenized_batch = {key: value.to(device) for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(**tokenized_batch)\n",
        "\n",
        "            # Apply softmax to logits and determine predicted labels\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "            # Collect predictions\n",
        "            predictions.extend(predicted_labels.cpu().numpy())  # Move predictions to CPU before storing\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# USAGE\n",
        "# set device\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare dataset\n",
        "disinfo_test_texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "# predictions in batches\n",
        "predicted_labels = batch_predict(bert_uncased, bert_tokenizer, disinfo_test_texts, batch_size=16, device=device)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "could do a load here about why the non-transformers didn't work so well (maybe a tweet is not long enough, didn;t do any hyperparameter tuning, non-decreasing LR means it often overshot; but of interest is that Transformers worked, so will take that moving forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Climate Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from '/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv'...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserScreenName</th>\n",
              "      <th>UserName</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Text</th>\n",
              "      <th>Embedded_text</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Retweets</th>\n",
              "      <th>Image link</th>\n",
              "      <th>Tweet URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lauren Boebert</td>\n",
              "      <td>@laurenboebert</td>\n",
              "      <td>2022-01-17T23:32:38.000Z</td>\n",
              "      <td>Lauren Boebert\\n@laurenboebert\\n·\\nJan 18</td>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1,683</td>\n",
              "      <td>2,259</td>\n",
              "      <td>11.7K</td>\n",
              "      <td>[]</td>\n",
              "      <td>https://twitter.com/laurenboebert/status/14832...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserScreenName        UserName                 Timestamp  \\\n",
              "0  Lauren Boebert  @laurenboebert  2022-01-17T23:32:38.000Z   \n",
              "\n",
              "                                        Text  \\\n",
              "0  Lauren Boebert\\n@laurenboebert\\n·\\nJan 18   \n",
              "\n",
              "                                       Embedded_text Emojis Comments  Likes  \\\n",
              "0  The only solution I’ve ever heard the Left pro...    NaN    1,683  2,259   \n",
              "\n",
              "  Retweets Image link                                          Tweet URL  \n",
              "0    11.7K         []  https://twitter.com/laurenboebert/status/14832...  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of inference dataset: (9050, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only solution I’ve ever heard the Left pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Climate change doesn’t cause volcanic eruption...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The only solution I’ve ever heard the Left pro...\n",
              "1  Climate change doesn’t cause volcanic eruption..."
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_climate_inference = df_climate[['Embedded_text']].rename(columns={'Embedded_text': 'text'})\n",
        "print(f\"Shape of inference dataset: {df_climate_inference.shape}\")\n",
        "df_climate_inference.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pickle file not found. Tokenizing climate tweets...\n",
            "Tokenized climate tweets and saved to file.\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# TRANSFORMER INFERENCE ================================================================\n",
        "\n",
        "model_path = \"./models/transformer_results\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "bert_uncased = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Tokenize climate tweets\n",
        "CLIMATE_TOKENISED_BERT_DIR = './cache/climate_tokenised_bert.pkl'\n",
        "\n",
        "if os.path.exists(CLIMATE_TOKENISED_BERT_DIR):\n",
        "    print(\"Tokenized climate tweets pkl file found. Loading data...\")\n",
        "    with open(CLIMATE_TOKENISED_BERT_DIR, 'rb') as f:\n",
        "        climate_tokenised_bert = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle file not found. Tokenizing climate tweets...\")\n",
        "    climate_tokenised_bert = bert_tokenizer(\n",
        "        list(df_climate_inference[\"text\"]),  \n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with open(CLIMATE_TOKENISED_BERT_DIR, 'wb') as f:\n",
        "        pickle.dump(climate_tokenised_bert, f)\n",
        "    print(\"Tokenized climate tweets and saved to file.\")\n",
        "\n",
        "# Predict using fine-tuned BERT model\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def predict_climate_tweets(model, tokenized_texts, batch_size=32, device='mps'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(tokenized_texts['input_ids']), batch_size):\n",
        "            batch_input_ids = tokenized_texts['input_ids'][i:i + batch_size].to(device)\n",
        "            batch_attention_mask = tokenized_texts['attention_mask'][i:i + batch_size].to(device)\n",
        "\n",
        "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(preds, dim=1)\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "predicted_labels = predict_climate_tweets(bert_uncased, climate_tokenised_bert, batch_size=32, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_labels[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text predicted_label\n",
            "0  The only solution I’ve ever heard the Left pro...  misinformation\n",
            "1  Climate change doesn’t cause volcanic eruption...  misinformation\n",
            "2  Vaccinated tennis ball boy collapses in the te...  misinformation\n",
            "3  North America has experienced an average winte...         factual\n",
            "4  They're gonna do the same with Climate Change ...  misinformation\n",
            "Predictions saved to /Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv.\n"
          ]
        }
      ],
      "source": [
        "df_climate_inference['predicted_label'] = predicted_labels\n",
        "\n",
        "# Replace 0 with 'factual' and 1 with 'misinformation' in the 'predicted_label' column\n",
        "df_climate_inference['predicted_label'] = df_climate_inference['predicted_label'].replace({0: 'factual', 1: 'misinformation'})\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(df_climate_inference.head())\n",
        "\n",
        "# Save predictions to CSV\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_bert.csv\"\n",
        "df_climate_inference.to_csv(output_path_climate, index=False)\n",
        "print(f\"Predictions saved to {output_path_climate}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribution of Factual vs Misinformation:\n",
            "predicted_label\n",
            "misinformation    8640\n",
            "factual            410\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAIiCAYAAAApTdcdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHMElEQVR4nO3deVxUZf//8fewDSibIKsS4PIzcd9SrDSLxMTMpdI0t0xbMHOp1LpzuyvLcs3Su+5cKr3TbDM1jcSlzN1cU8vENBWwVBAXUDi/P4L5OuICCAx5Xs/HYx4P5jrXnPM5w8zw5jrXOWMxDMMQAACAiTk5ugAAAABHIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxChzBs9erQsFkupbOuuu+7SXXfdZbu/atUqWSwWLVy4sFS237t3b0VERJTKtooqIyNDjz/+uIKDg2WxWDRo0CBHl1TqSvM1WRJmz54ti8WigwcPFupxFotFo0ePLtI2L168qBdeeEFhYWFycnJShw4dirSesqiozyfKFgIRSlXeB0fezd3dXaGhoYqNjdXUqVN1+vTpYtnO0aNHNXr0aG3btq1Y1lecynJtBfHaa69p9uzZeuqpp/TRRx+pR48eV+0bERFh9/u+9Hb+/PlirWvevHmaPHlysa6zrLvrrrtksVhUvXr1Ky5PSEiwPd+lFeqvZubMmXrzzTf14IMPas6cORo8eLBD6ymK1157TV9++aWjy0AJcXF0ATCnsWPHKjIyUhcuXFBycrJWrVqlQYMGaeLEiVq0aJHq1q1r6/uvf/1Lw4cPL9T6jx49qjFjxigiIkL169cv8OO+/fbbQm2nKK5V2/vvv6+cnJwSr+FGJCYmqlmzZho1alSB+tevX19Dhw7N1+7m5lasdc2bN0+7du0y3YiVu7u79u/fr40bN+q2226zWzZ37ly5u7vnC589evRQ165dZbVaC7Wtc+fOycWlaH82EhMTValSJU2aNKlIjy8LXnvtNT344IP5RreK+nyibCEQwSHuu+8+NW7c2HZ/xIgRSkxMVLt27dS+fXvt2bNHHh4ekiQXF5cifwgX1NmzZ1WuXLli/yNdWK6urg7dfkGkpqYqKiqqwP0rVaqkRx99tAQrMreqVavq4sWL+t///mcXiM6fP68vvvhCcXFx+uyzz+we4+zsLGdn50Jvy93dvch1pqamytfXt8iPv1xOTo6ysrJuqKbiUtTnE2ULh8xQZtx99916+eWX9fvvv+vjjz+2tV9pvkZCQoLuuOMO+fr6ytPTUzVq1NCLL74o6e95P02aNJEk9enTx3bIYPbs2ZL+PsxQu3ZtbdmyRS1atFC5cuVsj718DlGe7OxsvfjiiwoODlb58uXVvn17HT582K5PRESEevfune+xl67zerVdaQ7RmTNnNHToUIWFhclqtapGjRp66623ZBiGXT+LxaIBAwboyy+/VO3atWW1WlWrVi0tW7bsyk/4ZVJTU9W3b18FBQXJ3d1d9erV05w5c2zL8+ZTJSUlacmSJbbab2TexKxZs3T33XcrMDBQVqtVUVFRmj59+hX7fvPNN2rZsqW8vLzk7e2tJk2aaN68eZL+fo6XLFmi33//3VZX3vN4tfkdefuzatUqW9v333+vhx56SLfccousVqvCwsI0ePBgnTt3rtD7NmDAAHl6eurs2bP5lj3yyCMKDg5Wdna2JGnz5s2KjY1VxYoV5eHhocjISD322GMF3tYjjzyi+fPn240ufv311zp79qwefvjhfP2v9JwUpIbL5xDlvTf379+v3r17y9fXVz4+PurTp49tvw8ePCiLxaKVK1dq9+7dtt9P3vNe2Nf33LlzVatWLVmtVi1btsy2Lz/88IMGDhyogIAA+fr66oknnlBWVpZOnTqlnj17qkKFCqpQoYJeeOGFfOt+66231Lx5c/n7+8vDw0ONGjXKd4jRYrHozJkzmjNnjm0f8t7vV3uNvfvuu7ZaQ0NDFR8fr1OnTtn1yfs8+vnnn9WqVSuVK1dOlSpV0vjx4/P93lCyGCFCmdKjRw+9+OKL+vbbb9WvX78r9tm9e7fatWununXrauzYsbJardq/f7/Wrl0rSapZs6bGjh2rkSNHqn///rrzzjslSc2bN7et46+//tJ9992nrl276tFHH1VQUNA163r11VdlsVg0bNgwpaamavLkyYqJidG2bdtsI1kFUZDaLmUYhtq3b6+VK1eqb9++ql+/vpYvX67nn39eR44cyXf44YcfftDnn3+up59+Wl5eXpo6dao6d+6sQ4cOyd/f/6p1nTt3TnfddZf279+vAQMGKDIyUp9++ql69+6tU6dO6dlnn1XNmjX10UcfafDgwapcubLtMFhAQMA19/nChQv6888/7drKlSuncuXKafr06apVq5bat28vFxcXff3113r66aeVk5Oj+Ph4W//Zs2frscceU61atTRixAj5+vrqp59+0rJly9StWze99NJLSktL0x9//GF7Tjw9Pa9Z15V8+umnOnv2rJ566in5+/tr48aNevvtt/XHH3/o008/LdS6unTponfeeUdLlizRQw89ZGs/e/asvv76a/Xu3VvOzs5KTU1V69atFRAQoOHDh8vX11cHDx7U559/XuBtdevWTaNHj9aqVat09913S/r7EOI999yjwMDA6z7+Rmt4+OGHFRkZqXHjxmnr1q3673//q8DAQL3xxhsKCAjQRx99pFdffVUZGRkaN26cpL/fC4V9fScmJmrBggUaMGCAKlasqIiICNtcvGeeeUbBwcEaM2aM1q9fr/fee0++vr768ccfdcstt+i1117T0qVL9eabb6p27drq2bOnbb1TpkxR+/bt1b17d2VlZemTTz7RQw89pMWLFysuLk6S9NFHH+nxxx/Xbbfdpv79+0v6e3TuakaPHq0xY8YoJiZGTz31lPbt26fp06dr06ZNWrt2rd1o8MmTJ9WmTRt16tRJDz/8sBYuXKhhw4apTp06uu+++wr0O0AxMIBSNGvWLEOSsWnTpqv28fHxMRo0aGC7P2rUKOPSl+qkSZMMScbx48evuo5NmzYZkoxZs2blW9ayZUtDkjFjxowrLmvZsqXt/sqVKw1JRqVKlYz09HRb+4IFCwxJxpQpU2xt4eHhRq9eva67zmvV1qtXLyM8PNx2/8svvzQkGa+88opdvwcffNCwWCzG/v37bW2SDDc3N7u27du3G5KMt99+O9+2LjV58mRDkvHxxx/b2rKysozo6GjD09PTbt/Dw8ONuLi4a67v0r6S8t1GjRplGIZhnD17Nt9jYmNjjSpVqtjunzp1yvDy8jKaNm1qnDt3zq5vTk6O7ee4uDi75y5P3msuKSnJrj3vd7ty5Upb25XqGTdunGGxWIzff//d1nb5a/JKcnJyjEqVKhmdO3e2a8977axZs8YwDMP44osvrvueuJqWLVsatWrVMgzDMBo3bmz07dvXMAzDOHnypOHm5mbMmTPHtp+ffvqp7XGXPycFreHS351h/N/z8Nhjj9n169ixo+Hv73/VWvMU9vXt5ORk7N69265v3r7ExsbavR6io6MNi8ViPPnkk7a2ixcvGpUrV7Z7PxpG/t97VlaWUbt2bePuu++2ay9fvvwV3+OXP5+pqamGm5ub0bp1ayM7O9vWb9q0aYYkY+bMmXbPiyTjww8/tLVlZmYawcHB+V47KFkcMkOZ4+npec2zzfLmIXz11VdFnoBstVrVp0+fAvfv2bOnvLy8bPcffPBBhYSEaOnSpUXafkEtXbpUzs7OGjhwoF370KFDZRiGvvnmG7v2mJgYu/9a69atK29vbx04cOC62wkODtYjjzxia3N1ddXAgQOVkZGh1atXF3kfmjZtqoSEBLtb3n/nl46upaWl6c8//1TLli114MABpaWlSfr78Ojp06c1fPjwfPNFivvU90vrOXPmjP788081b95chmHop59+KtS6LBaLHnroIS1dulQZGRm29vnz56tSpUq64447JP3f63nx4sW6cOFCkWvv1q2bPv/8c2VlZWnhwoVydnZWx44dC/TYG63hySeftLt/55136q+//lJ6evo1H1fY13fLli2vOn+tb9++dq+Hpk2byjAM9e3b19bm7Oysxo0b53s/XPp7P3nypNLS0nTnnXdq69at16z/ar777jtlZWVp0KBBcnL6vz+z/fr1k7e3t5YsWWLX39PT026enZubm2677bbrvm9RvAhEKHMyMjLswsflunTpottvv12PP/64goKC1LVrVy1YsKBQ4ahSpUqFmkB9+WnNFotF1apVK/Hrjvz+++8KDQ3N93zUrFnTtvxSt9xyS751VKhQQSdPnrzudqpXr2734X2t7RRGxYoVFRMTY3erUqWKJGnt2rWKiYlR+fLl5evrq4CAANt8rrxA9Ntvv0mSateuXeQaCurQoUPq3bu3/Pz85OnpqYCAALVs2dKunsLo0qWLzp07p0WLFkn6+7W9dOlSPfTQQ7Y/3i1btlTnzp01ZswYVaxYUQ888IBmzZqlzMzMQm2ra9euSktL0zfffKO5c+eqXbt213wfXepGa7j8dVehQgVJKtDrrjCv78jIyALX4OPjI0kKCwvL1355XYsXL1azZs3k7u4uPz8/BQQEaPr06UX6nV9ad40aNeza3dzcVKVKlXz7Vbly5XzhviDvWxQvAhHKlD/++ENpaWmqVq3aVft4eHhozZo1+u6779SjRw/t2LFDXbp00b333mubpHo9hZn3U1BXG60oaE3F4WpnuhiXTSItC3777Tfdc889+vPPPzVx4kQtWbJECQkJtuvTFNflBwr6e8nOzta9996rJUuWaNiwYfryyy+VkJBgm/BelHqaNWumiIgILViwQNLfE53PnTunLl262NW3cOFCrVu3TgMGDNCRI0f02GOPqVGjRnYjS9cTEhKiu+66SxMmTNCaNWvUrVu3Aj/2Rmsordfdtd63V6vhSu2X1vX999+rffv2cnd317vvvqulS5cqISFB3bp1K7X3zT/pfXszIxChTPnoo48kSbGxsdfs5+TkpHvuuUcTJ07Uzz//rFdffVWJiYlauXKlpOI/lPLrr7/a3TcMQ/v377c7I6xChQr5ziCR8v+XW5jawsPDdfTo0XyHEPfu3WtbXhzCw8P166+/5vujX9zbudTXX3+tzMxMLVq0SE888YTatm2rmJiYfH/08g4B7tq165rru9rzmjdacfnv5vLfy86dO/XLL79owoQJGjZsmB544AHFxMQoNDS0MLuVz8MPP6xly5YpPT1d8+fPV0REhJo1a5avX7NmzfTqq69q8+bNmjt3rnbv3q1PPvmkUNvq1q2bvv/+e3l7e6tt27aFrrU4aiiM0np9X8tnn30md3d3LV++XI899pjuu+8+xcTEXLFvQd+7eXXv27fPrj0rK0tJSUmlsl8oPAIRyozExET9+9//VmRkpLp3737VfidOnMjXlneBw7wh/vLly0vK/0ewqD788EO7D+2FCxfq2LFjdmeAVK1aVevXr1dWVpatbfHixflOzy9MbW3btlV2dramTZtm1z5p0iRZLJZiOwOlbdu2Sk5O1vz5821tFy9e1Ntvvy1PT0/bYaPilPdf8aX/BaelpWnWrFl2/Vq3bi0vLy+NGzcu3wUGL31s+fLlr3iIIy9QrVmzxtaWnZ2t995777r1GIahKVOmFGq/LtelSxdlZmZqzpw5WrZsWb7T4E+ePJlvJODy13NBPfjggxo1apTefffdQh0SLs4aCqO0Xt/X4uzsLIvFYjdiePDgwStekbp8+fIFet/GxMTIzc1NU6dOtXteP/jgA6WlpdnOXEPZwmn3cIhvvvlGe/fu1cWLF5WSkqLExEQlJCQoPDxcixYtuubF1saOHas1a9YoLi5O4eHhSk1N1bvvvqvKlSvbJqpWrVpVvr6+mjFjhry8vFS+fHk1bdr0mnMQrsXPz0933HGH+vTpo5SUFE2ePFnVqlWzuzTA448/roULF6pNmzZ6+OGH9dtvv+njjz/Od2puYWq7//771apVK7300ks6ePCg6tWrp2+//VZfffWVBg0adM3Tfgujf//++s9//qPevXtry5YtioiI0MKFC7V27VpNnjy5wHNRCqN169Zyc3PT/fffryeeeEIZGRl6//33FRgYqGPHjtn6eXt7a9KkSXr88cfVpEkTdevWTRUqVND27dt19uxZ27WSGjVqpPnz52vIkCFq0qSJPD09df/996tWrVpq1qyZRowYoRMnTsjPz0+ffPKJLl68aFfPrbfeqqpVq+q5557TkSNH5O3trc8+++yG53E0bNhQ1apV00svvaTMzEy7w2WSNGfOHL377rvq2LGjqlatqtOnT+v9998v0iiPj49Pkb5rrDhrKIzSen1fS1xcnCZOnKg2bdqoW7duSk1N1TvvvKNq1appx44ddn0bNWqk7777ThMnTlRoaKgiIyPVtGnTfOsMCAjQiBEjNGbMGLVp00bt27fXvn379O6776pJkyZcqLSsKvXz2mBqeaen5t3c3NyM4OBg49577zWmTJlid3p3nstPcV6xYoXxwAMPGKGhoYabm5sRGhpqPPLII8Yvv/xi97ivvvrKiIqKMlxcXOxOc7/S6b95rnba/f/+9z9jxIgRRmBgoOHh4WHExcXZnYadZ8KECUalSpUMq9Vq3H777cbmzZvzrfNatV1+2r1hGMbp06eNwYMHG6GhoYarq6tRvXp1480337Q7xdgw/j4tOT4+Pl9NV7scwOVSUlKMPn36GBUrVjTc3NyMOnXqXPHSAIU97f5afRctWmTUrVvXcHd3NyIiIow33njDmDlz5hVPk1+0aJHRvHlzw8PDw/D29jZuu+0243//+59teUZGhtGtWzfD19fXkGT3PP72229GTEyMYbVajaCgIOPFF180EhIS8p12//PPPxsxMTGGp6enUbFiRaNfv362Sxdc+lwU5LT7S7300kuGJKNatWr5lm3dutV45JFHjFtuucWwWq1GYGCg0a5dO2Pz5s3XXe+1Xst5CnLafUFr0FVOu7/8EhhXutTB1Wq90df31S7lcbXaevXqZZQvX96u7YMPPjCqV69uWK1W49ZbbzVmzZp1xd/x3r17jRYtWhgeHh6GJNv76mqXdpg2bZpx6623Gq6urkZQUJDx1FNPGSdPnrTrc7Xn5UqfBShZFsNg1hYAADA35hABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADT48KMBZCTk6OjR4/Ky8ur2L8SAgAAlAzDMHT69GmFhobm+/LqyxGICuDo0aP5vjEZAAD8Mxw+fFiVK1e+Zh8CUQHkfW3B4cOH5e3t7eBqAABAQaSnpyssLKxAXz9EICqAvMNk3t7eBCIAAP5hCjLdhUnVAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9FwcXQDKtojhSxxdAkrRwdfjHF0CADgEI0QAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0HBqIsrOz9fLLLysyMlIeHh6qWrWq/v3vf8swDFsfwzA0cuRIhYSEyMPDQzExMfr111/t1nPixAl1795d3t7e8vX1Vd++fZWRkWHXZ8eOHbrzzjvl7u6usLAwjR8/vlT2EQAAlH0ODURvvPGGpk+frmnTpmnPnj164403NH78eL399tu2PuPHj9fUqVM1Y8YMbdiwQeXLl1dsbKzOnz9v69O9e3ft3r1bCQkJWrx4sdasWaP+/fvblqenp6t169YKDw/Xli1b9Oabb2r06NF67733SnV/AQBA2WQxLh2OKWXt2rVTUFCQPvjgA1tb586d5eHhoY8//liGYSg0NFRDhw7Vc889J0lKS0tTUFCQZs+era5du2rPnj2KiorSpk2b1LhxY0nSsmXL1LZtW/3xxx8KDQ3V9OnT9dJLLyk5OVlubm6SpOHDh+vLL7/U3r17r1tnenq6fHx8lJaWJm9v7xJ4JsquiOFLHF0CStHB1+McXQIAFJvC/P126AhR8+bNtWLFCv3yyy+SpO3bt+uHH37QfffdJ0lKSkpScnKyYmJibI/x8fFR06ZNtW7dOknSunXr5OvrawtDkhQTEyMnJydt2LDB1qdFixa2MCRJsbGx2rdvn06ePJmvrszMTKWnp9vdAADAzcvFkRsfPny40tPTdeutt8rZ2VnZ2dl69dVX1b17d0lScnKyJCkoKMjucUFBQbZlycnJCgwMtFvu4uIiPz8/uz6RkZH51pG3rEKFCnbLxo0bpzFjxhTTXgIAgLLOoSNECxYs0Ny5czVv3jxt3bpVc+bM0VtvvaU5c+Y4siyNGDFCaWlpttvhw4cdWg8AAChZDh0hev755zV8+HB17dpVklSnTh39/vvvGjdunHr16qXg4GBJUkpKikJCQmyPS0lJUf369SVJwcHBSk1NtVvvxYsXdeLECdvjg4ODlZKSYtcn735en0tZrVZZrdbi2UkAAFDmOXSE6OzZs3Jysi/B2dlZOTk5kqTIyEgFBwdrxYoVtuXp6enasGGDoqOjJUnR0dE6deqUtmzZYuuTmJionJwcNW3a1NZnzZo1unDhgq1PQkKCatSoke9wGQAAMB+HBqL7779fr776qpYsWaKDBw/qiy++0MSJE9WxY0dJksVi0aBBg/TKK69o0aJF2rlzp3r27KnQ0FB16NBBklSzZk21adNG/fr108aNG7V27VoNGDBAXbt2VWhoqCSpW7ducnNzU9++fbV7927Nnz9fU6ZM0ZAhQxy16wAAoAxx6CGzt99+Wy+//LKefvpppaamKjQ0VE888YRGjhxp6/PCCy/ozJkz6t+/v06dOqU77rhDy5Ytk7u7u63P3LlzNWDAAN1zzz1ycnJS586dNXXqVNtyHx8fffvtt4qPj1ejRo1UsWJFjRw50u5aRQAAwLwceh2ifwquQwSz4DpEAG4m/5jrEAEAAJQFBCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6BCIAAGB6Dg9ER44c0aOPPip/f395eHioTp062rx5s225YRgaOXKkQkJC5OHhoZiYGP3666926zhx4oS6d+8ub29v+fr6qm/fvsrIyLDrs2PHDt15551yd3dXWFiYxo8fXyr7BwAAyj6HBqKTJ0/q9ttvl6urq7755hv9/PPPmjBhgipUqGDrM378eE2dOlUzZszQhg0bVL58ecXGxur8+fO2Pt27d9fu3buVkJCgxYsXa82aNerfv79teXp6ulq3bq3w8HBt2bJFb775pkaPHq333nuvVPcXAACUTRbDMAxHbXz48OFau3atvv/++ysuNwxDoaGhGjp0qJ577jlJUlpamoKCgjR79mx17dpVe/bsUVRUlDZt2qTGjRtLkpYtW6a2bdvqjz/+UGhoqKZPn66XXnpJycnJcnNzs237yy+/1N69e69bZ3p6unx8fJSWliZvb+9i2vt/hojhSxxdAkrRwdfjHF0CABSbwvz9dugI0aJFi9S4cWM99NBDCgwMVIMGDfT+++/bliclJSk5OVkxMTG2Nh8fHzVt2lTr1q2TJK1bt06+vr62MCRJMTExcnJy0oYNG2x9WrRoYQtDkhQbG6t9+/bp5MmT+erKzMxUenq63Q0AANy8HBqIDhw4oOnTp6t69epavny5nnrqKQ0cOFBz5syRJCUnJ0uSgoKC7B4XFBRkW5acnKzAwEC75S4uLvLz87Prc6V1XLqNS40bN04+Pj62W1hYWDHsLQAAKKscGohycnLUsGFDvfbaa2rQoIH69++vfv36acaMGY4sSyNGjFBaWprtdvjwYYfWAwAASpZDA1FISIiioqLs2mrWrKlDhw5JkoKDgyVJKSkpdn1SUlJsy4KDg5Wammq3/OLFizpx4oRdnyut49JtXMpqtcrb29vuBgAAbl4ODUS333679u3bZ9f2yy+/KDw8XJIUGRmp4OBgrVixwrY8PT1dGzZsUHR0tCQpOjpap06d0pYtW2x9EhMTlZOTo6ZNm9r6rFmzRhcuXLD1SUhIUI0aNezOaAMAAObk0EA0ePBgrV+/Xq+99pr279+vefPm6b333lN8fLwkyWKxaNCgQXrllVe0aNEi7dy5Uz179lRoaKg6dOgg6e8RpTZt2qhfv37auHGj1q5dqwEDBqhr164KDQ2VJHXr1k1ubm7q27evdu/erfnz52vKlCkaMmSIo3YdAACUIS6O3HiTJk30xRdfaMSIERo7dqwiIyM1efJkde/e3dbnhRde0JkzZ9S/f3+dOnVKd9xxh5YtWyZ3d3dbn7lz52rAgAG655575OTkpM6dO2vq1Km25T4+Pvr2228VHx+vRo0aqWLFiho5cqTdtYoAAIB5OfQ6RP8UXIcIZsF1iADcTP4x1yECAAAoCwhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9AhEAADA9IoUiKpUqaK//vorX/upU6dUpUqVGy4KAACgNBUpEB08eFDZ2dn52jMzM3XkyJEbLgoAAKA0uRSm86JFi2w/L1++XD4+Prb72dnZWrFihSIiIoqtOAAAgNJQqEDUoUMHSZLFYlGvXr3slrm6uioiIkITJkwotuIAAABKQ6ECUU5OjiQpMjJSmzZtUsWKFUukKAAAgNJUqECUJykpqbjrAAAAcJgiBSJJWrFihVasWKHU1FTbyFGemTNn3nBhAAAApaVIgWjMmDEaO3asGjdurJCQEFksluKuCwAAoNQUKRDNmDFDs2fPVo8ePYq7HgAAgFJXpOsQZWVlqXnz5sVdCwAAgEMUKRA9/vjjmjdvXnHXAgAA4BBFOmR2/vx5vffee/ruu+9Ut25dubq62i2fOHFisRQHAABQGooUiHbs2KH69etLknbt2mW3jAnWAADgn6ZIgWjlypXFXQcAAIDDFGkOEQAAwM2kSCNErVq1uuahscTExCIXBAAAUNqKFIjy5g/luXDhgrZt26Zdu3bl+9JXAACAsq5IgWjSpElXbB89erQyMjJuqCAAAIDSVqxziB599FG+xwwAAPzjFGsgWrdundzd3YtzlQAAACWuSIfMOnXqZHffMAwdO3ZMmzdv1ssvv1wshQEAAJSWIgUiHx8fu/tOTk6qUaOGxo4dq9atWxdLYQAAAKWlSIFo1qxZxV0HAACAwxQpEOXZsmWL9uzZI0mqVauWGjRoUCxFAQAAlKYiBaLU1FR17dpVq1atkq+vryTp1KlTatWqlT755BMFBAQUZ40AAAAlqkhnmT3zzDM6ffq0du/erRMnTujEiRPatWuX0tPTNXDgwOKuEQAAoEQVaYRo2bJl+u6771SzZk1bW1RUlN555x0mVQMAgH+cIo0Q5eTkyNXVNV+7q6urcnJybrgoAACA0lSkQHT33Xfr2Wef1dGjR21tR44c0eDBg3XPPfcUW3EAAACloUiBaNq0aUpPT1dERISqVq2qqlWrKjIyUunp6Xr77beLu0YAAIASVaQ5RGFhYdq6dau+++477d27V5JUs2ZNxcTEFGtxAAAApaFQI0SJiYmKiopSenq6LBaL7r33Xj3zzDN65pln1KRJE9WqVUvff/99SdUKAABQIgoViCZPnqx+/frJ29s73zIfHx898cQTmjhxYrEVBwAAUBoKFYi2b9+uNm3aXHV569attWXLlhsuCgAAoDQVKhClpKRc8XT7PC4uLjp+/PgNFwUAAFCaChWIKlWqpF27dl11+Y4dOxQSElKkQl5//XVZLBYNGjTI1nb+/HnFx8fL399fnp6e6ty5s1JSUuwed+jQIcXFxalcuXIKDAzU888/r4sXL9r1WbVqlRo2bCir1apq1app9uzZRaoRAADcnAoViNq2bauXX35Z58+fz7fs3LlzGjVqlNq1a1foIjZt2qT//Oc/qlu3rl374MGD9fXXX+vTTz/V6tWrdfToUXXq1Mm2PDs7W3FxccrKytKPP/6oOXPmaPbs2Ro5cqStT1JSkuLi4tSqVStt27ZNgwYN0uOPP67ly5cXuk4AAHBzshiGYRS0c0pKiho2bChnZ2cNGDBANWrUkCTt3btX77zzjrKzs7V161YFBQUVuICMjAw1bNhQ7777rl555RXVr19fkydPVlpamgICAjRv3jw9+OCDtu3UrFlT69atU7NmzfTNN9+oXbt2Onr0qG2bM2bM0LBhw3T8+HG5ublp2LBhWrJkid3IVteuXXXq1CktW7asQDWmp6fLx8dHaWlpV5xQfjOLGL7E0SWgFB18Pc7RJQBAsSnM3+9CjRAFBQXpxx9/VO3atTVixAh17NhRHTt21IsvvqjatWvrhx9+KFQYkqT4+HjFxcXlu4bRli1bdOHCBbv2W2+9VbfccovWrVsnSVq3bp3q1Kljt83Y2Filp6dr9+7dtj6Xrzs2Nta2DgAAgEJfmDE8PFxLly7VyZMntX//fhmGoerVq6tChQqF3vgnn3yirVu3atOmTfmWJScny83NTb6+vnbtQUFBSk5OtvW5PIDl3b9en/T0dJ07d04eHh75tp2ZmanMzEzb/fT09ELvGwAA+Oco0pWqJalChQpq0qRJkTd8+PBhPfvss0pISJC7u3uR11MSxo0bpzFjxji6DAAAUEqK9F1mxWHLli1KTU1Vw4YN5eLiIhcXF61evVpTp06Vi4uLgoKClJWVpVOnTtk9LiUlRcHBwZKk4ODgfGed5d2/Xh9vb+8rjg5J0ogRI5SWlma7HT58uDh2GQAAlFEOC0T33HOPdu7cqW3bttlujRs3Vvfu3W0/u7q6asWKFbbH7Nu3T4cOHVJ0dLQkKTo6Wjt37lRqaqqtT0JCgry9vRUVFWXrc+k68vrkreNKrFarvL297W4AAODmVeRDZjfKy8tLtWvXtmsrX768/P39be19+/bVkCFD5OfnJ29vbz3zzDOKjo5Ws2bNJP19ZeyoqCj16NFD48ePV3Jysv71r38pPj5eVqtVkvTkk09q2rRpeuGFF/TYY48pMTFRCxYs0JIlnD0FAAD+5rBAVBCTJk2Sk5OTOnfurMzMTMXGxurdd9+1LXd2dtbixYv11FNPKTo6WuXLl1evXr00duxYW5/IyEgtWbJEgwcP1pQpU1S5cmX997//VWxsrCN2CQAAlEGFug6RWXEdIpgF1yECcDMpsesQAQAA3IwIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQcGojGjRunJk2ayMvLS4GBgerQoYP27dtn1+f8+fOKj4+Xv7+/PD091blzZ6WkpNj1OXTokOLi4lSuXDkFBgbq+eef18WLF+36rFq1Sg0bNpTValW1atU0e/bskt49AADwD+HQQLR69WrFx8dr/fr1SkhI0IULF9S6dWudOXPG1mfw4MH6+uuv9emnn2r16tU6evSoOnXqZFuenZ2tuLg4ZWVl6ccff9ScOXM0e/ZsjRw50tYnKSlJcXFxatWqlbZt26ZBgwbp8ccf1/Lly0t1fwEAQNlkMQzDcHQReY4fP67AwECtXr1aLVq0UFpamgICAjRv3jw9+OCDkqS9e/eqZs2aWrdunZo1a6ZvvvlG7dq109GjRxUUFCRJmjFjhoYNG6bjx4/Lzc1Nw4YN05IlS7Rr1y7btrp27apTp05p2bJl160rPT1dPj4+SktLk7e3d8nsfBkVMXyJo0tAKTr4epyjSwCAYlOYv99lag5RWlqaJMnPz0+StGXLFl24cEExMTG2PrfeeqtuueUWrVu3TpK0bt061alTxxaGJCk2Nlbp6enavXu3rc+l68jrk7eOy2VmZio9Pd3uBgAAbl5lJhDl5ORo0KBBuv3221W7dm1JUnJystzc3OTr62vXNygoSMnJybY+l4ahvOV5y67VJz09XefOnctXy7hx4+Tj42O7hYWFFcs+AgCAsqnMBKL4+Hjt2rVLn3zyiaNL0YgRI5SWlma7HT582NElAQCAEuTi6AIkacCAAVq8eLHWrFmjypUr29qDg4OVlZWlU6dO2Y0SpaSkKDg42NZn48aNduvLOwvt0j6Xn5mWkpIib29veXh45KvHarXKarUWy74BAICyz6EjRIZhaMCAAfriiy+UmJioyMhIu+WNGjWSq6urVqxYYWvbt2+fDh06pOjoaElSdHS0du7cqdTUVFufhIQEeXt7Kyoqytbn0nXk9clbBwAAMDeHjhDFx8dr3rx5+uqrr+Tl5WWb8+Pj4yMPDw/5+Piob9++GjJkiPz8/OTt7a1nnnlG0dHRatasmSSpdevWioqKUo8ePTR+/HglJyfrX//6l+Lj422jPE8++aSmTZumF154QY899pgSExO1YMECLVnCGVQAAMDBI0TTp09XWlqa7rrrLoWEhNhu8+fPt/WZNGmS2rVrp86dO6tFixYKDg7W559/blvu7OysxYsXy9nZWdHR0Xr00UfVs2dPjR071tYnMjJSS5YsUUJCgurVq6cJEybov//9r2JjY0t1fwEAQNlUpq5DVFZxHSKYBdchAnAz+cdehwgAAMARCEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0XBxdAADAMSKGL3F0CShFB1+Pc3QJZRojRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPRMFYjeeecdRUREyN3dXU2bNtXGjRsdXRIAACgDTBOI5s+fryFDhmjUqFHaunWr6tWrp9jYWKWmpjq6NAAA4GCmCUQTJ05Uv3791KdPH0VFRWnGjBkqV66cZs6c6ejSAACAg5kiEGVlZWnLli2KiYmxtTk5OSkmJkbr1q1zYGUAAKAscHF0AaXhzz//VHZ2toKCguzag4KCtHfv3nz9MzMzlZmZabuflpYmSUpPTy/ZQsugnMyzji4BpciMr3Ez4/1tLmZ8f+fts2EY1+1rikBUWOPGjdOYMWPytYeFhTmgGqD0+Ex2dAUASoqZ39+nT5+Wj4/PNfuYIhBVrFhRzs7OSklJsWtPSUlRcHBwvv4jRozQkCFDbPdzcnJ04sQJ+fv7y2KxlHi9cKz09HSFhYXp8OHD8vb2dnQ5AIoR729zMQxDp0+fVmho6HX7miIQubm5qVGjRlqxYoU6dOgg6e+Qs2LFCg0YMCBff6vVKqvVatfm6+tbCpWiLPH29uYDE7hJ8f42j+uNDOUxRSCSpCFDhqhXr15q3LixbrvtNk2ePFlnzpxRnz59HF0aAABwMNMEoi5duuj48eMaOXKkkpOTVb9+fS1btizfRGsAAGA+pglEkjRgwIArHiIDLmW1WjVq1Kh8h00B/PPx/sbVWIyCnIsGAABwEzPFhRkBAACuhUAEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMz1TXIQIAmEOnTp0K3Pfzzz8vwUrwT0EgAnJlZ2dr9uzZWrFihVJTU5WTk2O3PDEx0UGVASisgn5/FZCHCzMCuQYMGKDZs2crLi5OISEhslgsdssnTZrkoMoAACWNQATkqlixoj788EO1bdvW0aUAAEoZh8yAXG5ubqpWrZqjywBQAhYuXKgFCxbo0KFDysrKslu2detWB1WFsoSzzIBcQ4cO1ZQpU8SgKXBzmTp1qvr06aOgoCD99NNPuu222+Tv768DBw7ovvvuc3R5KCM4ZAbk6tixo1auXCk/Pz/VqlVLrq6udss5EwX4Z7r11ls1atQoPfLII/Ly8tL27dtVpUoVjRw5UidOnNC0adMcXSLKAA6ZAbl8fX3VsWNHR5cBoJgdOnRIzZs3lyR5eHjo9OnTkqQePXqoWbNmBCJIIhABNrNmzXJ0CQBKQHBwsE6cOKHw8HDdcsstWr9+verVq6ekpCQOkcOGQARc5vjx49q3b58kqUaNGgoICHBwRQBuxN13361FixapQYMG6tOnjwYPHqyFCxdq8+bNhbqAI25uzCECcp05c0bPPPOMPvzwQ9tFGZ2dndWzZ0+9/fbbKleunIMrBFAUOTk5ysnJkYvL32MAn3zyiX788UdVr15dTzzxhNzc3BxcIcoCAhGQ64knntB3332nadOm6fbbb5ck/fDDDxo4cKDuvfdeTZ8+3cEVAgBKCoEIyFWxYkUtXLhQd911l137ypUr9fDDD+v48eOOKQzADVmzZs01l7do0aKUKkFZxhwiINfZs2cVFBSUrz0wMFBnz551QEUAisPl/+RIsvtqnuzs7FKsBmUVF2YEckVHR2vUqFE6f/68re3cuXMaM2aMoqOjHVgZgBtx8uRJu1tqaqqWLVumJk2a6Ntvv3V0eSgjOGQG5Nq1a5diY2OVmZmpevXqSZK2b98ud3d3LV++XLVq1XJwhQCK0+rVqzVkyBBt2bLF0aWgDCAQAZc4e/as5s6dq71790qSatasqe7du8vDw8PBlQEobnv37lXjxo2VkZHh6FJQBhCIAAA3tR07dtjdNwxDx44d0+uvv66LFy/qhx9+cFBlKEsIRDC1RYsW6b777pOrq6sWLVp0zb7t27cvpaoAFCcnJydZLJZ8V6Vu1qyZZs6cqVtvvdVBlaEsIRDB1JycnJScnKzAwEA5OV39HAOLxcKZKMA/1O+//25338nJSQEBAXJ3d3dQRSiLOMsMppaTk6PAwEDbz1e7EYaAf67Vq1crODhY4eHhCg8PV1hYmNzd3ZWVlaUPP/zQ0eWhjCAQAbk+/PBDZWZm5mvnQxP4Z+vTp4/S0tLytZ8+fVp9+vRxQEUoiwhEQC4+NIGbk2EYdhdizPPHH3/Ix8fHARWhLOJK1UAuPjSBm0uDBg1ksVhksVh0zz332L7cVfr76tRJSUlq06aNAytEWUIggunxoQncnDp06CBJ2rZtm2JjY+Xp6Wlb5ubmpoiICHXu3NlB1aGsIRDB9PjQBG5Oo0aNkiRFRESoa9euslqtDq4IZRmn3QO55syZoy5dunAqLnCT2bRpk3JyctS0aVO79g0bNsjZ2VmNGzd2UGUoS5hUDeTq1asXYQi4CcXHx+vw4cP52o8cOaL4+HgHVISyiENmQK7s7GxNmjRJCxYs0KFDh5SVlWW3/MSJEw6qDMCN+Pnnn9WwYcN87Q0aNNDPP//sgIpQFjFCBOQaM2aMJk6cqC5duigtLU1DhgxRp06d5OTkpNGjRzu6PABFZLValZKSkq/92LFjdidRwNyYQwTkqlq1qqZOnaq4uDh5eXlp27Zttrb169dr3rx5ji4RQBE88sgjOnbsmL766ivbJTROnTqlDh06KDAwUAsWLHBwhSgLCERArvLly2vPnj265ZZbFBISoiVLlqhhw4Y6cOCAGjRocMWLNgIo+44cOaIWLVror7/+UoMGDST9fVZpUFCQEhISFBYW5uAKURZwyAzIVblyZR07dkzS36NF3377raS/z1DhdF3gn6tSpUrasWOHxo8fr6ioKDVq1EhTpkzRzp07CUOwYYQIyDV8+HB5e3vrxRdf1Pz58/Xoo48qIiJChw4d0uDBg/X66687ukQAQAkhEAFXsW7dOq1bt07Vq1fX/fff7+hyANygn3/++YpnkLZv395BFaEsIRABAG5qBw4cUMeOHbVz505ZLBbl/dnL++7C7OxsR5aHMoLzDYFLHD16VD/88INSU1OVk5Njt2zgwIEOqgrAjXj22WcVGRmpFStWKDIyUhs3btRff/2loUOH6q233nJ0eSgjGCECcs2ePVtPPPGE3Nzc5O/vb/vvUfr7P8kDBw44sDoARVWxYkUlJiaqbt268vHx0caNG1WjRg0lJiZq6NCh+umnnxxdIsoARoiAXC+//LJGjhypESNGyMmJEzCBm0V2dra8vLwk/R2Ojh49qho1aig8PFz79u1zcHUoKwhEQK6zZ8+qa9euhCHgJlO7dm1t375dkZGRatq0qcaPHy83Nze99957qlKliqPLQxnBJz+Qq2/fvvr0008dXQaAYrBjxw7bPMB//etftonUY8eOVVJSku68804tXbpUU6dOdWSZKEOYQwTkys7OVrt27XTu3DnVqVNHrq6udssnTpzooMoAFJazs7OOHTumwMBAValSRZs2bZK/v79t+YkTJ1ShQgW7uYIwNw6ZAbnGjRun5cuXq0aNGpKUb1I1gH8OX19fJSUlKTAwUAcPHsx31qifn5+DKkNZxQgRkKtChQqaNGmSevfu7ehSANyg/v3768MPP1RISIgOHTqkypUry9nZ+Yp9OYMUEiNEgI3VatXtt9/u6DIAFIP33ntPnTp10v79+zVw4ED169fPdqYZcCWMEAG5xo0bp2PHjjHJErjJ9OnTR1OnTiUQ4ZoIRECujh07KjExUf7+/qpVq1a+SdWff/65gyoDAJQ0DpkBuXx9fdWpUydHlwEAcAACESDp4sWLatWqlVq3bq3g4GBHlwMAKGUcMgNylStXTnv27FF4eLijSwEAlDKuVA3kuu222/iSRwAwKQ6ZAbmefvppDR06VH/88YcaNWqk8uXL2y2vW7eugyoDAJQ0DpkBua70pa4Wi0WGYchisSg7O9sBVQEASgMjRECupKQkR5cAAHAQRogAAIDpMUIEXOK3337T5MmTtWfPHklSVFSUnn32WVWtWtXBlQEAShJnmQG5li9frqioKG3cuFF169ZV3bp1tWHDBtWqVUsJCQmOLg8AUII4ZAbkatCggWJjY/X666/btQ8fPlzffvuttm7d6qDKAAAljUAE5HJ3d9fOnTtVvXp1u/ZffvlFdevW1fnz5x1UGQCgpHHIDMgVEBCgbdu25Wvftm2bAgMDS78gAECpYVI1kKtfv37q37+/Dhw4oObNm0uS1q5dqzfeeENDhgxxcHUAgJLEITMgl2EYmjx5siZMmKCjR49KkkJDQ/X8889r4MCBslgsDq4QAFBSCEQwtUWLFum+++6Tq6urXfvp06clSV5eXo4oCwBQyghEMDVnZ2clJycrICBAzs7OOnbsGPOFAMCEmFQNUwsICND69eslyfadZQAA82FSNUztySef1AMPPCCLxSKLxaLg4OCr9uXLXQHg5sUhM5je3r17tX//frVv316zZs2Sr6/vFfs98MADpVsYAKDUEIiAXGPGjNHzzz+vcuXKOboUAEApIxABAADTY1I1kCslJUU9evRQaGioXFxc5OzsbHcDANy8mFQN5Ordu7cOHTqkl19+WSEhIZxxBgAmwiEzIJeXl5e+//571a9f39GlAABKGYfMgFxhYWHi/wMAMCcCEZBr8uTJGj58uA4ePOjoUgAApYxDZkCuChUq6OzZs7p48aLKlSuX7/vNTpw44aDKAAAljUnVQK7Jkyc7ugQAgIMwQgQAAEyPESKYWnp6ury9vW0/X0tePwDAzYcRIpias7Ozjh07psDAQDk5OV3x2kOGYchisfDlrgBwE2OECKaWmJgoPz8/SdLKlSsdXA0AwFEYIQIAAKbHdYiAXMuWLdMPP/xgu//OO++ofv366tatm06ePOnAygAAJY1ABOR6/vnnbROrd+7cqSFDhqht27ZKSkrSkCFDHFwdAKAkMYcIyJWUlKSoqChJ0meffab7779fr732mrZu3aq2bds6uDoAQElihAjI5ebmprNnz0qSvvvuO7Vu3VqS5Ofnd91T8gEA/2yMEAG57rjjDg0ZMkS33367Nm7cqPnz50uSfvnlF1WuXNnB1QEAShIjRECuadOmycXFRQsXLtT06dNVqVIlSdI333yjNm3aOLg6AEBJ4rR7AABgehwyg6nx1R0AAIkRIpgcX90BAJAYIYLJ8dUdAACJESLAzvnz57Vjxw6lpqYqJyfHbln79u0dVBUAoKQxQgTkWrZsmXr27Kk///wz3zIOmQHAzY3T7oFczzzzjB566CEdO3ZMOTk5djfCEADc3DhkBuTy9vbWTz/9pKpVqzq6FABAKWOECMj14IMPatWqVY4uAwDgAIwQAbnOnj2rhx56SAEBAapTp45cXV3tlg8cONBBlQEAShqBCMj1wQcf6Mknn5S7u7v8/f3trklksVh04MABB1YHAChJBCIgV3BwsAYOHKjhw4fLyYmjyQBgJnzqA7mysrLUpUsXwhAAmBCf/ECuXr16af78+Y4uAwDgAFyYEciVnZ2t8ePHa/ny5apbt26+SdUTJ050UGUAgJLGHCIgV6tWra66zGKxKDExsRSrAQCUJgIRAAAwPeYQAQAA0yMQAQAA0yMQAQAA0yMQASjTevfurQ4dOtju33XXXRo0aFCp17Fq1SpZLBadOnWqxLZx+b4WRWnUCdyMCEQACq13796yWCyyWCxyc3NTtWrVNHbsWF28eLHEt/3555/r3//+d4H6lnY4iIiI0OTJk0tlWwCKF9chAlAkbdq00axZs5SZmamlS5cqPj5erq6uGjFiRL6+WVlZcnNzK5bt+vn5Fct6AOBSjBABKBKr1arg4GCFh4frqaeeUkxMjBYtWiTp/w79vPrqqwoNDVWNGjUkSYcPH9bDDz8sX19f+fn56YEHHtDBgwdt68zOztaQIUPk6+srf39/vfDCC7r8yiCXHzLLzMzUsGHDFBYWJqvVqmrVqumDDz7QwYMHbdeWqlChgiwWi3r37i1JysnJ0bhx4xQZGSkPDw/Vq1dPCxcutNvO0qVL9f/+3/+Th4eHWrVqZVdnUWRnZ6tv3762bdaoUUNTpky5Yt8xY8YoICBA3t7eevLJJ5WVlWVbVpDaARQeI0QAioWHh4f++usv2/0VK1bI29tbCQkJkqQLFy4oNjZW0dHR+v777+Xi4qJXXnlFbdq00Y4dO+Tm5qYJEyZo9uzZmjlzpmrWrKkJEyboiy++0N13333V7fbs2VPr1q3T1KlTVa9ePSUlJenPP/9UWFiYPvvsM3Xu3Fn79u2Tt7e3PDw8JEnjxo3Txx9/rBkzZqh69epas2aNHn30UQUEBKhly5Y6fPiwOnXqpPj4ePXv31+bN2/W0KFDb+j5ycnJUeXKlfXpp5/K399fP/74o/r376+QkBA9/PDDds+bu7u7Vq1apYMHD6pPnz7y9/fXq6++WqDaARSRAQCF1KtXL+OBBx4wDMMwcnJyjISEBMNqtRrPPfecbXlQUJCRmZlpe8xHH31k1KhRw8jJybG1ZWZmGh4eHsby5csNwzCMkJAQY/z48bblFy5cMCpXrmzblmEYRsuWLY1nn33WMAzD2LdvnyHJSEhIuGKdK1euNCQZJ0+etLWdP3/eKFeunPHjjz/a9e3bt6/xyCOPGIZhGCNGjDCioqLslg8bNizfui4XHh5uTJo06arLLxcfH2907tzZdr9Xr16Gn5+fcebMGVvb9OnTDU9PTyM7O7tAtV9pnwFcHyNEAIpk8eLF8vT01IULF5STk6Nu3bpp9OjRtuV16tSxmze0fft27d+/X15eXnbrOX/+vH777TelpaXp2LFjatq0qW2Zi4uLGjdunO+wWZ5t27bJ2dm5UCMj+/fv19mzZ3XvvffatWdlZalBgwaSpD179tjVIUnR0dEF3sbVvPPOO5o5c6YOHTqkc+fOKSsrS/Xr17frU69ePZUrV85uuxkZGTp8+LAyMjKuWzuAoiEQASiSVq1aafr06XJzc1NoaKhcXOw/TsqXL293PyMjQ40aNdLcuXPzrSsgIKBINeQdAiuMjIwMSdKSJUtUqVIlu2VWq7VIdRTEJ598oueee04TJkxQdHS0vLy89Oabb2rDhg0FXoejagfMgEAEoEjKly+vatWqFbh/w4YNNX/+fAUGBsrb2/uKfUJCQrRhwwa1aNFCknTx4kVt2bJFDRs2vGL/OnXqKCcnR6tXr1ZMTEy+5XkjVNnZ2ba2qKgoWa1WHTp06KojSzVr1rRNEM+zfv366+/kNaxdu1bNmzfX008/bWv77bff8vXbvn27zp07Zwt769evl6enp8LCwuTn53fd2gEUDWeZASgV3bt3V8WKFfXAAw/o+++/V1JSklatWqWBAwfqjz/+kCQ9++yzev311/Xll19q7969evrpp695DaGIiAj16tVLjz32mL788kvbOhcsWCBJCg8Pl8Vi0eLFi3X8+HFlZGTIy8tLzz33nAYPHqw5c+bot99+09atW/X2229rzpw5kqQnn3xSv/76q55//nnt27dP8+bN0+zZswu0n0eOHNG2bdvsbidPnlT16tW1efNmLV++XL/88otefvllbdq0Kd/js7Ky1LdvX/38889aunSpRo0apQEDBsjJyalAtQMoIkdPYgLwz3PppOrCLD927JjRs2dPo2LFiobVajWqVKli9OvXz0hLSzMM4+9J1M8++6zh7e1t+Pr6GkOGDDF69ux51UnVhmEY586dMwYPHmyEhIQYbm5uRrVq1YyZM2falo8dO9YIDg42LBaL0atXL8Mw/p4IPnnyZKNGjRqGq6urERAQYMTGxhqrV6+2Pe7rr782qlWrZlitVuPOO+80Zs6cWaBJ1ZLy3T766CPj/PnzRu/evQ0fHx/D19fXeOqpp4zhw4cb9erVy/e8jRw50vD39zc8PT2Nfv36GefPn7f1uV7tTKoGisZiGFeZrQgAAGASHDIDAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACmRyACAACm9/8BZ4WaA8eFRqIAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count occurrences of each label\n",
        "label_counts = df_climate_inference['predicted_label'].value_counts()\n",
        "\n",
        "print(\"Distribution of Factual vs Misinformation:\")\n",
        "print(label_counts)\n",
        "\n",
        "# If you want to visualise this distribution:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Distribution of Factual vs Misinformation')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "learly overly categorising things as misinfo... I could retune to reduce more false positives, but given that it got 96+% on F1 score, the model is accurate to the dataset; the issue lies with either a) the internal validity of the dataset; b) the external valifity / OOD assumptions when applied to climate tweets.\n",
        "\n",
        "a) looking at the constituent datasets, I can see how this happened...\n",
        "b) climate tweets are different...inference write up: not great, overrepresentation of misinfo - prob because in the training data i used climate was disproportionately in the misinfo category -> out of distribution... generally it identifies factual well, but overly classifies as misinfo\n",
        "\n",
        "was going to do an LDA, but unless the model is better performing, it makes more sense to do xAI to work out where it's performing badly\n",
        "\n",
        "looking at the tweets labeled misinfo, it's clear the model doesn't understand the substantive poits / the science, it's just classifying on style\n",
        "\n",
        "so next steps\n",
        "1) clear up the tweets to remove the other text -> get it closer to the others text\n",
        "2) xAI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Examples of Factual (predicted_label 0):\n",
            "                                                  text predicted_label\n",
            "3    North America has experienced an average winte...         factual\n",
            "16   I saw my Amish friend Elton today. \"Nice day.\"...         factual\n",
            "87   The concept of living through a \"discontinuity...         factual\n",
            "89   Border security is national security, and it h...         factual\n",
            "134  These investments are critical to building a s...         factual\n",
            "139  Australian high temperature records.\\nSpot the...         factual\n",
            "167  In late 2019, I issued an Exec. Order on clima...         factual\n",
            "168  Labor leader Anthony Albanese has gained an ed...         factual\n",
            "169  .\\n@POTUS\\n leads with honesty and integrity, ...         factual\n",
            "202  The Bank of Japan has made the first auction i...         factual\n",
            "\n",
            "10 Examples of Misinformation (predicted_label 1):\n",
            "                                                 text predicted_label\n",
            "0   The only solution I’ve ever heard the Left pro...  misinformation\n",
            "1   Climate change doesn’t cause volcanic eruption...  misinformation\n",
            "2   Vaccinated tennis ball boy collapses in the te...  misinformation\n",
            "4   They're gonna do the same with Climate Change ...  misinformation\n",
            "5   HELLO AMERICA,\\n\\nWho would have ever thought ...  misinformation\n",
            "6   fucking hell this weather makes me really fuck...  misinformation\n",
            "7   Great to finally have this important UNESCO/SC...  misinformation\n",
            "8   Climate change is one of the world's most pres...  misinformation\n",
            "9   Can people start questioning the \"Johnson got ...  misinformation\n",
            "10  I’m raising two kids and, ya know, I guess I j...  misinformation\n"
          ]
        }
      ],
      "source": [
        "# Extract 10 examples of factual (label == 0)\n",
        "factual_examples = df_climate_inference[df_climate_inference['predicted_label'] == 'factual'].head(10)\n",
        "\n",
        "# Extract 10 examples of misinformation (label == 1)\n",
        "misinfo_examples = df_climate_inference[df_climate_inference['predicted_label'] == 'misinformation'].head(10)\n",
        "\n",
        "print(\"10 Examples of Factual (predicted_label 0):\")\n",
        "print(factual_examples)\n",
        "\n",
        "print(\"\\n10 Examples of Misinformation (predicted_label 1):\")\n",
        "print(misinfo_examples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Non-Transfomer model\n",
        "\n",
        "Next best was bidirectional RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing batch 200 of 283...\n",
            "Tokenizing batch 283 of 283...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'ids'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     climate_tokenised \u001b[38;5;241m=\u001b[39m batch_tokenize(\n\u001b[1;32m     23\u001b[0m         df_climate_inference[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: custom_analyzer(text, trained_tokenizer\u001b[38;5;241m=\u001b[39mmisinfo_tokenizer)\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Convert to dictionaries with keys: \"input_ids\", \"attention_mask\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     climate_tokenised \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m         {\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding\u001b[38;5;241m.\u001b[39mids,\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding\u001b[38;5;241m.\u001b[39mattention_mask,\n\u001b[1;32m     32\u001b[0m         }\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m climate_tokenised\n\u001b[1;32m     34\u001b[0m     ]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INFERENCE_TOKENISED_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(climate_tokenised, f)\n",
            "Cell \u001b[0;32mIn[28], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     climate_tokenised \u001b[38;5;241m=\u001b[39m batch_tokenize(\n\u001b[1;32m     23\u001b[0m         df_climate_inference[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: custom_analyzer(text, trained_tokenizer\u001b[38;5;241m=\u001b[39mmisinfo_tokenizer)\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Convert to dictionaries with keys: \"input_ids\", \"attention_mask\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     climate_tokenised \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m         {\n\u001b[0;32m---> 30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m,\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding\u001b[38;5;241m.\u001b[39mattention_mask,\n\u001b[1;32m     32\u001b[0m         }\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m climate_tokenised\n\u001b[1;32m     34\u001b[0m     ]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INFERENCE_TOKENISED_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(climate_tokenised, f)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ids'"
          ]
        }
      ],
      "source": [
        "\n",
        "# pipeline for RNNs etc \n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(input_ids, dim=0),\n",
        "        \"attention_mask\": torch.stack(attention_masks, dim=0),\n",
        "    }\n",
        "\n",
        "TOKENIZER_DIR = './cache/misinfo_tokenizer.pkl'\n",
        "INFERENCE_TOKENISED_DIR = './cache/climate_tokenised.pkl'\n",
        "\n",
        "if os.path.exists(INFERENCE_TOKENISED_DIR):\n",
        "    with open(INFERENCE_TOKENISED_DIR, 'rb') as f:\n",
        "        climate_tokenised = pickle.load(f)\n",
        "    print(f\"Tokenized climate tweets pkl file found. Loading data...\")\n",
        "else:    \n",
        "    with open(TOKENIZER_DIR, 'rb') as f:\n",
        "        misinfo_tokenizer = pickle.load(f)\n",
        "        \n",
        "    climate_tokenised = batch_tokenize(\n",
        "        df_climate_inference[\"text\"],\n",
        "        32,\n",
        "        lambda text: custom_analyzer(text, trained_tokenizer=misinfo_tokenizer)\n",
        "    )\n",
        "    # Convert to dictionaries with keys: \"input_ids\", \"attention_mask\"\n",
        "    climate_tokenised = [\n",
        "        {\n",
        "            \"input_ids\": encoding.ids,\n",
        "            \"attention_mask\": encoding.attention_mask,\n",
        "        }\n",
        "        for encoding in climate_tokenised\n",
        "    ]\n",
        "\n",
        "    \n",
        "with open(INFERENCE_TOKENISED_DIR, 'wb') as f:\n",
        "    pickle.dump(climate_tokenised, f)\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=climate_tokenised) \n",
        "\n",
        "climate_dl = DataLoader(\n",
        "    dataset=list(climate_tokenised),\n",
        "    batch_size=32, \n",
        "    shuffle=False, \n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "EMBEDDINGS_FILE_PATH_CLIMATE = \"./cache/mapped_pretrained_embeddings_climate.pkl\"\n",
        "\n",
        "if os.path.exists(EMBEDDINGS_FILE_PATH_CLIMATE):\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'rb') as f:\n",
        "        embedding_tensor_climate = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n",
        "else:\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    mapped_pretrained_embeddings_climate = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "    embedding_tensor_climate = torch.FloatTensor(mapped_pretrained_embeddings_climate)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(EMBEDDINGS_FILE_PATH_CLIMATE, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor_climate, f)\n",
        "    print(f\"Saved embeddings to {EMBEDDINGS_FILE_PATH_CLIMATE}. Shape: {embedding_tensor_climate.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def bidirectional_rnn_inference(model, dataloader, use_lengths=True):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    # Inference loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            if use_lengths:\n",
        "                x_batch, lengths = batch\n",
        "                x_batch, lengths = x_batch.to(device), lengths.to(device)\n",
        "                logits = model(x_batch, lengths)\n",
        "            else:\n",
        "                x_batch = batch\n",
        "                x_batch = x_batch.to(device)\n",
        "                logits = model(x_batch)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)  # Get class predictions\n",
        "            predictions.extend(preds.cpu().numpy())  # Move to CPU and convert to NumPy\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Load the model \n",
        "model_path = \"./models/bi_lstm_model.pth\"  \n",
        "model_bi_lstm = torch.load(model_path)\n",
        "\n",
        "# Perform inference\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "predictions = bidirectional_rnn_inference(model_bi_lstm, climate_dl, use_lengths=True)\n",
        "\n",
        "# Save predictions to CSV\n",
        "df_climate_inference['predicted_label'] = predictions\n",
        "\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note/data/climate_predictions_rnn.csv\"\n",
        "df_climate_inference.to_csv(output_path_climate, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_path_climate}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# xAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSequenceClassification\n",
        "from captum.attr import IntegratedGradients, Lime\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import HTML, display\n",
        "from matplotlib import cm, colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Workflow Settings === #\n",
        "from IPython.display import display, HTML\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import cm, colors\n",
        "from captum.attr import IntegratedGradients, Lime\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pickle\n",
        "\n",
        "# === Setup === #\n",
        "device = torch.device(\"cpu\")  # Captum doesn't support MPS\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenized climate data\n",
        "CLIMATE_TOKENISED_BERT_DIR = './cache/climate_tokenised_bert.pkl'\n",
        "with open(CLIMATE_TOKENISED_BERT_DIR, 'rb') as f:\n",
        "    climate_tokenised_bert = pickle.load(f)\n",
        "\n",
        "subset_indices = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "climate_tokenised_bert_subset = {\n",
        "    \"input_ids\": climate_tokenised_bert[\"input_ids\"][subset_indices],\n",
        "    \"token_type_ids\": climate_tokenised_bert[\"token_type_ids\"][subset_indices],\n",
        "    \"attention_mask\": climate_tokenised_bert[\"attention_mask\"][subset_indices]\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"./models/transformer_results\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "bert_uncased_finetuned = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "bert_uncased_finetuned.to(device).eval()\n",
        "print(\"Model and tokenizer loaded successfully from:\", model_path)\n",
        "\n",
        "# Prepare data tensors and embeddings\n",
        "input_ids = torch.clone(climate_tokenised_bert_subset[\"input_ids\"]).detach().to(dtype=torch.long, device=device)\n",
        "attention_mask = torch.clone(climate_tokenised_bert_subset[\"attention_mask\"]).detach().to(dtype=torch.float32, device=device)\n",
        "# Extract embeddings directly from input_ids\n",
        "embedding_layer = bert_uncased_finetuned.get_input_embeddings()\n",
        "embeddings = embedding_layer(input_ids).clone().detach().requires_grad_(True).to(device)\n",
        "# Derive tokens from input_ids for reconstruction\n",
        "tokens_list = [bert_tokenizer.convert_ids_to_tokens(ids.cpu().numpy()) for ids in input_ids]\n",
        "\n",
        "\n",
        "# === Helper Functions === #\n",
        "\n",
        "# Reconstruct sentence and attributions\n",
        "def reconstruct_sentence(tokens, attributions, tokenizer):\n",
        "    word_tokens, word_attributions = [], []\n",
        "    current_word, current_score = \"\", 0.0\n",
        "\n",
        "    for token, score in zip(tokens, attributions):\n",
        "        if token.startswith(\"##\"):\n",
        "            current_score += score\n",
        "        else:\n",
        "            if current_word:  # Save previous word\n",
        "                word_tokens.append(current_word)\n",
        "                word_attributions.append(current_score)\n",
        "            current_word, current_score = token, score\n",
        "\n",
        "    if current_word:\n",
        "        word_tokens.append(current_word)\n",
        "        word_attributions.append(current_score)\n",
        "\n",
        "    special_tokens = tokenizer.special_tokens_map.values()\n",
        "    filtered_tokens = [t for t in word_tokens if t not in special_tokens]\n",
        "    filtered_attributions = [\n",
        "        a for t, a in zip(word_tokens, word_attributions) if t not in special_tokens\n",
        "    ]\n",
        "    sentence = \" \".join(filtered_tokens)\n",
        "    return sentence, filtered_attributions\n",
        "\n",
        "# Visualize token attributions\n",
        "def visualize_token_attributions(tokens, attributions, cmap='bwr', title=None):\n",
        "    attributions = np.array(attributions)\n",
        "    assert len(tokens) == len(attributions), \"Tokens and attributions lengths do not match!\"\n",
        "    norm = colors.Normalize(vmin=attributions.min(), vmax=attributions.max())\n",
        "    scalar_map = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "    html_content = f\"<div style='line-height:1.6;'>\"\n",
        "    for token, score in zip(tokens, attributions):\n",
        "        color = colors.rgb2hex(scalar_map.to_rgba(score))\n",
        "        html_content += f\"<span style='background-color:{color}; padding:2px; margin:2px;'>{token}</span> \"\n",
        "    html_content += \"</div>\"\n",
        "    display(HTML(html_content))\n",
        "\n",
        "# Compute Integrated Gradients\n",
        "def compute_integrated_gradients(embeddings, attention_mask, sample_input_ids, tokenizer, target_class=0, steps=50):\n",
        "    def forward_func(embeddings, attention_mask=None):\n",
        "        outputs = bert_uncased_finetuned(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
        "        return torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    ig = IntegratedGradients(forward_func)\n",
        "    baseline = torch.zeros_like(embeddings)\n",
        "    attributions_ig = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "        n_steps=steps,\n",
        "    )\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample_input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_ig.sum(dim=-1)[0].detach().cpu().numpy()\n",
        "    return reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "\n",
        "\n",
        "def compute_lime(sample_input_ids, attention_mask, tokenizer, target_class=0, n_samples=100):\n",
        "    def forward_func(input_ids, attention_mask=None):\n",
        "        outputs = bert_uncased_finetuned(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    lime = Lime(forward_func)\n",
        "    attributions_lime = lime.attribute(\n",
        "        inputs=sample_input_ids,\n",
        "        n_samples=n_samples,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_class,\n",
        "    )\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample_input_ids[0].cpu().numpy())\n",
        "    token_attributions = attributions_lime[0].detach().cpu().numpy().flatten()\n",
        "    return reconstruct_sentence(tokens, token_attributions, tokenizer)\n",
        "\n",
        "# === Main Execution Loop === #\n",
        "for idx in range(len(climate_tokenised_bert_subset[\"input_ids\"])):\n",
        "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
        "    \n",
        "    # Prepare single-sample embeddings, attention mask, and input_ids\n",
        "    sample_embeddings = embeddings[idx].unsqueeze(0)\n",
        "    sample_attention_mask = attention_mask[idx].unsqueeze(0)\n",
        "    sample_input_ids = input_ids[idx].unsqueeze(0)\n",
        "    \n",
        "    # IG\n",
        "    sentence_ig, word_attributions_ig = compute_integrated_gradients(\n",
        "        embeddings=sample_embeddings,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        sample_input_ids=sample_input_ids,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        steps=50\n",
        "    )\n",
        "    print(f\"Reconstructed Sentence: {sentence_ig}\")\n",
        "    visualize_token_attributions(sentence_ig.split(), word_attributions_ig, cmap='bwr', title=\"IG Attributions\")\n",
        "    print(f\"Word-Level Attributions (IG): {', '.join([f'{word} ({score:.4f})' for word, score in zip(sentence_ig.split(), word_attributions_ig)])}\")\n",
        "\n",
        "    # LIME\n",
        "    sentence_lime, word_attributions_lime = compute_lime(\n",
        "        sample_input_ids=sample_input_ids,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        target_class=0,\n",
        "        n_samples=100\n",
        "    )\n",
        "    visualize_token_attributions(sentence_lime.split(), word_attributions_lime, cmap='coolwarm', title=\"LIME Attributions\")\n",
        "    print(f\"Word-Level Attributions (LIME): {', '.join([f'{word} ({score:.4f})' for word, score in zip(sentence_lime.split(), word_attributions_lime)])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Blue represents negative attributions (meaning the token has a negative impact on the target class).\n",
        "Red represents positive attributions (meaning the token has a positive impact on the target class).\n",
        "\n",
        "target class = 0\n",
        "\n",
        "0 = Factual.\n",
        "\n",
        "So blue = more misinfo, red = more factual \n",
        "\n",
        "REVERSE THIS\n",
        "\n",
        "\n",
        "basically it's pretty much nonsense, but often it does seem to care about the end tags --> remove them and try again\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic modelling\n",
        "\n",
        "Tweets are short by nature, which could pose challenges for LDA, as it relies on the assumption that each document (tweet) has enough content to model a topic distribution.\n",
        "\n",
        "Preprocessing for LDA: You may need to aggregate tweets to form meaningful \"documents.\"\n",
        "LSA: Works more naturally with short texts but may not offer distinct \"topic distributions.\"\n",
        "\n",
        "If you want clear topic distributions, LDA is better.\n",
        "If you are looking for semantic relationships and dimensionality reduction (e.g., clustering or visualisation), LSA is suitable.\n",
        "\n",
        "## LSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_climate_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import streamlit as st\n",
        "\n",
        "misinfo_tweets = df_climate_inference[df_climate_inference['predicted_label'] == 'misinformation']\n",
        "\n",
        "\n",
        "# Extract the text column for LSA analysis\n",
        "misinfo_texts = misinfo_examples['text'].dropna()\n",
        "\n",
        "# Step 1: Vectorize the text using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(misinfo_texts)\n",
        "\n",
        "# Step 2: Perform Truncated SVD (LSA)\n",
        "n_components = 5  # Number of latent topics\n",
        "lsa_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Extract top words for each topic\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "top_words_per_topic = {}\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:10]\n",
        "    top_words_per_topic[f\"Topic {i+1}\"] = [term for term, _ in sorted_terms]\n",
        "\n",
        "topics_df = pd.DataFrame(top_words_per_topic)\n",
        "\n",
        "# Display topics with their top words\n",
        "st.title(\"LSA Topics with Top Words\")\n",
        "st.dataframe(topics_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Visualize the LSA topics using a bar chart for each topic\n",
        "for i, topic in enumerate(topics_df.columns):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    words = topics_df[topic].values\n",
        "    weights = lsa_model.components_[i][:len(words)]  # Corresponding weights for top words\n",
        "\n",
        "    # Plot bar chart for the current topic\n",
        "    plt.bar(words, weights[:len(words)], alpha=0.7)\n",
        "    plt.title(f\"Top Words in {topic}\", fontsize=14)\n",
        "    plt.xlabel(\"Words\", fontsize=12)\n",
        "    plt.ylabel(\"Weights\", fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# **3. Keyword Network Visualization**\n",
        "# Create a graph of keywords and their weights for each topic\n",
        "G = nx.Graph()\n",
        "for i, topic in enumerate(topics_df.columns):\n",
        "    for word, weight in zip(topics_df[topic], lsa_model.components_[i][:len(topics_df[topic])]):\n",
        "        G.add_edge(topic, word, weight=weight)\n",
        "\n",
        "# Draw the network\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "nx.draw(\n",
        "    G, pos, with_labels=True, node_size=500, font_size=10, edge_color=\"gray\", alpha=0.7\n",
        ")\n",
        "plt.title(\"Keyword Network Across Topics\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# **4. Sentiment Analysis**\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Calculate sentiment polarity for each misinformation tweet\n",
        "misinfo_tweets['sentiment'] = misinfo_tweets['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# Plot sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(misinfo_tweets['sentiment'], bins=20, color='blue', alpha=0.7)\n",
        "plt.title(\"Sentiment Distribution of Misinformation Tweets\", fontsize=14)\n",
        "plt.xlabel(\"Sentiment Polarity\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(axis=\"y\", alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Step 1: Clustering using KMeans\n",
        "n_clusters = 5  # Define the number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(lsa_matrix)\n",
        "\n",
        "# Add cluster labels to the original dataset\n",
        "misinfo_tweets['cluster'] = clusters\n",
        "\n",
        "# Step 2: Dimensionality reduction for visualization using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=10, learning_rate=200)\n",
        "tsne_matrix = tsne.fit_transform(lsa_matrix)\n",
        "\n",
        "# Step 3: Plot the clusters\n",
        "plt.figure(figsize=(10, 7))\n",
        "for cluster in range(n_clusters):\n",
        "    cluster_points = tsne_matrix[clusters == cluster]\n",
        "    plt.scatter(\n",
        "        cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster+1}\", alpha=0.7\n",
        "    )\n",
        "\n",
        "plt.title(\"Clusters of Misinformation Tweet Topics\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Share the clustered tweets\n",
        "tools.display_dataframe_to_user(name=\"Misinformation Tweet Clusters\", dataframe=misinfo_tweets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extend vocabulary to include topics\n",
        "\n",
        "vocabulary = vocab_idx\n",
        "extended_vocabulary = list(vocabulary) + list(topics_df.columns)\n",
        "\n",
        "# Map embeddings for the extended vocabulary\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        if word in pre_trained_embeddings:\n",
        "            embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "        else:\n",
        "            # Assign random embeddings for missing words (e.g., topics)\n",
        "            embedding_matrix[idx] = np.random.uniform(-0.1, 0.1, embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "# Generate embeddings for the extended vocabulary\n",
        "mapped_pretrained_embeddings = embedding_mapping_fasttext(\n",
        "    vocabulary=extended_vocabulary,\n",
        "    pre_trained_embeddings=ft\n",
        ")\n",
        "\n",
        "# Reduce embeddings to 2D\n",
        "embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "embedding_matrix = embedding_tensor.numpy()\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "# Map reduced embeddings to all nodes (including topics)\n",
        "pos = {word: reduced_embeddings[idx] for idx, word in enumerate(extended_vocabulary)}\n",
        "\n",
        "# Plot the network\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw(\n",
        "    G, pos, with_labels=True, node_size=500, font_size=10, edge_color=\"gray\", alpha=0.7\n",
        ")\n",
        "plt.title(\"Keyword Network with FastText Embedding-based Layout\", fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE  # or PCA/UMAP\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 1. Load your model & tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"./models/transformer_results\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "bert_uncased_finetuned = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "bert_uncased_finetuned.to(device).eval()\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully from:\", model_path)\n",
        "\n",
        "def get_embedding(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Returns the CLS embedding (or optionally the mean pooling)\n",
        "    from the last hidden layer of the model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs, output_hidden_states=True)\n",
        "        # The last hidden state is output.hidden_states[-1]\n",
        "        last_hidden_state = output.hidden_states[-1]  # shape: (batch_size, seq_len, hidden_size)\n",
        "        # Let's use the [CLS] token embedding by default\n",
        "        cls_embedding = last_hidden_state[:, 0, :].squeeze(0)\n",
        "    return cls_embedding.cpu().numpy()\n",
        "\n",
        "topics = topics_df.columns.tolist()\n",
        "\n",
        "node_embeddings = {}\n",
        "all_nodes = []\n",
        "\n",
        "# 3a. Collect topic embeddings\n",
        "for topic in topics:\n",
        "    # Use the topic name itself as text, or a short representative phrase if you have one\n",
        "    embedding = get_embedding(topic, bert_uncased_finetuned, bert_tokenizer, device)\n",
        "    node_embeddings[topic] = embedding\n",
        "    all_nodes.append(topic)\n",
        "\n",
        "# 3b. Collect keyword embeddings\n",
        "for topic in topics:\n",
        "    for keyword in topics_df[topic]:\n",
        "        keyword_str = keyword.strip()\n",
        "        if keyword_str not in node_embeddings:  # Avoid duplicate embeddings\n",
        "            embedding = get_embedding(keyword_str, bert_uncased_finetuned, bert_tokenizer, device)\n",
        "            node_embeddings[keyword_str] = embedding\n",
        "            all_nodes.append(keyword_str)\n",
        "\n",
        "X = np.array([node_embeddings[node] for node in all_nodes])\n",
        "\n",
        "# 4. Apply t-SNE to get 2D vectors\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
        "X_2d = tsne.fit_transform(X)\n",
        "\n",
        "# We'll store the 2D coords in a dictionary keyed by node\n",
        "pos_dict = {}\n",
        "for node, (x, y) in zip(all_nodes, X_2d):\n",
        "    pos_dict[node] = (x, y)\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "# Example: for each topic i, we take each word in topics_df[topic_i]\n",
        "# and link them with an edge whose weight is lsa_model.components_[i][j]\n",
        "# (assuming j indexes the same keyword in that row)\n",
        "for i, topic in enumerate(topics):\n",
        "    # i-th row in lsa_model.components_ corresponds to weights for topic i\n",
        "    # We'll match each keyword and weight by index\n",
        "    for keyword_index, keyword in enumerate(topics_df[topic]):\n",
        "        weight = lsa_model.components_[i][keyword_index]\n",
        "        # Add an edge between the topic name and the keyword\n",
        "        G.add_edge(topic, keyword, weight=weight)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# 6a. Draw the graph using our pos_dict as node positions\n",
        "nx.draw(\n",
        "    G, \n",
        "    pos_dict,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    font_size=10,\n",
        "    edge_color=\"gray\",\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title(\"Topic–Keyword Network (positioned by Transformer Embeddings)\", fontsize=14)\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Prepare the embeddings\n",
        "embedding_matrix = embeddings.cpu().detach().numpy().reshape(embeddings.shape[0], -1)\n",
        "\n",
        "# Step 2: Dimensionality Reduction using t-SNE (or PCA as an alternative)\n",
        "# t-SNE\n",
        "#tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200)\n",
        "#reduced_embeddings = tsne.fit_transform(embedding_matrix)\n",
        "\n",
        "# PCA (if you want a faster alternative)\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "# Step 3: Assign colors based on topics\n",
        "# Assume `topics` is a list where each entry corresponds to the topic of a document\n",
        "# Example: topics = [\"topic1\", \"topic2\", ...]\n",
        "topic_labels = topics_df.columns  # Get topic labels from your DataFrame\n",
        "topic_indices = np.argmax(lsa_model.transform(embedding_matrix), axis=1)  # Assign topics\n",
        "\n",
        "# Generate a color map\n",
        "unique_topics = np.unique(topic_labels)\n",
        "color_map = {topic: plt.cm.tab10(i / len(unique_topics)) for i, topic in enumerate(unique_topics)}\n",
        "\n",
        "# Step 4: Plot the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "for topic in unique_topics:\n",
        "    indices = np.where(topic_indices == topic_labels.index(topic))[0]\n",
        "    plt.scatter(\n",
        "        reduced_embeddings[indices, 0],\n",
        "        reduced_embeddings[indices, 1],\n",
        "        label=topic,\n",
        "        alpha=0.7,\n",
        "        edgecolors=\"k\",\n",
        "    )\n",
        "\n",
        "plt.title(\"2D Visualization of Embeddings by Topic\")\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend(title=\"Topics\", loc=\"best\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
