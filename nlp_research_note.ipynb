{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPU \n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "os.environ['PIP_CACHE_DIR'] = '/workspace/workspace/cache'\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/workspace/cache'\n",
    "os.environ['TORCH_CACHE'] = '/workspace/workspace/cache'\n",
    "os.environ['HF_CACHE'] = '/workspace/workspace/cache'\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fasttext.util as fasttext_util\n",
    "import fasttext\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore', path='/workspace/workspace/cache')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fasttext.util as fasttext_util\n",
    "import fasttext \n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# specify custom functions --------------------------------------------------------------\n",
    "def custom_tokenizer(text):\n",
    "    tokenized_text = nlp(text)\n",
    "    return [tok.text for tok in tokenized_text]\n",
    "\n",
    "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
    "    vocab_size = len(vocabulary)\n",
    "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
    "    return embedding_matrix\n",
    "\n",
    "# download pretrained embeddings --------------------------------------------------------\n",
    "fasttext_util.download_model('en', if_exists='ignore')\n",
    "\n",
    "# download spacy model for tokenization -------------------------------------------------\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load rumours dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "os.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\n",
    "print(\"Working Directory:\", os.getcwd())\n",
    "\n",
    "path_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\n",
    "print(\"Path to rumour dataset files:\", path_rumour)\n",
    "\n",
    "path_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
    "print(\"Path to dataset files:\", path_climate)\n",
    "\n",
    "# List all files in the directory\n",
    "files_rumour = os.listdir(path_rumour)\n",
    "files_climate = os.listdir(path_climate)\n",
    "\n",
    "# Print the list of files\n",
    "print(\"Files in rumour dataset directory:\", files_rumour)\n",
    "print(\"Files in climate dataset directory:\", files_climate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "combined_data = []\n",
    "\n",
    "for numb in [15, 16]:\n",
    "    path_numb = files_rumour[numb - 15]  # 'twitter15' or 'twitter16' folder path\n",
    "    label_file_path_numb = os.path.join(path_rumour, path_numb, 'label.txt')\n",
    "    tweets_file_path_numb = os.path.join(path_rumour, path_numb, 'source_tweets.txt')\n",
    "\n",
    "    # Read label.txt\n",
    "    label_dict = {}\n",
    "    with open(label_file_path_numb, 'r') as file:\n",
    "        for line in file:\n",
    "            label, tweet_id = line.strip().split(':')\n",
    "            label_dict[tweet_id] = label\n",
    "\n",
    "    # Read source_tweets.txt\n",
    "    tweets_dict = {}\n",
    "    with open(tweets_file_path_numb, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet_id, tweet_content = line.strip().split('\\t', 1)\n",
    "            tweets_dict[tweet_id] = tweet_content\n",
    "\n",
    "    # Combine labels with tweets\n",
    "    for tweet_id, tweet_content in tweets_dict.items():\n",
    "        if tweet_id in label_dict:\n",
    "            combined_data.append((label_dict[tweet_id], tweet_content))\n",
    "\n",
    "    print(f\"twitter_{numb}:\") \n",
    "\n",
    "    for label, tweet in combined_data[:5]:\n",
    "        print(f\"    Label: {label}, Tweet: {tweet}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop URL part\n",
    "chosen specifically because it is twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "print(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\n",
    "\n",
    "unique_labels = set(label for label, tweet in combined_data)\n",
    "\n",
    "print(\"Unique labels in combined data:\")\n",
    "for label in unique_labels:\n",
    "    print(f\"   \", label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "df_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\n",
    "\n",
    "print(df_combined.shape)\n",
    "df_combined.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "grouped = df_combined.groupby(\"Label\")\n",
    "\n",
    "# 5 random examples for each label\n",
    "for label, group in grouped:\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    sample = group.sample(n=5, random_state=42)  \n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"   Tweet: {row['Tweet']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Hugging Face misinfo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.cache/huggingface/datasets\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face\n",
    "ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
    "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"~/.cache/huggingface/datasets\")\n",
    "print(hf_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to balance the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (92394, 4), 'test': (10267, 4)}\n",
      "['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label']\n",
      "{'Unnamed: 0.1': [34366, 41656, 26726, 81585, 4016], 'Unnamed: 0': [34366, 41656, 26726, 81585, 4016], 'text': [\"Local Charlotte, NC news station WSOCTV is reporting that sources tell them dash cameras captured Keith Scott getting out of car and coming towards officers with a gun in his hand:#BREAKING: Sources tell Channel 9 dash camera video shows #KeithScott getting out car, coming toward officers with gun in his hand pic.twitter.com/GGuM2Ow3wk  WSOCTV (@wsoctv) September 21, 2016For a second night, protests over a deadly officer-involved shooting in Charlotte, North Carolina, turned violent, with police firing tear gas and demonstrators throwing objects and trying to damage vehicles.Keith Lamont Scott, a father of seven, was killed by police in an apartment complex parking lot Tuesday as officers looked for another man named in a warrant they were trying to serve. The shooting set off a long night of violent protests and Wednesday the demonstrations continued for a second night, starting off as a peaceful march through downtown Charlotte. But when the demonstrators neared an Omni Hotel, some people climbed the roof of an outdoor mall and started throwing objects at the crowd.Other people were banging on the doors of the hotel, and police in riot gear emerged in the entrance.His family said Scott, an African-American, was unarmed and sitting in his car reading a book, waiting for his son to come home from school.But Charlotte-Mecklenburg Police Chief Kerr Putney said Scott exited his car with a gun, not a book. He said officers couldn t find a book at the scene. It s time for the voiceless majority to stand up and be heard,  said the police chief, who is black. It s time to change the narrative because I can tell you from the facts that the story s a little bit different as to how it s been portrayed so far, especially through social media.   CNNBut b b but what about the book?Neighbor who says she saw police shoot #KeithScott disputes CMDP's claim that he was armed with a gun. She says he had a book. @wsoctv pic.twitter.com/X1PWw5QJ7O  Mark Barber (@MBarberWSOC9) September 21, 2016And of course, there s Hillary s irresponsible tweet confirming that the cops were in the wrong, without even having any facts to back up her ignorant statement:Keith Lamont Scott. Terence Crutcher. Too many others. This has got to end. -H  Hillary Clinton (@HillaryClinton) September 21, 2016 \", 'The tsunami has started President Obama s Kenyan half-brother wants to make America great again   so he s voting for Donald Trump. I like Donald Trump because he speaks from the heart,  Malik Obama told The Post from his home in the rural village of Kogelo.  Make America Great Again is a great slogan. I would like to meet him. Obama, 58, a longtime Democrat, said his  deep disappointment  in his brother Barack s administration has led him to recently switch allegiance to  the party of Lincoln. The last straw, he said, came earlier this month when FBI Director James Comey recommended not prosecuting Democratic presidential candidate Hillary Clinton over her use of a private e-mail servers while secretary of state. She should have known better as the custodian of classified information,  said Obama.He s also annoyed that Clinton and President Obama killed Libyan leader Moammar Khadafy, whom he called one of his best friends.Malik Obama dedicated his 2012 biography of his late father to Khadafy and others who were  making this world a better place. I still feel that getting rid of Khadafy didn t make things any better in Libya,  he said.  My brother and the secretary of state disappointed me in that regard. But what bothers him even more is the Democratic Party s support of same-sex marriage.Obama plans to trek back to the US to vote for Trump in November. Obama used to live in Maryland, where he worked for many years as an accountant and is registered to vote there, public records show. Mr. Trump is providing something new and something fresh,  he said.For entire story: NYP', 'The only reality show Donald Trump should have ever been featured in is The Biggest Loser because he just got his ass handed to him in court.Two years ago, Trump National Doral Miami golf resort signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course.Well, guess who tried to stiff The Paint Spot of the final $34,863 payment in the deal?Yeah, that would be Republican nominee Donald J. Trump.Trump and his company refused to honor the contract by not paying the final payment, saying that they ve  paid enough  for the paint. In other words, Trump negotiated a deal that ended up costing him more in the end, just like the kinds of deals he wants to negotiate for America with the rest of the world.Anyway, Paint Spot owner Juan Carlos Enriquez filed suit against Trump in court, and Judge Jorge Cueto just slapped Trump and his company with a $300,000 hit to cover The Paint Shop s attorney and court fees, nearly ten times more money than the $34,863 owed. And Trump STILL hasn t paid that debt, so how are we supposed to trust him to pay down the debt of en entire nation if he can t meet his own obligations? I m happy I have a judgment,  Enriquez told the Miami Herald.  But he [Trump] hasn t paid yet. You know how he says he ll surround himself with the greatest people if he is president? In this case, he might not be surrounded by the right people. This isn t even the first fine Trump has been ordered to pay this month.On the same day as his coronation as the Republican Party nominee last week, the National Labor Relations Board slapped Trump with an $11,200 fine for treating employees like shit because they tried to join a labor union.As it turns out, one employee was wrongfully fired and the other was retaliated against by denying them the promotion they had earned.For someone who claims to be the  law and order  candidate, Trump sure does break the law a lot.Featured Image: Christopher Furlong/Getty Images', 'No Food, No FEMA: Hurricane Michael’s Survivors Are Furious https://thebea.st/2CIXNoz\\xa0', 'WASHINGTON (Reuters) - Here are some of the highlights of the Reuters interview with U.S. President Donald Trump on Thursday. “There’s a chance that we could end up having a major, major, conflict with North Korea, absolutely.” QUESTION: Is that your biggest global worry at this point? “Yes, I would say that’s true, yes. ... North Korea would be certainly that.” ON GETTING SOUTH KOREA TO PAY FOR THAAD MISSILE DEFENSE SYSTEM “On the THAAD system, it’s about a billion dollars. I said,  ‘Why are we paying? Why are we paying a billion dollars? We’re protecting. Why are we paying a billion dollars?’ So I informed South Korea it would be appropriate if they paid. Nobody’s going to do that. Why are we paying a billion dollars? It’s a billion dollar system. It’s phenomenal. It’s the most incredible equipment you’ve ever seen - shoots missiles right out of the sky. And it protects them and I want to protect them. We’re going to protect them. But they should pay for that, and they understand that.” ON WHETHER THE WAR AGAINST ISLAMIST EXTREMISM WILL EVER END “Yours is the toughest question. Because at what point does it end? But we can’t let them come over here. I have to say, there is an end. And it has to be humiliation. There is an end. Otherwise it’s really tough. But there is an end. We are really eradicating some very bad people. When you take a look at what’s going on with the cutting off of the heads. We haven’t seen that since Medieval times. Right?”  ON CHINESE PRESIDENT XI’S EFFORTS TO REIN IN NORTH KOREA “He certainly doesn’t want to see turmoil and death. He doesn’t want to see it. He’s a good man. He’s a very good man and I got to know him very well ... We’ll see how it all works out. I know he would like to be able to do something. Perhaps it’s possible that he can’t. But I think he’d like to be able to do something.” “He’s 27 years old, his father dies, took over a regime, so say what you want but that’s not easy, especially at that age. You know you have plenty of generals in there and plenty of other people that would like to do what he’s doing. So I’ve said this before and I’ve, I’m just telling you, and I’m not giving him credit or not giving him credit. I’m just saying that’s a very hard thing to do.” “As to whether or not he’s rational, I have no opinion on it. I hope he’s rational.” “I get a call from Mexico yesterday, ‘We hear you’re going to terminate NAFTA.’ I said that’s right. They said, ‘Is there any way we can do something without you – without termination?’ I said, ‘What do you want to do?’ He said, ‘Well, we’d like to negotiate.’ I said we’ll think about it. Then I get a call, and they call me, I get a call from Justin Trudeau and he said, ‘We’d like to see if we can work something out,’ and I said that’s fine. Because I’ve always - I’ve been very consistent. It’s much less disruptive if we can make a fair trade deal than if we terminate.” “It’s unacceptable. It’s a horrible deal made by Hillary. It’s a horrible deal. And we’re going to renegotiate that deal, or terminate it.”  QUESTION: When will you announce it? “Very soon. I’m announcing it now.” “By the way, with South Korea, just so you know. They’re ready for it. Mike Pence was representing me, he was just over there, he’s told them. And we have the five-year anniversary coming up very shortly. And we thought that would be a good time to start ... It’s a great deal for South Korea. It’s a terrible deal for us.” “Frankly, Saudi Arabia has not treated us fairly, because we are losing a tremendous amount of money in defending Saudi Arabia.” “Well, my problem is that I’ve established a very good personal relationship with (Chinese) President Xi. And I really feel that he is doing everything in his power to help us with a big situation, so I wouldn’t want to be causing difficulty right now for him ... So I would certainly want to speak to him first.” “If there’s closure, there’s closure. We’ll see what happens. If there’s a shutdown. It’s the Democrats’ fault. Not our fault. It’s the Democrats’ fault. Maybe they’d like to see a shutdown.”  ON TRUMP’S PLAN TO GENERATE REVENUE TO OFFSET TAX CUTS “We will do trade deals that are going to make up for a tremendous amount of the deficit. We are going to be doing trade deals that are going to be much better trade deals ...  “There will be other ways that we are going to raise revenues. But we are going to run the country properly, and we are going to be reimbursed when we do things. Why should we be paying for somebody else’s military?” ON MIDDLE EAST PEACE AND POSSIBLE TRIP TO ISRAEL, SAUDI ARABIA “It’s a possibility, we’re talking to both. It’s a possibility, but I want to see peace with Israel and the Palestinians. There is no reason there’s not peace between Israel and the Palestinians - none whatsoever. So we’re looking at that and we’re also looking at the potential of going to Saudi Arabia.” '], 'label': [1, 1, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "\n",
    "print(ds['train'].column_names)\n",
    "print(ds['train'][:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (92394, 2) \n",
      "\n",
      "Training positive vs negative examples: \n",
      " label\n",
      "0    0.652737\n",
      "1    0.347263\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Testing positive vs negative examples: \n",
      " label\n",
      "0    0.659686\n",
      "1    0.340314\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Local Charlotte, NC news station WSOCTV is rep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The tsunami has started President Obama s Keny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The only reality show Donald Trump should have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Food, No FEMA: Hurricane Michael’s Survivor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WASHINGTON (Reuters) - Here are some of the hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Local Charlotte, NC news station WSOCTV is rep...      1\n",
       "1  The tsunami has started President Obama s Keny...      1\n",
       "2  The only reality show Donald Trump should have...      1\n",
       "3  No Food, No FEMA: Hurricane Michael’s Survivor...      0\n",
       "4  WASHINGTON (Reuters) - Here are some of the hi...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_cloned = ds.copy()\n",
    "\n",
    "# DATA PARTITIONING =====================================================================\n",
    "\n",
    "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "\n",
    "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
    "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
    "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
    "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
    "\n",
    "df_misinfo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance train split -------------------------------------------------------------------\n",
    "'''\n",
    "balancer = RandomUnderSampler(random_state=42, sampling_strategy = 'majority')\n",
    "df_misinfo_train_balanced = pd.concat(balancer.fit_resample(X = df_misinfo_train.iloc[:,[0]],\n",
    "                                                           y = df_misinfo_train.iloc[:,[1]]),\n",
    "                                     axis=1).sample(frac = 1).reset_index(drop=True)\n",
    "df_misinfo_train_balanced.value_counts(\"y\")/df_misinfo_train_balanced.shape[0]\n",
    "df_misinfo_train_balanced.shape[0]\n",
    "df_misinfo_train.shape[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "Fitting Tokenizer...\n",
      "Tokenizing Train Data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m misinfo_tokenizer_analyzer \u001b[38;5;241m=\u001b[39m misinfo_tokenizer\u001b[38;5;241m.\u001b[39mbuild_analyzer()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing Train Data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m misinfo_train_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mdf_misinfo_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmisinfo_tokenizer_analyzer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing Test Data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m misinfo_test_tokens \u001b[38;5;241m=\u001b[39m df_misinfo_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(misinfo_tokenizer_analyzer, trained_tokenizer\u001b[38;5;241m=\u001b[39mmisinfo_tokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/pandas/core/series.py:4700\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4621\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4622\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4623\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4624\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4626\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4627\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4698\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4700\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4702\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4703\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mcustom_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_tokenizer\u001b[39m(text):\n\u001b[0;32m----> 3\u001b[0m     tokenized_text \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [tok\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokenized_text]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/language.py:1123\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1121\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/tokenizer.pyx:161\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/tokenizer.pyx:264\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._apply_special_cases\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/tokenizer.pyx:304\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._prepare_special_spans\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/tokens/doc.pyx:500\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_1/lib/python3.9/site-packages/spacy/util.py:1282\u001b[0m, in \u001b[0;36mnormalize_slice\u001b[0;34m(length, start, stop, step)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             new_excs[new_key] \u001b[38;5;241m=\u001b[39m new_value\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_excs\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_slice\u001b[39m(\n\u001b[1;32m   1283\u001b[0m     length: \u001b[38;5;28mint\u001b[39m, start: \u001b[38;5;28mint\u001b[39m, stop: \u001b[38;5;28mint\u001b[39m, step: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE057)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "# specify custom functions --------------------------------------------------------------\n",
    "def custom_tokenizer(text):\n",
    "    tokenized_text = nlp(text)\n",
    "    return [tok.text for tok in tokenized_text]\n",
    "\n",
    "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
    "    vocab_size = len(vocabulary)\n",
    "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
    "    return embedding_matrix\n",
    "\n",
    "# download pretrained embeddings --------------------------------------------------------\n",
    "fasttext_util.download_model('en', if_exists='ignore')\n",
    "\n",
    "# download spacy model for tokenization -------------------------------------------------\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "'''\n",
    "\n",
    "# TOKENIZATION ==========================================================================\n",
    "# tokenize texts ------------------------------------------------------------------------\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "\n",
    "# Fit tokenizer with progress bar\n",
    "misinfo_tokenizer = CountVectorizer(analyzer=\"word\",\n",
    "                                   tokenizer=custom_tokenizer,\n",
    "                                   lowercase=False,\n",
    "                                   min_df=3)\n",
    "\n",
    "print(\"Fitting Tokenizer...\")\n",
    "misinfo_tokenizer.fit(df_misinfo_train[\"text\"])\n",
    "\n",
    "misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
    "\n",
    "\n",
    "print(\"Tokenizing Train Data...\")\n",
    "misinfo_train_tokens = df_misinfo_train[\"text\"].map(misinfo_tokenizer_analyzer)\n",
    "\n",
    "print(\"Tokenizing Test Data...\")\n",
    "misinfo_test_tokens = df_misinfo_test[\"text\"].apply(misinfo_tokenizer_analyzer, trained_tokenizer=misinfo_tokenizer)\n",
    "\n",
    "def custom_analyzer(text, trained_tokenizer):\n",
    "    tokens = custom_tokenizer(text)\n",
    "    vocab = trained_tokenizer.vocabulary_\n",
    "    return [token if token in vocab else \"<unk>\" for token in tokens]\n",
    "\n",
    "misinfo_test_tokens = df_misinfo_test[\"text\"].apply(custom_analyzer, trained_tokenizer=misinfo_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: INPUT PIPELINE ================================================================\n",
    "\n",
    "# vocabulary indexing -------------------------------------------------------------------\n",
    "def vocab_mapping(tokenized_text):\n",
    "    token_counts = Counter()\n",
    "    for text in tokenized_text:\n",
    "        token_counts.update(text)\n",
    "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
    "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "    return vocab\n",
    "\n",
    "vocab_idx = vocab_mapping(tokenized_text=reddit_train_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading climate df\n",
    "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\" \n",
    "\n",
    "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
    "\n",
    "df_climate = pd.read_csv(input_path_climate)\n",
    "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
    "df_climate.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
