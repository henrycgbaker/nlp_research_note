{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/nlp_research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W5n3aNK6TvsQ",
        "outputId": "3859cf8b-38e5-433d-9c4c-f41ce823d9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport os\\n# Set the environment variables\\nos.environ['HOME_CONFIG'] = './/workspace/workspace'\\nos.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\\nos.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\\n\\n# Optionally, check if the environment variables were set correctly\\nprint(os.getenv('HOME_CONFIG'))\\nprint(os.getenv('KAGGLE_CONFIG'))\\nprint(os.getenv('SPACY_CACHE'))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Hertie server\n",
        "'''\n",
        "import os\n",
        "# Set the environment variables\n",
        "os.environ['HOME_CONFIG'] = './/workspace/workspace'\n",
        "os.environ['KAGGLE_CONFIG'] = './workspace/workspace/.kaggle'\n",
        "os.environ['SPACY_CACHE'] = '/workspace/workspace/cache'\n",
        "\n",
        "# Optionally, check if the environment variables were set correctly\n",
        "print(os.getenv('HOME_CONFIG'))\n",
        "print(os.getenv('KAGGLE_CONFIG'))\n",
        "print(os.getenv('SPACY_CACHE'))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# For Google Colab (if needed)\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9ush0qlkpkB",
        "outputId": "bf339a12-4f5e-4604-a877-46d5f95878eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# specify custom functions --------------------------------------------------------------\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "# download pretrained embeddings --------------------------------------------------------\n",
        "# for local\n",
        "#fasttext_util.download_model('en', if_exists='ignore')\n",
        "\n",
        "# for Gdrive\n",
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/cc.en.300.bin\"\n",
        "ft = fasttext.load_model(model_path)\n",
        "\n",
        "\n",
        "# download spacy model for tokenization -------------------------------------------------\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVrRXb9OkpkC"
      },
      "source": [
        "# Import & process `rumours` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ebXrht6ykpkC",
        "outputId": "7fbfb277-d37f-44d5-f6af-b0f0da01cccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nos.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\\nprint(\"Working Directory:\", os.getcwd())\\n\\npath_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\\nprint(\"Path to rumour dataset files:\", path_rumour)\\n\\npath_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\\nprint(\"Path to dataset files:\", path_climate)\\n\\n# List all files in the directory\\nfiles_rumour = os.listdir(path_rumour)\\nfiles_climate = os.listdir(path_climate)\\n\\n# Print the list of files\\nprint(\"Files in rumour dataset directory:\", files_rumour)\\nprint(\"Files in climate dataset directory:\", files_climate)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"\"\"\n",
        "os.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\n",
        "print(\"Working Directory:\", os.getcwd())\n",
        "\n",
        "path_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\n",
        "print(\"Path to rumour dataset files:\", path_rumour)\n",
        "\n",
        "path_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
        "print(\"Path to dataset files:\", path_climate)\n",
        "\n",
        "# List all files in the directory\n",
        "files_rumour = os.listdir(path_rumour)\n",
        "files_climate = os.listdir(path_climate)\n",
        "\n",
        "# Print the list of files\n",
        "print(\"Files in rumour dataset directory:\", files_rumour)\n",
        "print(\"Files in climate dataset directory:\", files_climate)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "9V-VBbTdkpkC",
        "outputId": "834a6c22-5800-4911-ff75-f959ede44014"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncombined_data = []\\n\\nfor numb in [15, 16]:\\n    path_numb = files_rumour[numb - 15]  # \\'twitter15\\' or \\'twitter16\\' folder path\\n    label_file_path_numb = os.path.join(path_rumour, path_numb, \\'label.txt\\')\\n    tweets_file_path_numb = os.path.join(path_rumour, path_numb, \\'source_tweets.txt\\')\\n\\n    # Read label.txt\\n    label_dict = {}\\n    with open(label_file_path_numb, \\'r\\') as file:\\n        for line in file:\\n            label, tweet_id = line.strip().split(\\':\\')\\n            label_dict[tweet_id] = label\\n\\n    # Read source_tweets.txt\\n    tweets_dict = {}\\n    with open(tweets_file_path_numb, \\'r\\') as file:\\n        for line in file:\\n            tweet_id, tweet_content = line.strip().split(\\'\\t\\', 1)\\n            tweets_dict[tweet_id] = tweet_content\\n\\n    # Combine labels with tweets\\n    for tweet_id, tweet_content in tweets_dict.items():\\n        if tweet_id in label_dict:\\n            combined_data.append((label_dict[tweet_id], tweet_content))\\n\\n    print(f\"twitter_{numb}:\")\\n\\n    for label, tweet in combined_data[:5]:\\n        print(f\"    Label: {label}, Tweet: {tweet}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\"\"\"\n",
        "combined_data = []\n",
        "\n",
        "for numb in [15, 16]:\n",
        "    path_numb = files_rumour[numb - 15]  # 'twitter15' or 'twitter16' folder path\n",
        "    label_file_path_numb = os.path.join(path_rumour, path_numb, 'label.txt')\n",
        "    tweets_file_path_numb = os.path.join(path_rumour, path_numb, 'source_tweets.txt')\n",
        "\n",
        "    # Read label.txt\n",
        "    label_dict = {}\n",
        "    with open(label_file_path_numb, 'r') as file:\n",
        "        for line in file:\n",
        "            label, tweet_id = line.strip().split(':')\n",
        "            label_dict[tweet_id] = label\n",
        "\n",
        "    # Read source_tweets.txt\n",
        "    tweets_dict = {}\n",
        "    with open(tweets_file_path_numb, 'r') as file:\n",
        "        for line in file:\n",
        "            tweet_id, tweet_content = line.strip().split('\\t', 1)\n",
        "            tweets_dict[tweet_id] = tweet_content\n",
        "\n",
        "    # Combine labels with tweets\n",
        "    for tweet_id, tweet_content in tweets_dict.items():\n",
        "        if tweet_id in label_dict:\n",
        "            combined_data.append((label_dict[tweet_id], tweet_content))\n",
        "\n",
        "    print(f\"twitter_{numb}:\")\n",
        "\n",
        "    for label, tweet in combined_data[:5]:\n",
        "        print(f\"    Label: {label}, Tweet: {tweet}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rkaJwNkpkD"
      },
      "source": [
        "crop URL part\n",
        "chosen specifically because it is twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "q8zTf_BZkpkD",
        "outputId": "66396403-c5cb-4311-8105-3c08e6037052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\\n\\nunique_labels = set(label for label, tweet in combined_data)\\n\\nprint(\"Unique labels in combined data:\")\\nfor label in unique_labels:\\n    print(f\"   \", label)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\n",
        "\n",
        "unique_labels = set(label for label, tweet in combined_data)\n",
        "\n",
        "print(\"Unique labels in combined data:\")\n",
        "for label in unique_labels:\n",
        "    print(f\"   \", label)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "F2rdYR43kpkE",
        "outputId": "0c476405-5efe-4378-86b1-3aeff74e5d3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndf_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\\n\\nprint(df_combined.shape)\\ndf_combined.head()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "\"\"\"\n",
        "df_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\n",
        "\n",
        "print(df_combined.shape)\n",
        "df_combined.head()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d6vzv6U-kpkE",
        "outputId": "aff5bcf8-ae55-4f04-d493-2e8a9ad8178e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ngrouped = df_combined.groupby(\"Label\")\\n\\n# 5 random examples for each label\\nfor label, group in grouped:\\n    print(f\"\\nLabel: {label}\")\\n    sample = group.sample(n=5, random_state=42)\\n    for _, row in sample.iterrows():\\n        print(f\"   Tweet: {row[\\'Tweet\\']}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\"\"\"\n",
        "grouped = df_combined.groupby(\"Label\")\n",
        "\n",
        "# 5 random examples for each label\n",
        "for label, group in grouped:\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    sample = group.sample(n=5, random_state=42)\n",
        "    for _, row in sample.iterrows():\n",
        "        print(f\"   Tweet: {row['Tweet']}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckd7tkDtkpkE",
        "outputId": "32784aa9-e08a-44ef-e0ac-b3561df3c9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~/.cache/huggingface/datasets\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"~/.cache/huggingface/datasets\")\n",
        "print(hf_cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oYz9mC3kpkF"
      },
      "source": [
        "need to balance the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpTGTCzvkpkF",
        "outputId": "aceae088-9ce7-4a39-9563-d3c38265eb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label']\n",
            "{'Unnamed: 0.1': [34366, 41656, 26726, 81585, 4016], 'Unnamed: 0': [34366, 41656, 26726, 81585, 4016], 'text': [\"Local Charlotte, NC news station WSOCTV is reporting that sources tell them dash cameras captured Keith Scott getting out of car and coming towards officers with a gun in his hand:#BREAKING: Sources tell Channel 9 dash camera video shows #KeithScott getting out car, coming toward officers with gun in his hand pic.twitter.com/GGuM2Ow3wk  WSOCTV (@wsoctv) September 21, 2016For a second night, protests over a deadly officer-involved shooting in Charlotte, North Carolina, turned violent, with police firing tear gas and demonstrators throwing objects and trying to damage vehicles.Keith Lamont Scott, a father of seven, was killed by police in an apartment complex parking lot Tuesday as officers looked for another man named in a warrant they were trying to serve. The shooting set off a long night of violent protests and Wednesday the demonstrations continued for a second night, starting off as a peaceful march through downtown Charlotte. But when the demonstrators neared an Omni Hotel, some people climbed the roof of an outdoor mall and started throwing objects at the crowd.Other people were banging on the doors of the hotel, and police in riot gear emerged in the entrance.His family said Scott, an African-American, was unarmed and sitting in his car reading a book, waiting for his son to come home from school.But Charlotte-Mecklenburg Police Chief Kerr Putney said Scott exited his car with a gun, not a book. He said officers couldn t find a book at the scene. It s time for the voiceless majority to stand up and be heard,  said the police chief, who is black. It s time to change the narrative because I can tell you from the facts that the story s a little bit different as to how it s been portrayed so far, especially through social media.   CNNBut b b but what about the book?Neighbor who says she saw police shoot #KeithScott disputes CMDP's claim that he was armed with a gun. She says he had a book. @wsoctv pic.twitter.com/X1PWw5QJ7O  Mark Barber (@MBarberWSOC9) September 21, 2016And of course, there s Hillary s irresponsible tweet confirming that the cops were in the wrong, without even having any facts to back up her ignorant statement:Keith Lamont Scott. Terence Crutcher. Too many others. This has got to end. -H  Hillary Clinton (@HillaryClinton) September 21, 2016 \", 'The tsunami has started President Obama s Kenyan half-brother wants to make America great again   so he s voting for Donald Trump. I like Donald Trump because he speaks from the heart,  Malik Obama told The Post from his home in the rural village of Kogelo.  Make America Great Again is a great slogan. I would like to meet him. Obama, 58, a longtime Democrat, said his  deep disappointment  in his brother Barack s administration has led him to recently switch allegiance to  the party of Lincoln. The last straw, he said, came earlier this month when FBI Director James Comey recommended not prosecuting Democratic presidential candidate Hillary Clinton over her use of a private e-mail servers while secretary of state. She should have known better as the custodian of classified information,  said Obama.He s also annoyed that Clinton and President Obama killed Libyan leader Moammar Khadafy, whom he called one of his best friends.Malik Obama dedicated his 2012 biography of his late father to Khadafy and others who were  making this world a better place. I still feel that getting rid of Khadafy didn t make things any better in Libya,  he said.  My brother and the secretary of state disappointed me in that regard. But what bothers him even more is the Democratic Party s support of same-sex marriage.Obama plans to trek back to the US to vote for Trump in November. Obama used to live in Maryland, where he worked for many years as an accountant and is registered to vote there, public records show. Mr. Trump is providing something new and something fresh,  he said.For entire story: NYP', 'The only reality show Donald Trump should have ever been featured in is The Biggest Loser because he just got his ass handed to him in court.Two years ago, Trump National Doral Miami golf resort signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course.Well, guess who tried to stiff The Paint Spot of the final $34,863 payment in the deal?Yeah, that would be Republican nominee Donald J. Trump.Trump and his company refused to honor the contract by not paying the final payment, saying that they ve  paid enough  for the paint. In other words, Trump negotiated a deal that ended up costing him more in the end, just like the kinds of deals he wants to negotiate for America with the rest of the world.Anyway, Paint Spot owner Juan Carlos Enriquez filed suit against Trump in court, and Judge Jorge Cueto just slapped Trump and his company with a $300,000 hit to cover The Paint Shop s attorney and court fees, nearly ten times more money than the $34,863 owed. And Trump STILL hasn t paid that debt, so how are we supposed to trust him to pay down the debt of en entire nation if he can t meet his own obligations? I m happy I have a judgment,  Enriquez told the Miami Herald.  But he [Trump] hasn t paid yet. You know how he says he ll surround himself with the greatest people if he is president? In this case, he might not be surrounded by the right people. This isn t even the first fine Trump has been ordered to pay this month.On the same day as his coronation as the Republican Party nominee last week, the National Labor Relations Board slapped Trump with an $11,200 fine for treating employees like shit because they tried to join a labor union.As it turns out, one employee was wrongfully fired and the other was retaliated against by denying them the promotion they had earned.For someone who claims to be the  law and order  candidate, Trump sure does break the law a lot.Featured Image: Christopher Furlong/Getty Images', 'No Food, No FEMA: Hurricane Michael’s Survivors Are Furious https://thebea.st/2CIXNoz\\xa0', 'WASHINGTON (Reuters) - Here are some of the highlights of the Reuters interview with U.S. President Donald Trump on Thursday. “There’s a chance that we could end up having a major, major, conflict with North Korea, absolutely.” QUESTION: Is that your biggest global worry at this point? “Yes, I would say that’s true, yes. ... North Korea would be certainly that.” ON GETTING SOUTH KOREA TO PAY FOR THAAD MISSILE DEFENSE SYSTEM “On the THAAD system, it’s about a billion dollars. I said,  ‘Why are we paying? Why are we paying a billion dollars? We’re protecting. Why are we paying a billion dollars?’ So I informed South Korea it would be appropriate if they paid. Nobody’s going to do that. Why are we paying a billion dollars? It’s a billion dollar system. It’s phenomenal. It’s the most incredible equipment you’ve ever seen - shoots missiles right out of the sky. And it protects them and I want to protect them. We’re going to protect them. But they should pay for that, and they understand that.” ON WHETHER THE WAR AGAINST ISLAMIST EXTREMISM WILL EVER END “Yours is the toughest question. Because at what point does it end? But we can’t let them come over here. I have to say, there is an end. And it has to be humiliation. There is an end. Otherwise it’s really tough. But there is an end. We are really eradicating some very bad people. When you take a look at what’s going on with the cutting off of the heads. We haven’t seen that since Medieval times. Right?”  ON CHINESE PRESIDENT XI’S EFFORTS TO REIN IN NORTH KOREA “He certainly doesn’t want to see turmoil and death. He doesn’t want to see it. He’s a good man. He’s a very good man and I got to know him very well ... We’ll see how it all works out. I know he would like to be able to do something. Perhaps it’s possible that he can’t. But I think he’d like to be able to do something.” “He’s 27 years old, his father dies, took over a regime, so say what you want but that’s not easy, especially at that age. You know you have plenty of generals in there and plenty of other people that would like to do what he’s doing. So I’ve said this before and I’ve, I’m just telling you, and I’m not giving him credit or not giving him credit. I’m just saying that’s a very hard thing to do.” “As to whether or not he’s rational, I have no opinion on it. I hope he’s rational.” “I get a call from Mexico yesterday, ‘We hear you’re going to terminate NAFTA.’ I said that’s right. They said, ‘Is there any way we can do something without you – without termination?’ I said, ‘What do you want to do?’ He said, ‘Well, we’d like to negotiate.’ I said we’ll think about it. Then I get a call, and they call me, I get a call from Justin Trudeau and he said, ‘We’d like to see if we can work something out,’ and I said that’s fine. Because I’ve always - I’ve been very consistent. It’s much less disruptive if we can make a fair trade deal than if we terminate.” “It’s unacceptable. It’s a horrible deal made by Hillary. It’s a horrible deal. And we’re going to renegotiate that deal, or terminate it.”  QUESTION: When will you announce it? “Very soon. I’m announcing it now.” “By the way, with South Korea, just so you know. They’re ready for it. Mike Pence was representing me, he was just over there, he’s told them. And we have the five-year anniversary coming up very shortly. And we thought that would be a good time to start ... It’s a great deal for South Korea. It’s a terrible deal for us.” “Frankly, Saudi Arabia has not treated us fairly, because we are losing a tremendous amount of money in defending Saudi Arabia.” “Well, my problem is that I’ve established a very good personal relationship with (Chinese) President Xi. And I really feel that he is doing everything in his power to help us with a big situation, so I wouldn’t want to be causing difficulty right now for him ... So I would certainly want to speak to him first.” “If there’s closure, there’s closure. We’ll see what happens. If there’s a shutdown. It’s the Democrats’ fault. Not our fault. It’s the Democrats’ fault. Maybe they’d like to see a shutdown.”  ON TRUMP’S PLAN TO GENERATE REVENUE TO OFFSET TAX CUTS “We will do trade deals that are going to make up for a tremendous amount of the deficit. We are going to be doing trade deals that are going to be much better trade deals ...  “There will be other ways that we are going to raise revenues. But we are going to run the country properly, and we are going to be reimbursed when we do things. Why should we be paying for somebody else’s military?” ON MIDDLE EAST PEACE AND POSSIBLE TRIP TO ISRAEL, SAUDI ARABIA “It’s a possibility, we’re talking to both. It’s a possibility, but I want to see peace with Israel and the Palestinians. There is no reason there’s not peace between Israel and the Palestinians - none whatsoever. So we’re looking at that and we’re also looking at the potential of going to Saudi Arabia.” '], 'label': [1, 1, 1, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "print(ds.shape)\n",
        "\n",
        "print(ds['train'].column_names)\n",
        "print(ds['train'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "mt7Fa1Y6kpkF",
        "outputId": "52c37625-0c4b-4d94-a20c-13e10703e5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape (92394, 2) \n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.652737\n",
            "1    0.347263\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.659686\n",
            "1    0.340314\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Local Charlotte, NC news station WSOCTV is rep...      1\n",
              "1  The tsunami has started President Obama s Keny...      1\n",
              "2  The only reality show Donald Trump should have...      1\n",
              "3  No Food, No FEMA: Hurricane Michael’s Survivor...      0\n",
              "4  WASHINGTON (Reuters) - Here are some of the hi...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9bd5cffd-6f0c-426a-b028-3efc8f8218c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Local Charlotte, NC news station WSOCTV is rep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The tsunami has started President Obama s Keny...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The only reality show Donald Trump should have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No Food, No FEMA: Hurricane Michael’s Survivor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WASHINGTON (Reuters) - Here are some of the hi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bd5cffd-6f0c-426a-b028-3efc8f8218c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9bd5cffd-6f0c-426a-b028-3efc8f8218c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9bd5cffd-6f0c-426a-b028-3efc8f8218c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7c7d468-8478-4fc9-9330-8fef43bdb6eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7c7d468-8478-4fc9-9330-8fef43bdb6eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7c7d468-8478-4fc9-9330-8fef43bdb6eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_misinfo_train",
              "summary": "{\n  \"name\": \"df_misinfo_train\",\n  \"rows\": 92394,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 78199,\n        \"samples\": [\n          \"WASHINGTON (Reuters) - The U.S. Justice Department on Friday asked the Supreme Court to block a judge\\u2019s ruling that prevented President Donald Trump\\u2019s travel ban from being applied to grandparents of U.S. citizens and refugees already being processed by resettlement agencies. In a court filing, the administration asked the justices to overturn Thursday\\u2019s decision by a U.S. district judge in Hawaii, which limited the scope of the administration\\u2019s temporary ban on refugees and travelers from six Muslim-majority countries. The latest round in the fight over Trump\\u2019s March 6 executive order, which he says is needed for national security reasons, came after the Supreme Court intervened last month to partially revive the two bans, which were blocked by lower courts. The Supreme Court said then that the ban could take effect, but that people with a \\u201cbona fide relationship\\u201d to a U.S. person or entity could not be barred.  The administration had narrowly interpreted that language, saying the ban would apply to grandparents and other family members, prompting the state of Hawaii to ask Hawaii-based U.S. District Judge Derrick Watson to expand the definition of who could be admitted. He ruled for the state late on Thursday. In the court filing, the Justice Department said the judge\\u2019s ruling \\u201cempties the (Supreme) Court\\u2019s decision of meaning, as it encompasses not just \\u201cclose\\u201d family members but virtually all family members. The conservative-leaning Supreme Court is not currently in session but the justices can handle emergency requests. The administration\\u2019s application could be directed either to Justice Anthony Kennedy, who has responsibility for emergency requests from western states, or to the nine justices as a whole. If the court as a whole is asked to weigh in, five votes are needed to grant such a request. \\u201cThe truth here is that the government\\u2019s interpretation of the Supreme Court\\u2019s stay order defies common sense,\\u201d said Omar Jadwat, a lawyer with the American Civil Liberties Union involved in challenging the ban. \\u201cThat\\u2019s what the district court correctly found and the attorney general\\u2019s misleading attacks on its decision can\\u2019t change that fact.\\u201d In his decision, Watson harshly criticized the government\\u2019s definition of close family relations as \\u201cthe antithesis of common sense.\\u201d Watson also ruled that the assurance by a resettlement agency to provide basic services to a newly arrived refugee constitutes an adequate connection to the United States because it is a sufficiently formal and documented agreement that triggers responsibilities and compensation. In the court filing, the Justice Department said Watson\\u2019s ruling on refugees would make the Supreme Court\\u2019s decision on that part of the executive order \\u201ceffectively meaningless.\\u201d The ruling, if left in place, means refugees can continue to be resettled in the United States, beyond a cap of 50,000 set by the executive order. That limit was reached this week.  The Supreme Court\\u2019s decision last month revived parts of Trump\\u2019s March 6 executive order banning travelers from Iran, Libya, Somalia, Sudan, Syria and Yemen for 90 days, as well as refugees for 120 days. The court also agreed to hear oral arguments in the fall over whether the ban violates the U.S. Constitution. \",\n          \"Friends!!! As you may know, #mamatortuga home is located in #southflorida and we are going to received #hurricane #dorian \\nIn the past, we have chose to evacuate, but at this moment,  my husband and car are not in\\u2026 https://www.instagram.com/p/B1y2PK5gOaK/?igshid=e9g3ug3vhixr\\u00a0\\u2026\",\n          \"WASHINGTON (Reuters) - The U.S. Senate voted on Thursday to confirm retired neurosurgeon Ben Carson as secretary of the Department of Housing and Urban Development (HUD) in President Donald Trump\\u2019s Cabinet. Carson, who ran for the 2016 Republican presidential nomination and later endorsed Trump, becomes the only African-American in the Trump Cabinet. The Senate confirmed his appointment by a vote of 58 to 41. Carson was sworn in by Vice President Mike Pence later on Thursday. During his confirmation hearing in January, Carson, 65, told the Senate Committee on Banking, Housing, and Urban Affairs that he would monitor any potential conflicts of interest between his agency and properties controlled by Trump. He also told lawmakers he was fit to lead HUD, an agency whose mission includes helping the poor get housing, even though he has sometimes criticized its work. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ds_cloned = ds.copy()\n",
        "\n",
        "# DATA PARTITIONING =====================================================================\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UzViGPVkpkF",
        "outputId": "011910aa-2288-4337-ad6c-9a92958c3f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64170"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# balance train split -------------------------------------------------------------------\n",
        "\n",
        "balancer = RandomUnderSampler(random_state=42, sampling_strategy = 'majority')\n",
        "df_misinfo_train_balanced = pd.concat(balancer.fit_resample(X = df_misinfo_train.iloc[:,[0]],\n",
        "                                                           y = df_misinfo_train.iloc[:,[1]]),\n",
        "                                     axis=1).sample(frac = 1).reset_index(drop=True)\n",
        "df_misinfo_train_balanced.value_counts(\"label\")/df_misinfo_train_balanced.shape[0]\n",
        "df_misinfo_train_balanced.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5D-Y3ovkpkF",
        "outputId": "f3fad7f7-fc39-4fc4-fcf7-2203dcaa4515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pickle files not found. Running tokenization...\n",
            "Loading spaCy model...\n",
            "Fitting Tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Train Data in Batches...\n",
            "Tokenizing batch 1/93...\n",
            "Tokenizing batch 2/93...\n",
            "Tokenizing batch 3/93...\n",
            "Tokenizing batch 4/93...\n",
            "Tokenizing batch 5/93...\n",
            "Tokenizing batch 6/93...\n",
            "Tokenizing batch 7/93...\n",
            "Tokenizing batch 8/93...\n",
            "Tokenizing batch 9/93...\n",
            "Tokenizing batch 10/93...\n",
            "Tokenizing batch 11/93...\n",
            "Tokenizing batch 12/93...\n",
            "Tokenizing batch 13/93...\n",
            "Tokenizing batch 14/93...\n",
            "Tokenizing batch 15/93...\n",
            "Tokenizing batch 16/93...\n",
            "Tokenizing batch 17/93...\n",
            "Tokenizing batch 18/93...\n",
            "Tokenizing batch 19/93...\n",
            "Tokenizing batch 20/93...\n",
            "Tokenizing batch 21/93...\n",
            "Tokenizing batch 22/93...\n",
            "Tokenizing batch 23/93...\n",
            "Tokenizing batch 24/93...\n",
            "Tokenizing batch 25/93...\n",
            "Tokenizing batch 26/93...\n",
            "Tokenizing batch 27/93...\n",
            "Tokenizing batch 28/93...\n",
            "Tokenizing batch 29/93...\n",
            "Tokenizing batch 30/93...\n",
            "Tokenizing batch 31/93...\n",
            "Tokenizing batch 32/93...\n",
            "Tokenizing batch 33/93...\n",
            "Tokenizing batch 34/93...\n",
            "Tokenizing batch 35/93...\n",
            "Tokenizing batch 36/93...\n",
            "Tokenizing batch 37/93...\n",
            "Tokenizing batch 38/93...\n",
            "Tokenizing batch 39/93...\n",
            "Tokenizing batch 40/93...\n",
            "Tokenizing batch 41/93...\n",
            "Tokenizing batch 42/93...\n",
            "Tokenizing batch 43/93...\n",
            "Tokenizing batch 44/93...\n",
            "Tokenizing batch 45/93...\n",
            "Tokenizing batch 46/93...\n",
            "Tokenizing batch 47/93...\n",
            "Tokenizing batch 48/93...\n",
            "Tokenizing batch 49/93...\n",
            "Tokenizing batch 50/93...\n",
            "Tokenizing batch 51/93...\n",
            "Tokenizing batch 52/93...\n",
            "Tokenizing batch 53/93...\n",
            "Tokenizing batch 54/93...\n",
            "Tokenizing batch 55/93...\n",
            "Tokenizing batch 56/93...\n",
            "Tokenizing batch 57/93...\n",
            "Tokenizing batch 58/93...\n",
            "Tokenizing batch 59/93...\n",
            "Tokenizing batch 60/93...\n",
            "Tokenizing batch 61/93...\n",
            "Tokenizing batch 62/93...\n",
            "Tokenizing batch 63/93...\n",
            "Tokenizing batch 64/93...\n",
            "Tokenizing batch 65/93...\n",
            "Tokenizing batch 66/93...\n",
            "Tokenizing batch 67/93...\n",
            "Tokenizing batch 68/93...\n",
            "Tokenizing batch 69/93...\n",
            "Tokenizing batch 70/93...\n",
            "Tokenizing batch 71/93...\n",
            "Tokenizing batch 72/93...\n",
            "Tokenizing batch 73/93...\n",
            "Tokenizing batch 74/93...\n",
            "Tokenizing batch 75/93...\n",
            "Tokenizing batch 76/93...\n",
            "Tokenizing batch 77/93...\n",
            "Tokenizing batch 78/93...\n",
            "Tokenizing batch 79/93...\n",
            "Tokenizing batch 80/93...\n",
            "Tokenizing batch 81/93...\n",
            "Tokenizing batch 82/93...\n",
            "Tokenizing batch 83/93...\n",
            "Tokenizing batch 84/93...\n",
            "Tokenizing batch 85/93...\n",
            "Tokenizing batch 86/93...\n",
            "Tokenizing batch 87/93...\n",
            "Tokenizing batch 88/93...\n",
            "Tokenizing batch 89/93...\n",
            "Tokenizing batch 90/93...\n",
            "Tokenizing batch 91/93...\n",
            "Tokenizing batch 92/93...\n",
            "Tokenizing batch 93/93...\n",
            "Tokenizing Test Data in Batches...\n",
            "Tokenizing batch 1/11...\n",
            "Tokenizing batch 2/11...\n",
            "Tokenizing batch 3/11...\n",
            "Tokenizing batch 4/11...\n",
            "Tokenizing batch 5/11...\n",
            "Tokenizing batch 6/11...\n",
            "Tokenizing batch 7/11...\n",
            "Tokenizing batch 8/11...\n",
            "Tokenizing batch 9/11...\n",
            "Tokenizing batch 10/11...\n",
            "Tokenizing batch 11/11...\n"
          ]
        }
      ],
      "source": [
        "# TOKENIZATION ==========================================================================\n",
        "\n",
        "# Define a function for batch processing\n",
        "def batch_tokenize(data, tokenizer, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Tokenizes the input data in batches to prevent memory issues.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The dataset to tokenize (as a pandas Series).\n",
        "    - tokenizer: The tokenizer function.\n",
        "    - batch_size: Number of rows to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "    - A list of tokenized outputs.\n",
        "    \"\"\"\n",
        "    tokenized_batches = []\n",
        "    for start in range(0, len(data), batch_size):\n",
        "        batch = data[start:start + batch_size]\n",
        "        print(f\"Tokenizing batch {start // batch_size + 1}/{len(data) // batch_size + 1}...\")\n",
        "        tokenized_batch = batch.map(tokenizer)\n",
        "        tokenized_batches.extend(tokenized_batch)\n",
        "    return tokenized_batches\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "        tokens = custom_tokenizer(text)\n",
        "        vocab = trained_tokenizer.vocabulary_\n",
        "        return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "# Check if the pickle files already exist\n",
        "# local\n",
        "train_tokens_file = 'misinfo_train_tokens.pkl'\n",
        "test_tokens_file = 'misinfo_test_tokens.pkl'\n",
        "\n",
        "\n",
        "# for Hertie GPU:\n",
        "# train_tokens_file = '/workspace/workspace/cache/misinfo_train_tokens.pkl'\n",
        "# test_tokens_file = '/workspace/workspace/cache/misinfo_test_tokens.pkl'\n",
        "\n",
        "\n",
        "if os.path.exists(train_tokens_file) and os.path.exists(test_tokens_file):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load the pre-saved tokenized data\n",
        "    with open(train_tokens_file, 'rb') as f:\n",
        "        misinfo_train_tokens = pickle.load(f)\n",
        "\n",
        "    with open(test_tokens_file, 'rb') as f:\n",
        "        misinfo_test_tokens = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    print(\"Loading spaCy model...\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "    misinfo_tokenizer = CountVectorizer(analyzer=\"word\",\n",
        "                                        tokenizer=custom_tokenizer,\n",
        "                                        lowercase=False,\n",
        "                                        min_df=3)\n",
        "\n",
        "    print(\"Fitting Tokenizer...\")\n",
        "    misinfo_tokenizer.fit(df_misinfo_train[\"text\"])\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokens = batch_tokenize(df_misinfo_train[\"text\"], misinfo_tokenizer_analyzer)\n",
        "\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokens = batch_tokenize(df_misinfo_test[\"text\"],\n",
        "                                         lambda x: custom_analyzer(x, misinfo_tokenizer))\n",
        "\n",
        "    # Save tokenized train and test data\n",
        "    with open(train_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokens, f)\n",
        "\n",
        "    with open(test_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokens, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMnZ2N8kpkG",
        "outputId": "d37574ba-c3be-4cb8-95c8-1dcbbdce13d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab indexing\n",
            "creating data loaders\n",
            "created data loaders\n",
            "Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\n",
            "Saved embeddings to mapped_pretrained_embeddings.pkl. Shape: torch.Size([217732, 300])\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "print (\"vocab indexing\")\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokens)\n",
        "\n",
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "print(\"creating data loaders\")\n",
        "\n",
        "def collate_fn(data):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in data:\n",
        "        # integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "    label_list = torch.tensor(label_list)\n",
        "    # padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokens,\n",
        "                                         df_misinfo_train[\"label\"])), # was meant to be balanced\n",
        "                        batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokens,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"created data loaders\")\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "# Define the file path for the pickle file\n",
        "# for local:\n",
        "pickle_file_path = \"mapped_pretrained_embeddings.pkl\"\n",
        "# for Hertie GPU:\n",
        "#pickle_file_path = \"/workspace/workspace/mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "# Check if the pickle file already exists\n",
        "if os.path.exists(pickle_file_path):\n",
        "    # If the file exists, load it from the pickle file\n",
        "    with open(pickle_file_path, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {pickle_file_path}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    # If the file does not exist, proceed with creating the embeddings and save them\n",
        "    # Load pre-trained FastText model\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    # Map pretrained FastText embeddings to vocabulary indices\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "\n",
        "    # Convert mapped embeddings to a tensor\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save the embeddings to a pickle file\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {pickle_file_path}. Shape: {embedding_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Qu0_t5xTkpkG"
      },
      "outputs": [],
      "source": [
        "# STEP 2: LOSS FUNCTION AND OPTIMIZER SPECIFICATION =====================================\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # moved this within train function\n",
        "\n",
        "# STEP 3: MODEL TRAINING AND EVALUATION =================================================\n",
        "\n",
        "def train(model, num_epochs, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Ensure the optimizer uses the correct device (it should automatically use the same device as the model)\n",
        "    optimizer = torch.optim.Adam(model.parameters())  # Assuming Adam optimizer here\n",
        "\n",
        "    loss_hist_train = [0] * num_epochs\n",
        "    accuracy_hist_train = [0] * num_epochs\n",
        "    f1_hist_train = [0] * num_epochs\n",
        "    loss_hist_test = [0] * num_epochs\n",
        "    accuracy_hist_test = [0] * num_epochs\n",
        "    f1_hist_test = [0] * num_epochs\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # set training mode\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_dl):\n",
        "            # Move data to device (GPU or CPU)\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            # pred = model(x_batch)[:, 0]  # FOR CNN & RNN\n",
        "            loss = loss_fn(pred, y_batch.float())  # compute loss\n",
        "            lengths = torch.sum(x_batch != 0, dim=1)\n",
        "            pred = model(x_batch, lengths)[:, 0] # FOR LTSM\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()  # compute gradients\n",
        "            optimizer.step()  # update parameters\n",
        "            optimizer.zero_grad()  # reset gradients\n",
        "\n",
        "            # evaluate train\n",
        "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
        "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "            accuracy_hist_train[epoch] += is_correct.sum()\n",
        "            all_train_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            # Print batch progress\n",
        "            if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(train_dl):\n",
        "                print(f\"    Batch {batch_idx + 1}/{len(train_dl)}: \"\n",
        "                      f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        f1_hist_train[epoch] = f1_score(all_train_labels, all_train_preds)\n",
        "\n",
        "        # evaluate model\n",
        "        model.eval()  # set evaluation mode\n",
        "        all_test_preds = []\n",
        "        all_test_labels = []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x_batch, y_batch) in enumerate(test_dl):\n",
        "                # Move data to device (GPU or CPU)\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                pred = model(x_batch)[:, 0]\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # evaluate test\n",
        "                loss_hist_test[epoch] += loss.item() * y_batch.size(0)\n",
        "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "                accuracy_hist_test[epoch] += is_correct.sum()\n",
        "                all_test_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "                # Print batch progress\n",
        "                if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(test_dl):\n",
        "                    print(f\"    Batch {batch_idx + 1}/{len(test_dl)}: \"\n",
        "                          f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        accuracy_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        f1_hist_test[epoch] = f1_score(all_test_labels, all_test_preds)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Accuracy: {accuracy_hist_train[epoch]:.3f}, F1: {f1_hist_train[epoch]:.3f}\")\n",
        "        print(f\"    Test  - Accuracy: {accuracy_hist_test[epoch]:.3f}, F1: {f1_hist_test[epoch]:.3f}\")\n",
        "\n",
        "    return [loss_hist_train, loss_hist_test, accuracy_hist_train,\n",
        "            accuracy_hist_test, f1_hist_train, f1_hist_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8qQ1LhWHrXP-",
        "outputId": "fc0b6758-11ea-4661-bef9-29f74e8dd679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.2583\n",
            "    Batch 2000/2888: Loss: 0.2183\n",
            "    Batch 2888/2888: Loss: 0.1334\n",
            "Epoch 1/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.2034\n",
            "Epoch 1/10 Summary:\n",
            "    Train - Accuracy: 0.848, F1: 0.725\n",
            "    Test  - Accuracy: 0.878, F1: 0.791\n",
            "Epoch 2/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.3400\n",
            "    Batch 2000/2888: Loss: 0.5061\n",
            "    Batch 2888/2888: Loss: 0.2426\n",
            "Epoch 2/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.1428\n",
            "Epoch 2/10 Summary:\n",
            "    Train - Accuracy: 0.887, F1: 0.812\n",
            "    Test  - Accuracy: 0.891, F1: 0.814\n",
            "Epoch 3/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1859\n",
            "    Batch 2000/2888: Loss: 0.2405\n",
            "    Batch 2888/2888: Loss: 0.1362\n",
            "Epoch 3/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.1026\n",
            "Epoch 3/10 Summary:\n",
            "    Train - Accuracy: 0.905, F1: 0.849\n",
            "    Test  - Accuracy: 0.917, F1: 0.868\n",
            "Epoch 4/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0946\n",
            "    Batch 2000/2888: Loss: 0.2065\n",
            "    Batch 2888/2888: Loss: 0.0402\n",
            "Epoch 4/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0957\n",
            "Epoch 4/10 Summary:\n",
            "    Train - Accuracy: 0.919, F1: 0.875\n",
            "    Test  - Accuracy: 0.925, F1: 0.883\n",
            "Epoch 5/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1444\n",
            "    Batch 2000/2888: Loss: 0.2417\n",
            "    Batch 2888/2888: Loss: 0.1218\n",
            "Epoch 5/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0797\n",
            "Epoch 5/10 Summary:\n",
            "    Train - Accuracy: 0.930, F1: 0.893\n",
            "    Test  - Accuracy: 0.933, F1: 0.896\n",
            "Epoch 6/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1883\n",
            "    Batch 2000/2888: Loss: 0.0694\n",
            "    Batch 2888/2888: Loss: 0.2322\n",
            "Epoch 6/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0700\n",
            "Epoch 6/10 Summary:\n",
            "    Train - Accuracy: 0.937, F1: 0.906\n",
            "    Test  - Accuracy: 0.940, F1: 0.907\n",
            "Epoch 7/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.2929\n",
            "    Batch 2000/2888: Loss: 0.2158\n",
            "    Batch 2888/2888: Loss: 0.1186\n",
            "Epoch 7/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0645\n",
            "Epoch 7/10 Summary:\n",
            "    Train - Accuracy: 0.945, F1: 0.918\n",
            "    Test  - Accuracy: 0.948, F1: 0.921\n",
            "Epoch 8/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0876\n",
            "    Batch 2000/2888: Loss: 0.1055\n",
            "    Batch 2888/2888: Loss: 0.0132\n",
            "Epoch 8/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0598\n",
            "Epoch 8/10 Summary:\n",
            "    Train - Accuracy: 0.954, F1: 0.932\n",
            "    Test  - Accuracy: 0.954, F1: 0.930\n",
            "Epoch 9/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0399\n",
            "    Batch 2000/2888: Loss: 0.2745\n",
            "    Batch 2888/2888: Loss: 0.6827\n",
            "Epoch 9/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0569\n",
            "Epoch 9/10 Summary:\n",
            "    Train - Accuracy: 0.957, F1: 0.937\n",
            "    Test  - Accuracy: 0.954, F1: 0.930\n",
            "Epoch 10/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0863\n",
            "    Batch 2000/2888: Loss: 0.1073\n",
            "    Batch 2888/2888: Loss: 0.2026\n",
            "Epoch 10/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0543\n",
            "Epoch 10/10 Summary:\n",
            "    Train - Accuracy: 0.959, F1: 0.941\n",
            "    Test  - Accuracy: 0.955, F1: 0.932\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: MODEL BUILDING ================================================================\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = TextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "hist_cnn = train(model, num_epochs, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "o_5dUjnvkpkG",
        "outputId": "9cf1c408-2493-43f9-8d6f-9e5892559419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1073\n",
            "    Batch 2000/2888: Loss: 0.0263\n",
            "    Batch 2888/2888: Loss: 0.1515\n",
            "Epoch 1/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0536\n",
            "Epoch 1/10 Summary:\n",
            "    Train - Accuracy: 0.961, F1: 0.943\n",
            "    Test  - Accuracy: 0.959, F1: 0.938\n",
            "Epoch 2/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1465\n",
            "    Batch 2000/2888: Loss: 0.1635\n",
            "    Batch 2888/2888: Loss: 0.0102\n",
            "Epoch 2/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0463\n",
            "Epoch 2/10 Summary:\n",
            "    Train - Accuracy: 0.962, F1: 0.945\n",
            "    Test  - Accuracy: 0.960, F1: 0.941\n",
            "Epoch 3/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1117\n",
            "    Batch 2000/2888: Loss: 0.1026\n",
            "    Batch 2888/2888: Loss: 0.0168\n",
            "Epoch 3/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0470\n",
            "Epoch 3/10 Summary:\n",
            "    Train - Accuracy: 0.964, F1: 0.948\n",
            "    Test  - Accuracy: 0.962, F1: 0.944\n",
            "Epoch 4/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0475\n",
            "    Batch 2000/2888: Loss: 0.1583\n",
            "    Batch 2888/2888: Loss: 0.1525\n",
            "Epoch 4/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0461\n",
            "Epoch 4/10 Summary:\n",
            "    Train - Accuracy: 0.965, F1: 0.949\n",
            "    Test  - Accuracy: 0.959, F1: 0.939\n",
            "Epoch 5/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0972\n",
            "    Batch 2000/2888: Loss: 0.0691\n",
            "    Batch 2888/2888: Loss: 0.0105\n",
            "Epoch 5/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0382\n",
            "Epoch 5/10 Summary:\n",
            "    Train - Accuracy: 0.966, F1: 0.951\n",
            "    Test  - Accuracy: 0.963, F1: 0.946\n",
            "Epoch 6/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.2503\n",
            "    Batch 2000/2888: Loss: 0.0633\n",
            "    Batch 2888/2888: Loss: 0.1056\n",
            "Epoch 6/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0387\n",
            "Epoch 6/10 Summary:\n",
            "    Train - Accuracy: 0.967, F1: 0.953\n",
            "    Test  - Accuracy: 0.964, F1: 0.947\n",
            "Epoch 7/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0537\n",
            "    Batch 2000/2888: Loss: 0.0093\n",
            "    Batch 2888/2888: Loss: 0.1026\n",
            "Epoch 7/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0368\n",
            "Epoch 7/10 Summary:\n",
            "    Train - Accuracy: 0.968, F1: 0.954\n",
            "    Test  - Accuracy: 0.964, F1: 0.947\n",
            "Epoch 8/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0182\n",
            "    Batch 2000/2888: Loss: 0.0748\n",
            "    Batch 2888/2888: Loss: 0.2096\n",
            "Epoch 8/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0338\n",
            "Epoch 8/10 Summary:\n",
            "    Train - Accuracy: 0.969, F1: 0.955\n",
            "    Test  - Accuracy: 0.966, F1: 0.951\n",
            "Epoch 9/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.0195\n",
            "    Batch 2000/2888: Loss: 0.0609\n",
            "    Batch 2888/2888: Loss: 0.0422\n",
            "Epoch 9/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0340\n",
            "Epoch 9/10 Summary:\n",
            "    Train - Accuracy: 0.970, F1: 0.956\n",
            "    Test  - Accuracy: 0.963, F1: 0.944\n",
            "Epoch 10/10 Training...\n",
            "    Batch 1000/2888: Loss: 0.1111\n",
            "    Batch 2000/2888: Loss: 0.0190\n",
            "    Batch 2888/2888: Loss: 0.0048\n",
            "Epoch 10/10 Evaluating...\n",
            "    Batch 321/321: Loss: 0.0308\n",
            "Epoch 10/10 Summary:\n",
            "    Train - Accuracy: 0.970, F1: 0.957\n",
            "    Test  - Accuracy: 0.967, F1: 0.951\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "\n",
        "hist_rnn = train(model, num_epochs, train_dl, test_dl) # fluctuating f1 scores, exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HK1Kjy-CkpkG",
        "outputId": "7b39eb33-b8b6-4f08-fa39-6b41f76df1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'pred' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2d766464c61e>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# better but not great\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-b5d086238afa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# pred = model(x_batch)[:, 0]  # FOR CNN & RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# FOR LTSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pred' referenced before assignment"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl) # better but not great"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "r1aBkTs_kpkH",
        "outputId": "3e585584-5b5e-40ac-9f55-2d7d3df335d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "StackedLSTMTextClassificationModel.forward() missing 1 required positional argument: 'lengths'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-fbfd256c544e>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-222f842bb94e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: StackedLSTMTextClassificationModel.forward() missing 1 required positional argument: 'lengths'"
          ]
        }
      ],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GoGdIxt7kpkJ",
        "outputId": "c8fedc40-5a25-4b11-82b1-487ab89a326f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2505d723a3a2>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Make predictions in batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Evaluate the performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2505d723a3a2>\u001b[0m in \u001b[0;36mbatch_predict\u001b[0;34m(model, tokenizer, texts, batch_size, max_length)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load checkpoint and tokenizer\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Helper function to process data in batches\n",
        "def batch_predict(model, tokenizer, texts, batch_size=16, max_length=512):\n",
        "    all_preds = []\n",
        "    # Check if GPU is available and move model to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, len(texts), batch_size):\n",
        "            end = min(start + batch_size, len(texts))\n",
        "            batch_texts = texts[start:end]\n",
        "\n",
        "            # Tokenize the batch of texts\n",
        "            tokenized_batch = tokenizer(batch_texts, truncation=True, padding=\"max_length\",\n",
        "                                        max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "            # Move tensors to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                tokenized_batch = {key: value.cuda() for key, value in tokenized_batch.items()}\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(**tokenized_batch)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_labels = torch.argmax(predictions, dim=1)\n",
        "            all_preds.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "# Prepare your dataset\n",
        "texts = df_misinfo_test[\"text\"].to_list()\n",
        "true_labels = df_misinfo_test[\"label\"].to_list()\n",
        "\n",
        "# Make predictions in batches\n",
        "predicted_labels = batch_predict(model, tokenizer, texts, batch_size=16)\n",
        "\n",
        "# Evaluate the performance\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f1)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "ZIO7Uv3xZIJg",
        "outputId": "6ece97d6-d687-4d19-dbc0-fb4137de4e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'f1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9c5e1fd547f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5eMBCBpkpkJ"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IHtNWdu2kpkJ",
        "outputId": "4bb3095e-fa0a-48d1-85f5-b661abe48db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5150f6b47b5a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert train and test data to hugging face Dataset -----------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m features = Features({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mClassLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'not_housing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'housing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m })\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Features' is not defined"
          ]
        }
      ],
      "source": [
        "# convert train and test data to hugging face Dataset -----------------------------------\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['not_housing', 'housing']),\n",
        "})\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# create a hugging face DatasetDict -----------------------------------------------------\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "print(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./results\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=True,\n",
        "                                  use_cpu=True)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# predict -------------------------------------------------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_texts)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "predicted_labels = torch.argmax(predictions, dim=1)\n",
        "true_labels = torch.tensor(df_misinfo_test[\"label\"].to_list())\n",
        "f1 = f1_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "acc = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "f1\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEToEHskpkK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL7RAEQYkpkK"
      },
      "outputs": [],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
        "\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}