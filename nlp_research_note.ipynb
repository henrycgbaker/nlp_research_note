{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQscgt_pkpj7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrycgbaker/nlp_research_note/blob/main/nlp_research_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVV99Amdkpj-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext.util as fasttext_util\n",
        "import fasttext\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import tqdm\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# for GPU\n",
        "\"\"\"\n",
        "os.environ['PIP_CACHE_DIR'] = '/workspace/workspace/cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/workspace/workspace/cache'\n",
        "os.environ['TORCH_CACHE'] = '/workspace/workspace/cache'\n",
        "os.environ['HF_CACHE'] = '/workspace/workspace/cache'\n",
        "\"\"\"\n",
        "\n",
        "# for Google Colab\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t9ush0qlkpkB",
        "outputId": "01adaf69-20de-43d1-877c-7f69771fc7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# specify custom functions --------------------------------------------------------------\n",
        "def custom_tokenizer(text):\n",
        "    tokenized_text = nlp(text)\n",
        "    return [tok.text for tok in tokenized_text]\n",
        "\n",
        "def embedding_mapping_fasttext(vocabulary, pre_trained_embeddings):\n",
        "    vocab_size = len(vocabulary)\n",
        "    embedding_dim = pre_trained_embeddings.get_dimension()\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for idx, word in enumerate(vocabulary):\n",
        "        embedding_matrix[idx] = pre_trained_embeddings.get_word_vector(word)\n",
        "    return embedding_matrix\n",
        "\n",
        "# download pretrained embeddings --------------------------------------------------------\n",
        "# for local\n",
        "#fasttext_util.download_model('en', if_exists='ignore')\n",
        "\n",
        "# for Gdrive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/cc.en.300.bin\"\n",
        "ft = fasttext.load_model(model_path)\n",
        "\n",
        "\n",
        "# download spacy model for tokenization -------------------------------------------------\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVrRXb9OkpkC"
      },
      "source": [
        "# Import & process `rumours` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ebXrht6ykpkC",
        "outputId": "2c940dac-d71f-411e-a757-75813dbd8eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nos.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\\nprint(\"Working Directory:\", os.getcwd())\\n\\npath_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\\nprint(\"Path to rumour dataset files:\", path_rumour)\\n\\npath_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\\nprint(\"Path to dataset files:\", path_climate)\\n\\n# List all files in the directory\\nfiles_rumour = os.listdir(path_rumour)\\nfiles_climate = os.listdir(path_climate)\\n\\n# Print the list of files\\nprint(\"Files in rumour dataset directory:\", files_rumour)\\nprint(\"Files in climate dataset directory:\", files_climate)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "os.chdir(\"/Users/henrybaker/Documents/repositories/nlp/nlp_research_note\")\n",
        "print(\"Working Directory:\", os.getcwd())\n",
        "\n",
        "path_rumour = kagglehub.dataset_download(\"syntheticprogrammer/rumor-detection-acl-2017\")\n",
        "print(\"Path to rumour dataset files:\", path_rumour)\n",
        "\n",
        "path_climate = kagglehub.dataset_download(\"die9origephit/climate-change-tweets\")\n",
        "print(\"Path to dataset files:\", path_climate)\n",
        "\n",
        "# List all files in the directory\n",
        "files_rumour = os.listdir(path_rumour)\n",
        "files_climate = os.listdir(path_climate)\n",
        "\n",
        "# Print the list of files\n",
        "print(\"Files in rumour dataset directory:\", files_rumour)\n",
        "print(\"Files in climate dataset directory:\", files_climate)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9V-VBbTdkpkC",
        "outputId": "05cf0752-eb06-4e88-8e59-32d121bd990a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\ncombined_data = []\\n\\nfor numb in [15, 16]:\\n    path_numb = files_rumour[numb - 15]  # \\'twitter15\\' or \\'twitter16\\' folder path\\n    label_file_path_numb = os.path.join(path_rumour, path_numb, \\'label.txt\\')\\n    tweets_file_path_numb = os.path.join(path_rumour, path_numb, \\'source_tweets.txt\\')\\n\\n    # Read label.txt\\n    label_dict = {}\\n    with open(label_file_path_numb, \\'r\\') as file:\\n        for line in file:\\n            label, tweet_id = line.strip().split(\\':\\')\\n            label_dict[tweet_id] = label\\n\\n    # Read source_tweets.txt\\n    tweets_dict = {}\\n    with open(tweets_file_path_numb, \\'r\\') as file:\\n        for line in file:\\n            tweet_id, tweet_content = line.strip().split(\\'\\t\\', 1)\\n            tweets_dict[tweet_id] = tweet_content\\n\\n    # Combine labels with tweets\\n    for tweet_id, tweet_content in tweets_dict.items():\\n        if tweet_id in label_dict:\\n            combined_data.append((label_dict[tweet_id], tweet_content))\\n\\n    print(f\"twitter_{numb}:\") \\n\\n    for label, tweet in combined_data[:5]:\\n        print(f\"    Label: {label}, Tweet: {tweet}\")\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "combined_data = []\n",
        "\n",
        "for numb in [15, 16]:\n",
        "    path_numb = files_rumour[numb - 15]  # 'twitter15' or 'twitter16' folder path\n",
        "    label_file_path_numb = os.path.join(path_rumour, path_numb, 'label.txt')\n",
        "    tweets_file_path_numb = os.path.join(path_rumour, path_numb, 'source_tweets.txt')\n",
        "\n",
        "    # Read label.txt\n",
        "    label_dict = {}\n",
        "    with open(label_file_path_numb, 'r') as file:\n",
        "        for line in file:\n",
        "            label, tweet_id = line.strip().split(':')\n",
        "            label_dict[tweet_id] = label\n",
        "\n",
        "    # Read source_tweets.txt\n",
        "    tweets_dict = {}\n",
        "    with open(tweets_file_path_numb, 'r') as file:\n",
        "        for line in file:\n",
        "            tweet_id, tweet_content = line.strip().split('\\t', 1)\n",
        "            tweets_dict[tweet_id] = tweet_content\n",
        "\n",
        "    # Combine labels with tweets\n",
        "    for tweet_id, tweet_content in tweets_dict.items():\n",
        "        if tweet_id in label_dict:\n",
        "            combined_data.append((label_dict[tweet_id], tweet_content))\n",
        "\n",
        "    print(f\"twitter_{numb}:\")\n",
        "\n",
        "    for label, tweet in combined_data[:5]:\n",
        "        print(f\"    Label: {label}, Tweet: {tweet}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rkaJwNkpkD"
      },
      "source": [
        "crop URL part\n",
        "chosen specifically because it is twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q8zTf_BZkpkD",
        "outputId": "f0623845-db42-4f01-c4ec-3d03374c5c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\nprint(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\\n\\nunique_labels = set(label for label, tweet in combined_data)\\n\\nprint(\"Unique labels in combined data:\")\\nfor label in unique_labels:\\n    print(f\"   \", label)\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(f\"Total number of entries in combined data: {len(combined_data)} \\n\")\n",
        "\n",
        "unique_labels = set(label for label, tweet in combined_data)\n",
        "\n",
        "print(\"Unique labels in combined data:\")\n",
        "for label in unique_labels:\n",
        "    print(f\"   \", label)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F2rdYR43kpkE",
        "outputId": "f34efd78-26d4-46fc-d423-f21fe1a683aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\ndf_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\\n\\nprint(df_combined.shape)\\ndf_combined.head()\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "df_combined = pd.DataFrame(combined_data, columns=[\"Label\", \"Tweet\"])\n",
        "\n",
        "print(df_combined.shape)\n",
        "df_combined.head()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d6vzv6U-kpkE",
        "outputId": "58565887-b234-4c5e-d85c-be9252d2c079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\ngrouped = df_combined.groupby(\"Label\")\\n\\n# 5 random examples for each label\\nfor label, group in grouped:\\n    print(f\"\\nLabel: {label}\")\\n    sample = group.sample(n=5, random_state=42)  \\n    for _, row in sample.iterrows():\\n        print(f\"   Tweet: {row[\\'Tweet\\']}\")\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "grouped = df_combined.groupby(\"Label\")\n",
        "\n",
        "# 5 random examples for each label\n",
        "for label, group in grouped:\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    sample = group.sample(n=5, random_state=42)\n",
        "    for _, row in sample.iterrows():\n",
        "        print(f\"   Tweet: {row['Tweet']}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheMDmROkpkE"
      },
      "source": [
        "---\n",
        "# Import & process Hugging Face `misinfo` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ckd7tkDtkpkE",
        "outputId": "0ad74e62-098a-4718-de1a-90796d900d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~/.cache/huggingface/datasets\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "hf_cache_dir = os.getenv(\"HF_DATASETS_CACHE\", \"~/.cache/huggingface/datasets\")\n",
        "print(hf_cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oYz9mC3kpkF"
      },
      "source": [
        "need to balance the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kpTGTCzvkpkF",
        "outputId": "0ccb3b7e-64f0-4c41-d914-a487dc455410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': (92394, 4), 'test': (10267, 4)}\n",
            "['Unnamed: 0.1', 'Unnamed: 0', 'text', 'label']\n",
            "{'Unnamed: 0.1': [34366, 41656, 26726, 81585, 4016], 'Unnamed: 0': [34366, 41656, 26726, 81585, 4016], 'text': [\"Local Charlotte, NC news station WSOCTV is reporting that sources tell them dash cameras captured Keith Scott getting out of car and coming towards officers with a gun in his hand:#BREAKING: Sources tell Channel 9 dash camera video shows #KeithScott getting out car, coming toward officers with gun in his hand pic.twitter.com/GGuM2Ow3wk  WSOCTV (@wsoctv) September 21, 2016For a second night, protests over a deadly officer-involved shooting in Charlotte, North Carolina, turned violent, with police firing tear gas and demonstrators throwing objects and trying to damage vehicles.Keith Lamont Scott, a father of seven, was killed by police in an apartment complex parking lot Tuesday as officers looked for another man named in a warrant they were trying to serve. The shooting set off a long night of violent protests and Wednesday the demonstrations continued for a second night, starting off as a peaceful march through downtown Charlotte. But when the demonstrators neared an Omni Hotel, some people climbed the roof of an outdoor mall and started throwing objects at the crowd.Other people were banging on the doors of the hotel, and police in riot gear emerged in the entrance.His family said Scott, an African-American, was unarmed and sitting in his car reading a book, waiting for his son to come home from school.But Charlotte-Mecklenburg Police Chief Kerr Putney said Scott exited his car with a gun, not a book. He said officers couldn t find a book at the scene. It s time for the voiceless majority to stand up and be heard,  said the police chief, who is black. It s time to change the narrative because I can tell you from the facts that the story s a little bit different as to how it s been portrayed so far, especially through social media.   CNNBut b b but what about the book?Neighbor who says she saw police shoot #KeithScott disputes CMDP's claim that he was armed with a gun. She says he had a book. @wsoctv pic.twitter.com/X1PWw5QJ7O  Mark Barber (@MBarberWSOC9) September 21, 2016And of course, there s Hillary s irresponsible tweet confirming that the cops were in the wrong, without even having any facts to back up her ignorant statement:Keith Lamont Scott. Terence Crutcher. Too many others. This has got to end. -H  Hillary Clinton (@HillaryClinton) September 21, 2016 \", 'The tsunami has started President Obama s Kenyan half-brother wants to make America great again   so he s voting for Donald Trump. I like Donald Trump because he speaks from the heart,  Malik Obama told The Post from his home in the rural village of Kogelo.  Make America Great Again is a great slogan. I would like to meet him. Obama, 58, a longtime Democrat, said his  deep disappointment  in his brother Barack s administration has led him to recently switch allegiance to  the party of Lincoln. The last straw, he said, came earlier this month when FBI Director James Comey recommended not prosecuting Democratic presidential candidate Hillary Clinton over her use of a private e-mail servers while secretary of state. She should have known better as the custodian of classified information,  said Obama.He s also annoyed that Clinton and President Obama killed Libyan leader Moammar Khadafy, whom he called one of his best friends.Malik Obama dedicated his 2012 biography of his late father to Khadafy and others who were  making this world a better place. I still feel that getting rid of Khadafy didn t make things any better in Libya,  he said.  My brother and the secretary of state disappointed me in that regard. But what bothers him even more is the Democratic Party s support of same-sex marriage.Obama plans to trek back to the US to vote for Trump in November. Obama used to live in Maryland, where he worked for many years as an accountant and is registered to vote there, public records show. Mr. Trump is providing something new and something fresh,  he said.For entire story: NYP', 'The only reality show Donald Trump should have ever been featured in is The Biggest Loser because he just got his ass handed to him in court.Two years ago, Trump National Doral Miami golf resort signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course.Well, guess who tried to stiff The Paint Spot of the final $34,863 payment in the deal?Yeah, that would be Republican nominee Donald J. Trump.Trump and his company refused to honor the contract by not paying the final payment, saying that they ve  paid enough  for the paint. In other words, Trump negotiated a deal that ended up costing him more in the end, just like the kinds of deals he wants to negotiate for America with the rest of the world.Anyway, Paint Spot owner Juan Carlos Enriquez filed suit against Trump in court, and Judge Jorge Cueto just slapped Trump and his company with a $300,000 hit to cover The Paint Shop s attorney and court fees, nearly ten times more money than the $34,863 owed. And Trump STILL hasn t paid that debt, so how are we supposed to trust him to pay down the debt of en entire nation if he can t meet his own obligations? I m happy I have a judgment,  Enriquez told the Miami Herald.  But he [Trump] hasn t paid yet. You know how he says he ll surround himself with the greatest people if he is president? In this case, he might not be surrounded by the right people. This isn t even the first fine Trump has been ordered to pay this month.On the same day as his coronation as the Republican Party nominee last week, the National Labor Relations Board slapped Trump with an $11,200 fine for treating employees like shit because they tried to join a labor union.As it turns out, one employee was wrongfully fired and the other was retaliated against by denying them the promotion they had earned.For someone who claims to be the  law and order  candidate, Trump sure does break the law a lot.Featured Image: Christopher Furlong/Getty Images', 'No Food, No FEMA: Hurricane Michael’s Survivors Are Furious https://thebea.st/2CIXNoz\\xa0', 'WASHINGTON (Reuters) - Here are some of the highlights of the Reuters interview with U.S. President Donald Trump on Thursday. “There’s a chance that we could end up having a major, major, conflict with North Korea, absolutely.” QUESTION: Is that your biggest global worry at this point? “Yes, I would say that’s true, yes. ... North Korea would be certainly that.” ON GETTING SOUTH KOREA TO PAY FOR THAAD MISSILE DEFENSE SYSTEM “On the THAAD system, it’s about a billion dollars. I said,  ‘Why are we paying? Why are we paying a billion dollars? We’re protecting. Why are we paying a billion dollars?’ So I informed South Korea it would be appropriate if they paid. Nobody’s going to do that. Why are we paying a billion dollars? It’s a billion dollar system. It’s phenomenal. It’s the most incredible equipment you’ve ever seen - shoots missiles right out of the sky. And it protects them and I want to protect them. We’re going to protect them. But they should pay for that, and they understand that.” ON WHETHER THE WAR AGAINST ISLAMIST EXTREMISM WILL EVER END “Yours is the toughest question. Because at what point does it end? But we can’t let them come over here. I have to say, there is an end. And it has to be humiliation. There is an end. Otherwise it’s really tough. But there is an end. We are really eradicating some very bad people. When you take a look at what’s going on with the cutting off of the heads. We haven’t seen that since Medieval times. Right?”  ON CHINESE PRESIDENT XI’S EFFORTS TO REIN IN NORTH KOREA “He certainly doesn’t want to see turmoil and death. He doesn’t want to see it. He’s a good man. He’s a very good man and I got to know him very well ... We’ll see how it all works out. I know he would like to be able to do something. Perhaps it’s possible that he can’t. But I think he’d like to be able to do something.” “He’s 27 years old, his father dies, took over a regime, so say what you want but that’s not easy, especially at that age. You know you have plenty of generals in there and plenty of other people that would like to do what he’s doing. So I’ve said this before and I’ve, I’m just telling you, and I’m not giving him credit or not giving him credit. I’m just saying that’s a very hard thing to do.” “As to whether or not he’s rational, I have no opinion on it. I hope he’s rational.” “I get a call from Mexico yesterday, ‘We hear you’re going to terminate NAFTA.’ I said that’s right. They said, ‘Is there any way we can do something without you – without termination?’ I said, ‘What do you want to do?’ He said, ‘Well, we’d like to negotiate.’ I said we’ll think about it. Then I get a call, and they call me, I get a call from Justin Trudeau and he said, ‘We’d like to see if we can work something out,’ and I said that’s fine. Because I’ve always - I’ve been very consistent. It’s much less disruptive if we can make a fair trade deal than if we terminate.” “It’s unacceptable. It’s a horrible deal made by Hillary. It’s a horrible deal. And we’re going to renegotiate that deal, or terminate it.”  QUESTION: When will you announce it? “Very soon. I’m announcing it now.” “By the way, with South Korea, just so you know. They’re ready for it. Mike Pence was representing me, he was just over there, he’s told them. And we have the five-year anniversary coming up very shortly. And we thought that would be a good time to start ... It’s a great deal for South Korea. It’s a terrible deal for us.” “Frankly, Saudi Arabia has not treated us fairly, because we are losing a tremendous amount of money in defending Saudi Arabia.” “Well, my problem is that I’ve established a very good personal relationship with (Chinese) President Xi. And I really feel that he is doing everything in his power to help us with a big situation, so I wouldn’t want to be causing difficulty right now for him ... So I would certainly want to speak to him first.” “If there’s closure, there’s closure. We’ll see what happens. If there’s a shutdown. It’s the Democrats’ fault. Not our fault. It’s the Democrats’ fault. Maybe they’d like to see a shutdown.”  ON TRUMP’S PLAN TO GENERATE REVENUE TO OFFSET TAX CUTS “We will do trade deals that are going to make up for a tremendous amount of the deficit. We are going to be doing trade deals that are going to be much better trade deals ...  “There will be other ways that we are going to raise revenues. But we are going to run the country properly, and we are going to be reimbursed when we do things. Why should we be paying for somebody else’s military?” ON MIDDLE EAST PEACE AND POSSIBLE TRIP TO ISRAEL, SAUDI ARABIA “It’s a possibility, we’re talking to both. It’s a possibility, but I want to see peace with Israel and the Palestinians. There is no reason there’s not peace between Israel and the Palestinians - none whatsoever. So we’re looking at that and we’re also looking at the potential of going to Saudi Arabia.” '], 'label': [1, 1, 1, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "print(ds.shape)\n",
        "\n",
        "print(ds['train'].column_names)\n",
        "print(ds['train'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mt7Fa1Y6kpkF",
        "outputId": "a4a7d11a-d47b-41a4-e454-c576ef6ab1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape (92394, 2) \n",
            "\n",
            "Training positive vs negative examples: \n",
            " label\n",
            "0    0.652737\n",
            "1    0.347263\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Testing positive vs negative examples: \n",
            " label\n",
            "0    0.659686\n",
            "1    0.340314\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_misinfo_train\",\n  \"rows\": 92394,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 78199,\n        \"samples\": [\n          \"WASHINGTON (Reuters) - The U.S. Justice Department on Friday asked the Supreme Court to block a judge\\u2019s ruling that prevented President Donald Trump\\u2019s travel ban from being applied to grandparents of U.S. citizens and refugees already being processed by resettlement agencies. In a court filing, the administration asked the justices to overturn Thursday\\u2019s decision by a U.S. district judge in Hawaii, which limited the scope of the administration\\u2019s temporary ban on refugees and travelers from six Muslim-majority countries. The latest round in the fight over Trump\\u2019s March 6 executive order, which he says is needed for national security reasons, came after the Supreme Court intervened last month to partially revive the two bans, which were blocked by lower courts. The Supreme Court said then that the ban could take effect, but that people with a \\u201cbona fide relationship\\u201d to a U.S. person or entity could not be barred.  The administration had narrowly interpreted that language, saying the ban would apply to grandparents and other family members, prompting the state of Hawaii to ask Hawaii-based U.S. District Judge Derrick Watson to expand the definition of who could be admitted. He ruled for the state late on Thursday. In the court filing, the Justice Department said the judge\\u2019s ruling \\u201cempties the (Supreme) Court\\u2019s decision of meaning, as it encompasses not just \\u201cclose\\u201d family members but virtually all family members. The conservative-leaning Supreme Court is not currently in session but the justices can handle emergency requests. The administration\\u2019s application could be directed either to Justice Anthony Kennedy, who has responsibility for emergency requests from western states, or to the nine justices as a whole. If the court as a whole is asked to weigh in, five votes are needed to grant such a request. \\u201cThe truth here is that the government\\u2019s interpretation of the Supreme Court\\u2019s stay order defies common sense,\\u201d said Omar Jadwat, a lawyer with the American Civil Liberties Union involved in challenging the ban. \\u201cThat\\u2019s what the district court correctly found and the attorney general\\u2019s misleading attacks on its decision can\\u2019t change that fact.\\u201d In his decision, Watson harshly criticized the government\\u2019s definition of close family relations as \\u201cthe antithesis of common sense.\\u201d Watson also ruled that the assurance by a resettlement agency to provide basic services to a newly arrived refugee constitutes an adequate connection to the United States because it is a sufficiently formal and documented agreement that triggers responsibilities and compensation. In the court filing, the Justice Department said Watson\\u2019s ruling on refugees would make the Supreme Court\\u2019s decision on that part of the executive order \\u201ceffectively meaningless.\\u201d The ruling, if left in place, means refugees can continue to be resettled in the United States, beyond a cap of 50,000 set by the executive order. That limit was reached this week.  The Supreme Court\\u2019s decision last month revived parts of Trump\\u2019s March 6 executive order banning travelers from Iran, Libya, Somalia, Sudan, Syria and Yemen for 90 days, as well as refugees for 120 days. The court also agreed to hear oral arguments in the fall over whether the ban violates the U.S. Constitution. \",\n          \"Friends!!! As you may know, #mamatortuga home is located in #southflorida and we are going to received #hurricane #dorian \\nIn the past, we have chose to evacuate, but at this moment,  my husband and car are not in\\u2026 https://www.instagram.com/p/B1y2PK5gOaK/?igshid=e9g3ug3vhixr\\u00a0\\u2026\",\n          \"WASHINGTON (Reuters) - The U.S. Senate voted on Thursday to confirm retired neurosurgeon Ben Carson as secretary of the Department of Housing and Urban Development (HUD) in President Donald Trump\\u2019s Cabinet. Carson, who ran for the 2016 Republican presidential nomination and later endorsed Trump, becomes the only African-American in the Trump Cabinet. The Senate confirmed his appointment by a vote of 58 to 41. Carson was sworn in by Vice President Mike Pence later on Thursday. During his confirmation hearing in January, Carson, 65, told the Senate Committee on Banking, Housing, and Urban Affairs that he would monitor any potential conflicts of interest between his agency and properties controlled by Trump. He also told lawmakers he was fit to lead HUD, an agency whose mission includes helping the poor get housing, even though he has sometimes criticized its work. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_misinfo_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b60fb51c-fabf-4812-9a4b-549422af48ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Local Charlotte, NC news station WSOCTV is rep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The tsunami has started President Obama s Keny...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The only reality show Donald Trump should have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No Food, No FEMA: Hurricane Michael’s Survivor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WASHINGTON (Reuters) - Here are some of the hi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b60fb51c-fabf-4812-9a4b-549422af48ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b60fb51c-fabf-4812-9a4b-549422af48ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b60fb51c-fabf-4812-9a4b-549422af48ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0cd7879-49f8-41e2-9fb5-246cf035fd42\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0cd7879-49f8-41e2-9fb5-246cf035fd42')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0cd7879-49f8-41e2-9fb5-246cf035fd42 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Local Charlotte, NC news station WSOCTV is rep...      1\n",
              "1  The tsunami has started President Obama s Keny...      1\n",
              "2  The only reality show Donald Trump should have...      1\n",
              "3  No Food, No FEMA: Hurricane Michael’s Survivor...      0\n",
              "4  WASHINGTON (Reuters) - Here are some of the hi...      0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_cloned = ds.copy()\n",
        "\n",
        "# DATA PARTITIONING =====================================================================\n",
        "\n",
        "ds_cloned['train'] = ds_cloned['train'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "ds_cloned['test'] = ds_cloned['test'].remove_columns(['Unnamed: 0', 'Unnamed: 0.1'])\n",
        "\n",
        "df_misinfo_train = pd.DataFrame(ds_cloned['train'], columns=[\"text\", \"label\"])\n",
        "df_misinfo_test = pd.DataFrame(ds_cloned['test'], columns=[\"text\", \"label\"])\n",
        "\n",
        "\n",
        "print(f\"Train shape {df_misinfo_train.shape} \\n\")\n",
        "print(\"Training positive vs negative examples: \\n\", df_misinfo_train.value_counts(\"label\")/df_misinfo_train.shape[0])\n",
        "print(\"\\nTesting positive vs negative examples: \\n\",df_misinfo_test.value_counts(\"label\")/df_misinfo_test.shape[0])\n",
        "\n",
        "df_misinfo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2UzViGPVkpkF",
        "outputId": "706c76ea-4286-40f3-d99a-06685b3866b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "64170"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# balance train split -------------------------------------------------------------------\n",
        "\n",
        "balancer = RandomUnderSampler(random_state=42, sampling_strategy = 'majority')\n",
        "df_misinfo_train_balanced = pd.concat(balancer.fit_resample(X = df_misinfo_train.iloc[:,[0]],\n",
        "                                                           y = df_misinfo_train.iloc[:,[1]]),\n",
        "                                     axis=1).sample(frac = 1).reset_index(drop=True)\n",
        "df_misinfo_train_balanced.value_counts(\"label\")/df_misinfo_train_balanced.shape[0]\n",
        "df_misinfo_train_balanced.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W5D-Y3ovkpkF",
        "outputId": "f9ca0d26-abee-40cc-cfe8-8df614f09eb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized text pkl files found: loading data...\n"
          ]
        }
      ],
      "source": [
        "# TOKENIZATION ==========================================================================\n",
        "# Define a function for batch processing\n",
        "def batch_tokenize(data, tokenizer, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Tokenizes the input data in batches to prevent memory issues.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The dataset to tokenize (as a pandas Series).\n",
        "    - tokenizer: The tokenizer function.\n",
        "    - batch_size: Number of rows to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "    - A list of tokenized outputs.\n",
        "    \"\"\"\n",
        "    tokenized_batches = []\n",
        "    for start in range(0, len(data), batch_size):\n",
        "        batch = data[start:start + batch_size]\n",
        "        print(f\"Tokenizing batch {start // batch_size + 1}/{len(data) // batch_size + 1}...\")\n",
        "        tokenized_batch = batch.map(tokenizer)\n",
        "        tokenized_batches.extend(tokenized_batch)\n",
        "    return tokenized_batches\n",
        "\n",
        "def custom_analyzer(text, trained_tokenizer):\n",
        "        tokens = custom_tokenizer(text)\n",
        "        vocab = trained_tokenizer.vocabulary_\n",
        "        return [token if token in vocab else \"<unk>\" for token in tokens]\n",
        "\n",
        "# Check if the pickle files already exist\n",
        "train_tokens_file = 'misinfo_train_tokens.pkl'\n",
        "test_tokens_file = 'misinfo_test_tokens.pkl'\n",
        "\n",
        "if os.path.exists(train_tokens_file) and os.path.exists(test_tokens_file):\n",
        "    print(\"Tokenized text pkl files found: loading data...\")\n",
        "    # Load the pre-saved tokenized data\n",
        "    with open(train_tokens_file, 'rb') as f:\n",
        "        misinfo_train_tokens = pickle.load(f)\n",
        "\n",
        "    with open(test_tokens_file, 'rb') as f:\n",
        "        misinfo_test_tokens = pickle.load(f)\n",
        "else:\n",
        "    print(\"Pickle files not found. Running tokenization...\")\n",
        "\n",
        "    print(\"Loading spaCy model...\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "\n",
        "    misinfo_tokenizer = CountVectorizer(analyzer=\"word\",\n",
        "                                        tokenizer=custom_tokenizer,\n",
        "                                        lowercase=False,\n",
        "                                        min_df=3)\n",
        "\n",
        "    print(\"Fitting Tokenizer...\")\n",
        "    misinfo_tokenizer.fit(df_misinfo_train[\"text\"])\n",
        "    misinfo_tokenizer_analyzer = misinfo_tokenizer.build_analyzer()\n",
        "\n",
        "    print(\"Tokenizing Train Data in Batches...\")\n",
        "    misinfo_train_tokens = batch_tokenize(df_misinfo_train[\"text\"], misinfo_tokenizer_analyzer)\n",
        "\n",
        "    print(\"Tokenizing Test Data in Batches...\")\n",
        "    misinfo_test_tokens = batch_tokenize(df_misinfo_test[\"text\"],\n",
        "                                         lambda x: custom_analyzer(x, misinfo_tokenizer))\n",
        "\n",
        "    # Save tokenized train and test data\n",
        "    with open(train_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_train_tokens, f)\n",
        "\n",
        "    with open(test_tokens_file, 'wb') as f:\n",
        "        pickle.dump(misinfo_test_tokens, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aJMnZ2N8kpkG",
        "outputId": "ee687dfb-fec5-4f58-de83-e53db2df6581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab indexing\n",
            "creating data loaders\n",
            "created data loaders\n",
            "Emebddings pre-exists: loaded embeddings from mapped_pretrained_embeddings.pkl. Shape: torch.Size([217732, 300])\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: INPUT PIPELINE ================================================================\n",
        "\n",
        "# vocabulary indexing -------------------------------------------------------------------\n",
        "print (\"vocab indexing\")\n",
        "\n",
        "def vocab_mapping(tokenized_text):\n",
        "    token_counts = Counter()\n",
        "    for text in tokenized_text:\n",
        "        token_counts.update(text)\n",
        "    special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "    vocab_tokens = special_tokens + [token for token, freq in token_counts.most_common()]\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "    return vocab\n",
        "\n",
        "vocab_idx = vocab_mapping(tokenized_text=misinfo_train_tokens)\n",
        "\n",
        "# create data loaders -------------------------------------------------------------------\n",
        "\n",
        "print(\"creating data loaders\")\n",
        "\n",
        "def collate_fn(data):\n",
        "    text_list, label_list = [], []\n",
        "    for _text, _label in data:\n",
        "        # integer encoding with truncation\n",
        "        processed_text = torch.tensor([vocab_idx[token] for token in _text][:max_seq_length],\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        label_list.append(_label)\n",
        "    label_list = torch.tensor(label_list)\n",
        "    # padding\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(text_list,\n",
        "                                                 batch_first=True,\n",
        "                                                 padding_value=0)\n",
        "    return padded_text_list, label_list\n",
        "\n",
        "max_seq_length = 300\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(dataset=list(zip(misinfo_train_tokens,\n",
        "                                         df_misinfo_train[\"label\"])), # was meant to be balanced\n",
        "                        batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = DataLoader(dataset=list(zip(misinfo_test_tokens,\n",
        "                                         df_misinfo_test[\"label\"])),\n",
        "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"created data loaders\")\n",
        "\n",
        "# map pretrained fasttext embeddings to vocabulary indices ------------------------------\n",
        "\n",
        "# Define the file path for the pickle file\n",
        "pickle_file_path = \"mapped_pretrained_embeddings.pkl\"\n",
        "\n",
        "# Check if the pickle file already exists\n",
        "if os.path.exists(pickle_file_path):\n",
        "    # If the file exists, load it from the pickle file\n",
        "    with open(pickle_file_path, 'rb') as f:\n",
        "        embedding_tensor = pickle.load(f)\n",
        "    print(f\"Emebddings pre-exists: loaded embeddings from {pickle_file_path}. Shape: {embedding_tensor.shape}\")\n",
        "else:\n",
        "    # If the file does not exist, proceed with creating the embeddings and save them\n",
        "    # Load pre-trained FastText model\n",
        "    print(\"Embeddings do not pre-exist: mapping pretrained fasttext embeddings to vocabulary indices\")\n",
        "\n",
        "    # Map pretrained FastText embeddings to vocabulary indices\n",
        "    mapped_pretrained_embeddings = embedding_mapping_fasttext(vocabulary=vocab_idx,\n",
        "                                                              pre_trained_embeddings=ft)\n",
        "\n",
        "    # Convert mapped embeddings to a tensor\n",
        "    embedding_tensor = torch.FloatTensor(mapped_pretrained_embeddings)\n",
        "\n",
        "    # Save the embeddings to a pickle file\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump(embedding_tensor, f)\n",
        "    print(f\"Saved embeddings to {pickle_file_path}. Shape: {embedding_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jMT0Vm5WkpkG"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qu0_t5xTkpkG",
        "outputId": "b95b0b6e-12bc-4135-a4c3-108a9a58ccdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-40a729599432>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# STEP 2: LOSS FUNCTION AND OPTIMIZER SPECIFICATION =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# STEP 3: MODEL TRAINING AND EVALUATION =================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# STEP 2: LOSS FUNCTION AND OPTIMIZER SPECIFICATION =====================================\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# STEP 3: MODEL TRAINING AND EVALUATION =================================================\n",
        "\n",
        "def train(model, num_epochs, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available\n",
        "    model.to(device)  # Move the model to GPU or CPU\n",
        "\n",
        "    # Ensure the optimizer uses the correct device (it should automatically use the same device as the model)\n",
        "    optimizer = torch.optim.Adam(model.parameters())  # Assuming Adam optimizer here\n",
        "\n",
        "    loss_hist_train = [0] * num_epochs\n",
        "    accuracy_hist_train = [0] * num_epochs\n",
        "    f1_hist_train = [0] * num_epochs\n",
        "    loss_hist_test = [0] * num_epochs\n",
        "    accuracy_hist_test = [0] * num_epochs\n",
        "    f1_hist_test = [0] * num_epochs\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # set training mode\n",
        "        all_train_preds = []\n",
        "        all_train_labels = []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Training...\")\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_dl):\n",
        "            # Move data to device (GPU or CPU)\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            pred = model(x_batch)[:, 0]  # generate predictions\n",
        "            loss = loss_fn(pred, y_batch.float())  # compute loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()  # compute gradients\n",
        "            optimizer.step()  # update parameters\n",
        "            optimizer.zero_grad()  # reset gradients\n",
        "\n",
        "            # evaluate train\n",
        "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
        "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "            accuracy_hist_train[epoch] += is_correct.sum()\n",
        "            all_train_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "            all_train_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "            # Print batch progress\n",
        "            if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(train_dl):\n",
        "                print(f\"    Batch {batch_idx + 1}/{len(train_dl)}: \"\n",
        "                      f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        f1_hist_train[epoch] = f1_score(all_train_labels, all_train_preds)\n",
        "\n",
        "        # evaluate model\n",
        "        model.eval()  # set evaluation mode\n",
        "        all_test_preds = []\n",
        "        all_test_labels = []\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Evaluating...\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (x_batch, y_batch) in enumerate(test_dl):\n",
        "                # Move data to device (GPU or CPU)\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                # forward pass\n",
        "                pred = model(x_batch)[:, 0]\n",
        "                loss = loss_fn(pred, y_batch.float())\n",
        "\n",
        "                # evaluate test\n",
        "                loss_hist_test[epoch] += loss.item() * y_batch.size(0)\n",
        "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
        "                accuracy_hist_test[epoch] += is_correct.sum()\n",
        "                all_test_preds.extend((pred >= 0.5).cpu().numpy())\n",
        "                all_test_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "                # Print batch progress\n",
        "                if (batch_idx + 1) % 1000 == 0 or (batch_idx + 1) == len(test_dl):\n",
        "                    print(f\"    Batch {batch_idx + 1}/{len(test_dl)}: \"\n",
        "                          f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # record epoch progress\n",
        "        loss_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        accuracy_hist_test[epoch] /= len(test_dl.dataset)\n",
        "        f1_hist_test[epoch] = f1_score(all_test_labels, all_test_preds)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} Summary:\")\n",
        "        print(f\"    Train - Accuracy: {accuracy_hist_train[epoch]:.3f}, F1: {f1_hist_train[epoch]:.3f}\")\n",
        "        print(f\"    Test  - Accuracy: {accuracy_hist_test[epoch]:.3f}, F1: {f1_hist_test[epoch]:.3f}\")\n",
        "\n",
        "    return [loss_hist_train, loss_hist_test, accuracy_hist_train,\n",
        "            accuracy_hist_test, f1_hist_train, f1_hist_test]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: MODEL BUILDING ================================================================\n",
        "# CNN-based text classification model\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layers\n",
        "        convolution_layer = nn.Conv1d(in_channels=embedding_tensor.size(1),\n",
        "                                      out_channels=128,\n",
        "                                      kernel_size=3,\n",
        "                                      padding=\"same\")\n",
        "        activation_layer = nn.ReLU()\n",
        "        pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "        h_layers = [convolution_layer, activation_layer, pooling_layer]\n",
        "        self.hidden_layers = nn.ModuleList(h_layers)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.embedding_layer(x).permute(0, 2, 1)\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = TextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "hist_cnn = train(model, num_epochs, train_dl, test_dl)"
      ],
      "metadata": {
        "id": "8qQ1LhWHrXP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_5dUjnvkpkG"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 1: RNN =====================================================================\n",
        "\n",
        "class RNNTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.rnn_layer = nn.RNN(input_size=embedding_tensor.size(1),\n",
        "                                hidden_size=32,\n",
        "                                num_layers=1, # increase to stack RNNs\n",
        "                                batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, h_t = self.rnn_layer(x) # o_t includes the outputs,\n",
        "                                     # h_t the hidden state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model_rnn = RNNTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "\n",
        "hist_rnn = train(model, num_epochs, train_dl, test_dl) # fluctuating f1 scores, exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK1Kjy-CkpkG"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 2: LSTM =====================================================================\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                  hidden_size=32,\n",
        "                                  num_layers=1,\n",
        "                                  batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.lstm_layer(x) # c_t the cell state at the last time step\n",
        "        x = h_t[-1, :, :] # extract from last layer (in case of num_layers > 1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = LSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl) # better but not great"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1aBkTs_kpkH"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 2.5: STACKING LSTM LAYERS WITH DIFFERENT HIDDEN SIZES =========================\n",
        "\n",
        "class StackedLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                    hidden_size=64,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=64,\n",
        "                                    hidden_size=32,\n",
        "                                    num_layers=1,\n",
        "                                    batch_first=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t_1, (h_t_1, c_t_1) = self.lstm_layer_1(x)\n",
        "        o_t_2, (h_t_2, c_t_2) = self.lstm_layer_2(o_t_1)\n",
        "        x = h_t_2[-1, :, :]\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = StackedLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e26pcU0tkpkH"
      },
      "outputs": [],
      "source": [
        "# EXTENSION 4: BI-DIRECTIONAL LSTM ======================================================\n",
        "\n",
        "class BidirectionalLSTMTextClassificationModel(nn.Module):\n",
        "    # create layers\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        # input layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
        "        # hidden layer\n",
        "        self.bid_lstm_layer = nn.LSTM(input_size=embedding_tensor.size(1),\n",
        "                                      hidden_size=32,\n",
        "                                      num_layers=1,\n",
        "                                      batch_first=True,\n",
        "                                      bidirectional=True)\n",
        "        # classification layer\n",
        "        self.classification_layer = nn.Linear(in_features=32*2, out_features=1)\n",
        "\n",
        "    # define forward pass\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x,\n",
        "                                              lengths.cpu().numpy(),\n",
        "                                              enforce_sorted=False,\n",
        "                                              batch_first=True)\n",
        "        o_t, (h_t, c_t) = self.bid_lstm_layer(x)\n",
        "        x = torch.cat((h_t[-2, :, :],\n",
        "                       h_t[-1, :, :]), dim=1)\n",
        "        x = self.classification_layer(x)\n",
        "        return x\n",
        "\n",
        "model = BidirectionalLSTMTextClassificationModel(embedding_tensor=embedding_tensor)\n",
        "model\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "hist = train(model, num_epochs, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNKdlv4kpkJ"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoGdIxt7kpkJ"
      },
      "outputs": [],
      "source": [
        "# load checkpoint -----------------------------------------------------------------------\n",
        "checkpoint = \"google/bert_uncased_L-2_H-128_A-2\" # aka BERT-Tiny\n",
        "# model card: https://huggingface.co/google/bert_uncased_L-2_H-128_A-2\n",
        "\n",
        "# load corresponding tokenizer ----------------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# load corresponding model with binary classification head ------------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 2)\n",
        "# to see other heads, type transformers.AutoModel and see autocomplete\n",
        "\n",
        "\n",
        "# APPLYING A MODEL WITHOUT FINE-TUNING (ignoring the warning) ===========================\n",
        "\n",
        "# process texts and apply model ---------------------------------------------------------\n",
        "tokenized_texts = tokenizer(df_misinfo_test[\"text\"].to_list(), truncation=True,\n",
        "                            padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# predict and evaluate ------------------------------------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_texts)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "predicted_labels = torch.argmax(predictions, dim=1)\n",
        "true_labels = torch.tensor(df_misinfo_test[\"label\"].to_list())\n",
        "f1 = f1_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "acc = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "# obviously bad and erratic performance as model is not tailored to the task at hand\n",
        "f1\n",
        "acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5eMBCBpkpkJ"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHtNWdu2kpkJ"
      },
      "outputs": [],
      "source": [
        "# convert train and test data to hugging face Dataset -----------------------------------\n",
        "features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['not_housing', 'housing']),\n",
        "})\n",
        "dataset_train = Dataset.from_pandas(df_misinfo_train, features=features)\n",
        "dataset_test = Dataset.from_pandas(df_misinfo_test, features=features)\n",
        "\n",
        "# create a hugging face DatasetDict -----------------------------------------------------\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test\n",
        "})\n",
        "print(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gICHMe6LkpkK"
      },
      "outputs": [],
      "source": [
        "# tokenize ------------------------------------------------------------------------------\n",
        "def tokenize_function(dataset):\n",
        "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    # truncates at 512 for the chosen checkpoint\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n",
        "tokenized_datasets['train'][0]['text']\n",
        "tokenized_datasets['train'][0]['label']\n",
        "tokenized_datasets['train'][0]['input_ids']\n",
        "tokenized_datasets['train'][0]['attention_mask']\n",
        "\n",
        "# fine-tune -----------------------------------------------------------------------------\n",
        "training_args = TrainingArguments(output_dir=\"./results\",\n",
        "                                  eval_strategy=\"epoch\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=32,\n",
        "                                  per_device_eval_batch_size=32,\n",
        "                                  num_train_epochs=30,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model='f1',\n",
        "                                  disable_tqdm=True,\n",
        "                                  use_cpu=True)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# predict -------------------------------------------------------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_texts)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "predicted_labels = torch.argmax(predictions, dim=1)\n",
        "true_labels = torch.tensor(df_misinfo_test[\"label\"].to_list())\n",
        "f1 = f1_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "acc = accuracy_score(true_labels.numpy(), predicted_labels.numpy())\n",
        "f1\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEToEHskpkK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL7RAEQYkpkK"
      },
      "outputs": [],
      "source": [
        "# reading climate df\n",
        "input_path_climate = \"/Users/henrybaker/.cache/kagglehub/datasets/die9origephit/climate-change-tweets/versions/1/Climate change_2022-1-17_2022-7-19.csv\"\n",
        "\n",
        "output_path_climate = \"/Users/henrybaker/Documents/repositories/NLP/nlp_project/data/climate-change-tweets.csv\"\n",
        "\n",
        "df_climate = pd.read_csv(input_path_climate)\n",
        "print(f\"Loading dataset from '{input_path_climate}'...\")\n",
        "df_climate.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}